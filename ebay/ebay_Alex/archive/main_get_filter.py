"""
–†–£–ö–û–í–û–î–°–¢–í–û –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ò–ó–í–õ–ï–ö–ê–¢–ï–õ–Ø –§–ò–õ–¨–¢–†–û–í EBAY

–≠—Ç–æ—Ç –∫–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã: Brand, Condition, Type, Price –∏ —Ç.–¥.
"""

import json

import requests
from bs4 import BeautifulSoup
from universal_filter_extractor import (
    extract_all_filters,
    extract_condition_codes,
    extract_filter_options,
    save_filter_data,
)


def extract_all_ebay_filters():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É eBay –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
    """
    url = "https://www.ebay.com/b/Car-Truck-ECUs-Computer-Modules/33596/bn_584314"
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    }

    try:
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É eBay...")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤)")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open("ebay_filters_page.html", "w", encoding="utf-8") as f:
            f.write(response.text)
        print("üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: ebay_filters_page.html")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï —Ñ–∏–ª—å—Ç—Ä—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        print("\nüîç –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã...")
        all_filters = extract_all_filters(response.text)

        print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(all_filters)} —Ñ–∏–ª—å—Ç—Ä–æ–≤:")
        for filter_name, options in all_filters.items():
            print(f"  ‚Ä¢ {filter_name}: {len(options)} –æ–ø—Ü–∏–π")

        return all_filters, response.text

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {str(e)}")
        return {}, ""


def extract_specific_filters(html_content, filter_names):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã

    Args:
        html_content (str): HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        filter_names (list): –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    """
    filters = {}

    print(f"\nüéØ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã: {filter_names}")

    for filter_name in filter_names:
        print(f"\n--- –§–∏–ª—å—Ç—Ä: {filter_name} ---")
        options = extract_filter_options(html_content, filter_name)

        if options:
            filters[filter_name] = options
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(options)} –æ–ø—Ü–∏–π")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ–ø—Ü–∏–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            for i, (option, value) in enumerate(options.items()):
                if i < 3:
                    print(f"   {option}: {value[:80]}...")
                elif i == 3:
                    print(f"   ... –∏ –µ—â–µ {len(options) - 3} –æ–ø—Ü–∏–π")
                    break
        else:
            print(f"‚ùå –§–∏–ª—å—Ç—Ä '{filter_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")

    return filters


def extract_condition_mapping(html_content):
    """
    –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–æ–≤ —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞
    """
    print("\nüè∑Ô∏è –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥—ã —Å–æ—Å—Ç–æ—è–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤...")

    condition_codes = extract_condition_codes(html_content)

    if condition_codes:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(condition_codes)} —Å–æ—Å—Ç–æ—è–Ω–∏–π:")
        for condition, code in condition_codes.items():
            print(f"   {condition}: {code}")

        # –°–æ–∑–¥–∞–µ–º —É–¥–æ–±–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        scraper_conditions = {
            "new": "1000",
            "used": "3000",
            "remanufactured": "2500",
            "parts": "7000",
        }

        print(f"\nüìã –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–¥—ã –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞:")
        for key, code in scraper_conditions.items():
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            condition_name = next(
                (name for name, c in condition_codes.items() if c == code), "Unknown"
            )
            print(f"   '{key}': '{code}',  # {condition_name}")

        return condition_codes
    else:
        print("‚ùå –°–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return {}


def demonstrate_usage():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    """
    print("=" * 60)
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –§–ò–õ–¨–¢–†–û–í EBAY")
    print("=" * 60)

    # –°–ø–æ—Å–æ–± 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    print("\n1Ô∏è‚É£ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –í–°–ï–• –§–ò–õ–¨–¢–†–û–í")
    all_filters, html_content = extract_all_ebay_filters()

    if html_content:
        # –°–ø–æ—Å–æ–± 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        print("\n2Ô∏è‚É£ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–ù–ö–†–ï–¢–ù–´–• –§–ò–õ–¨–¢–†–û–í")
        target_filters = ["Brand", "Condition", "Type", "Price"]
        specific_filters = extract_specific_filters(html_content, target_filters)

        # –°–ø–æ—Å–æ–± 3: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        print("\n3Ô∏è‚É£ –°–ü–ï–¶–ò–ê–õ–¨–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–û–°–¢–û–Ø–ù–ò–ô")
        condition_codes = extract_condition_mapping(html_content)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_data = {
            "all_filters": all_filters,
            "specific_filters": specific_filters,
            "condition_codes": condition_codes,
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        save_filter_data(all_filters, "all_ebay_filters.json")
        save_filter_data(specific_filters, "specific_filters.json")

        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        create_scraper_config(specific_filters, condition_codes)

        return final_data

    return {}


def create_scraper_config(filters, condition_codes):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞
    """
    print("\n‚öôÔ∏è –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞...")

    config = {
        "base_url": "https://www.ebay.com/b/Car-Truck-ECUs-Computer-Modules/33596/bn_584314",
        "filters": {},
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –±—Ä–µ–Ω–¥—ã
    if "Brand" in filters:
        brands = {}
        for brand_name, brand_url in filters["Brand"].items():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π URL –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            base_url = brand_url.split("?")[0] if "?" in brand_url else brand_url
            brands[brand_name] = base_url
        config["filters"]["brands"] = brands

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
    if condition_codes:
        config["filters"]["conditions"] = condition_codes

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if "Type" in filters:
        config["filters"]["types"] = filters["Type"]

    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
    config["filters"]["price_ranges"] = [
        {"name": "low", "params": {"_udhi": "75"}},
        {"name": "medium", "params": {"_udlo": "75", "_udhi": "150"}},
        {"name": "high", "params": {"_udlo": "150"}},
    ]

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open("scraper_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=4, ensure_ascii=False)

    # –°–æ–∑–¥–∞–µ–º Python —Ñ–∞–π–ª —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    with open("scraper_config.py", "w", encoding="utf-8") as f:
        f.write("# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞ eBay\n\n")
        f.write(f"BASE_URL = \"{config['base_url']}\"\n\n")

        if "brands" in config["filters"]:
            f.write("BRAND_URLS = {\n")
            for brand, url in sorted(config["filters"]["brands"].items()):
                f.write(f'    "{brand}": "{url}",\n')
            f.write("}\n\n")

        if "conditions" in config["filters"]:
            f.write("CONDITION_CODES = {\n")
            for condition, code in sorted(config["filters"]["conditions"].items()):
                f.write(f'    "{condition}": "{code}",\n')
            f.write("}\n\n")

        f.write("PRICE_RANGES = [\n")
        for price_range in config["filters"]["price_ranges"]:
            f.write(f"    {price_range},\n")
        f.write("]\n")

    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
    print("   üìÑ scraper_config.json")
    print("   üêç scraper_config.py")


def quick_extract_example():
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞
    """
    print("\n" + "=" * 40)
    print("‚ö° –ë–´–°–¢–†–´–ô –ü–†–ò–ú–ï–†")
    print("=" * 40)

    # –ï—Å–ª–∏ —É –≤–∞—Å —É–∂–µ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π HTML —Ñ–∞–π–ª
    try:
        with open("ebay_filters_page.html", "r", encoding="utf-8") as f:
            html_content = f.read()

        print("üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π HTML —Ñ–∞–π–ª")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –±—Ä–µ–Ω–¥—ã
        brands = extract_filter_options(html_content, "Brand")
        print(f"üè∑Ô∏è –ù–∞–π–¥–µ–Ω–æ –±—Ä–µ–Ω–¥–æ–≤: {len(brands)}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        conditions = extract_filter_options(html_content, "Condition")
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π: {len(conditions)}")

        return {"brands": brands, "conditions": conditions}

    except FileNotFoundError:
        print("‚ùå HTML —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ demonstrate_usage()")
        return {}


# –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
if __name__ == "__main__":
    print("üéâ –†–£–ö–û–í–û–î–°–¢–í–û –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ò–ó–í–õ–ï–ö–ê–¢–ï–õ–Ø –§–ò–õ–¨–¢–†–û–í")
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–±:")
    print("1. demonstrate_usage() - –ü–æ–ª–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("2. quick_extract_example() - –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–º–µ—Ä")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é
    result = demonstrate_usage()

    print("\n" + "=" * 60)
    print("‚úÖ –ì–û–¢–û–í–û! –í—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    print("üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("   ‚Ä¢ all_ebay_filters.json - –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã")
    print("   ‚Ä¢ specific_filters.json - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã")
    print("   ‚Ä¢ scraper_config.json - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∫—Ä–∞–ø–µ—Ä–∞")
    print("   ‚Ä¢ scraper_config.py - Python –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
    print("=" * 60)
