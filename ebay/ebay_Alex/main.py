import argparse
import concurrent.futures
import json
import random
import re
import time
from pathlib import Path
from threading import Lock

import pandas as pd
import requests
import urllib3
from bs4 import BeautifulSoup
from config.logger import logger
from requests.exceptions import HTTPError
from scrap import scrap_online
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
current_directory = Path.cwd()
data_directory = current_directory / "data"
config_directory = current_directory / "config"
progress_directory = current_directory / "progress"
temp_directory = current_directory / "temp"
xlsx_directory = temp_directory / "xlsx"
json_directory = temp_directory / "json"

html_directory = temp_directory / "html"
config_directory = current_directory / "config"
temp_directory.mkdir(parents=True, exist_ok=True)
json_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)


config_file = config_directory / "config.json"

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
    "sec-ch-ua-full-version": '"135.0.7049.115"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-model": '""',
    "sec-ch-ua-platform": '"Windows"',
    "sec-ch-ua-platform-version": '"19.0.0"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
}
# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏
proxy_list = []


def load_proxies():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json"""
    global proxy_list
    try:
        if config_file.exists():
            with open(config_file, "r", encoding="utf-8") as f:
                config = json.load(f)
                if "proxy_server" in config and isinstance(
                    config["proxy_server"], list
                ):
                    proxy_list = config["proxy_server"]
                    logger.info(
                        f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json"
                    )
                else:
                    logger.warning("–í config.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤")
        else:
            logger.warning("–§–∞–π–ª config.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏: {str(e)}")


def get_random_proxy():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞"""
    if not proxy_list:
        return None
    proxy_url = random.choice(proxy_list)
    proxy_url = proxy_url.strip()
    return {"http": proxy_url, "https": proxy_url}


@retry(
    stop=stop_after_attempt(10),
    wait=wait_fixed(10),
    retry=retry_if_exception_type(
        (HTTPError, requests.exceptions.ConnectionError, requests.exceptions.Timeout)
    ),
)
def make_request(url):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å"""
    try:
        proxies = get_random_proxy()
        response = requests.get(
            url,
            # cook/ies=cookies,
            proxies=proxies,
            headers=headers,
            timeout=30,
            verify=False,
        )
        response.raise_for_status()
        return response.text
    except requests.exceptions.ProxyError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è {url}: {e}")
        raise

    except requests.exceptions.ConnectionError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è {url}: {e}")
        raise

    except requests.exceptions.Timeout as e:
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        raise

    except requests.exceptions.HTTPError as e:
        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {url}: {e}")
        raise

    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        raise


def make_soup(html_content):
    """–°–æ–∑–¥–∞–µ—Ç BeautifulSoup –æ–±—ä–µ–∫—Ç"""
    return BeautifulSoup(html_content, "lxml")


def extract_filter_options(soup, filter_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ü–∏–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞"""
    options = []
    try:
        table = soup.find("section", attrs={"id": "brw-refinement-root"})
        filter_containers = table.find_all("span", class_="filter-menu-button")

        target_container = None
        for container in filter_containers:
            filter_label = container.find("span", class_="filter-label")
            if filter_label and filter_label.get_text().strip() == filter_name:
                target_container = container
                break

        if not target_container:
            logger.warning(f"‚ö†Ô∏è  –§–∏–ª—å—Ç—Ä '{filter_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return options

        option_items = target_container.find_all("li", class_="brwr__inputs")
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(option_items)} –æ–ø—Ü–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ '{filter_name}'")

        for item in option_items:
            link = item.find("a", class_="brwr__inputs__actions")
            if not link:
                continue

            href = link.get("href")
            if not href:
                continue

            option_span = link.find("span", class_="textual-display")
            if not option_span:
                continue

            option_name = option_span.get_text().strip()
            clean_url = href.replace("&amp;", "&")

            options.append({"name": option_name, "url": clean_url})

        return options
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞ '{filter_name}': {e}")
        return []


def get_results_count(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        count_element = soup.find("h2", class_="textual-display brw-controls__count")

        if not count_element:
            count_element = soup.find("span", class_="brw-controls__count")

        if count_element:
            count_text = count_element.get_text()
            match = re.search(r"([\d,]+)", count_text)
            if match:
                count_str = match.group(1).replace(",", "")
                count = int(count_str)
                return count

        return 0
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: {e}")
        return 0


def get_final_segment_urls(url, applied_filters=None, max_results=10000):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç URL –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã ‚â§ max_results
    –°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤

    Args:
        url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        applied_filters (list): –£–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        max_results (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ

    Yields:
        tuple: (url, name) - –§–∏–Ω–∞–ª—å–Ω—ã–µ URL —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
    """
    if applied_filters is None:
        applied_filters = []

    filter_path = " ‚Üí ".join(applied_filters) if applied_filters else "–ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
    logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º: {filter_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    response_text = make_request(url)
    if not response_text:
        return

    soup = make_soup(response_text)
    count = get_results_count(soup)

    logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {count:,}")

    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–µ–º–ª–µ–º–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º URL —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –±—Ä–µ–Ω–¥–µ
    if count <= max_results:
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç: {count:,} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                # –û—á–∏—â–∞–µ–º –∏–º—è –±—Ä–µ–Ω–¥–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–∞–ø–∫–∏
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
    logger.info(
        f"‚ö†Ô∏è  –ú–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({count:,} > {max_results:,}). –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é."
    )

    filters_sequence = [
        "Brand",
        "Condition",
        "Price",
        "Type",
        "Brand Type",
        "Programming Required",
        "Country/Region of Manufacture",
    ]

    # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π —Ñ–∏–ª—å—Ç—Ä
    next_filter = None
    for filter_name in filters_sequence:
        if not any(filter_name in af for af in applied_filters):
            next_filter = filter_name
            break

    if not next_filter:
        logger.warning(
            f"‚ö†Ô∏è  –í—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {count:,}. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å."
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        brand_name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    logger.info(f"üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä: {next_filter}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ü–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞
    filter_options = extract_filter_options(soup, next_filter)

    if not filter_options:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–ø—Ü–∏–∏ –¥–ª—è '{next_filter}'")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        brand_name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –æ–ø—Ü–∏—é
    for i, option in enumerate(filter_options):
        option_name = option["name"]
        option_url = option["url"]

        logger.info(f"üìå –û–ø—Ü–∏—è {i+1}/{len(filter_options)}: {option_name}")

        new_applied_filters = applied_filters + [f"{next_filter}: {option_name}"]

        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ–º URL –∏–∑ —ç—Ç–æ–π –æ–ø—Ü–∏–∏
        yield from get_final_segment_urls(option_url, new_applied_filters, max_results)


def scrape_page(full_url, params=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã

    Args:
        url (str): –ë–∞–∑–æ–≤—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        tuple: (next_url, page_hrefs) - URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    """
    page_hrefs = []

    try:

        logger.info(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º: {full_url}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        src = make_request(full_url)
        soup = BeautifulSoup(src, "lxml")

        # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ JSON-LD
        json_ld_scripts = soup.find_all("script", {"type": "application/ld+json"})

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫ JSON-LD
        for script in json_ld_scripts:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–≥–∞ script
                json_data = json.loads(script.string)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
                if (
                    "about" in json_data
                    and "offers" in json_data["about"]
                    and "itemOffered" in json_data["about"]["offers"]
                ):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
                    items = json_data["about"]["offers"]["itemOffered"]

                    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Ç–æ–≤–∞—Ä—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º URL
                    for item in items:
                        if "url" in item:
                            # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                            item_url = item["url"]
                            if "?" in item_url:
                                cleaned_url = item_url.split("?")[0]
                                page_hrefs.append(cleaned_url)
                            else:
                                page_hrefs.append(item_url)
            except json.JSONDecodeError:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON-LD")
                continue
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON-LD: {str(e)}")
                continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_button = soup.select_one("a.pagination__next")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–Ω–æ–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç href –∏ –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω–∞
        if next_button and "href" in next_button.attrs:
            disabled_next = soup.select_one('a.pagination__next[aria-disabled="true"]')

            if not disabled_next:
                next_url = next_button["href"]
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {next_url}")
                return next_url, page_hrefs

        return None, page_hrefs
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {full_url}: {str(e)}")
        return None, []


def collect_segment_urls(base_url, threads, max_results=10000):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ URL —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –ø–æ –æ–¥–Ω–æ–º—É
    –°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤

    Args:
        base_url (str): –ë–∞–∑–æ–≤—ã–π URL eBay
        max_results (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
    """
    logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–£ –°–ï–ì–ú–ï–ù–¢–û–í")
    logger.info("=" * 60)
    logger.info(f"üéØ –¶–µ–ª—å: URL —Å —Ç–æ–≤–∞—Ä–∞–º–∏ ‚â§ {max_results:,}")
    logger.info("=" * 60)

    segment_counter = 0

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π URL —Å—Ä–∞–∑—É –∫–∞–∫ —Ç–æ–ª—å–∫–æ –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    for segment_url, brand_name in get_final_segment_urls(
        base_url, max_results=max_results
    ):
        segment_counter += 1
        logger.info(
            f"üîó –ü–æ–ª—É—á–µ–Ω —Å–µ–≥–º–µ–Ω—Ç #{segment_counter} –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand_name}': {segment_url}"
        )

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –±—Ä–µ–Ω–¥–∞
        brand_directory = json_directory / brand_name
        brand_directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ –ø–∞–ø–∫–∞: {brand_directory}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç —Å—Ä–∞–∑—É
        success = process_single_segment(
            segment_url, segment_counter, brand_name, brand_directory, threads
        )
        if success:
            logger.info(f"‚úÖ –ì–û–¢–û–í–û! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {segment_counter} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            break


def process_single_segment(
    segment_url, segment_number, brand_name, brand_directory, threads
):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç - –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã

    Args:
        segment_url (str): URL —Å–µ–≥–º–µ–Ω—Ç–∞
        segment_number (int): –ù–æ–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        brand_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
        brand_directory (Path): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –±—Ä–µ–Ω–¥–∞
        threads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
    """
    logger.info(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –°–ï–ì–ú–ï–ù–¢–ê #{segment_number}")
    logger.info(f"üìå URL: {segment_url}")

    total_processed_urls = 0
    total_processed_pages = 0
    current_page = 1
    next_url = None

    # –¶–∏–∫–ª –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Ç–µ–∫—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    while True:
        logger.info(f"üìÑ –°–µ–≥–º–µ–Ω—Ç #{segment_number} | –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_page == 1:
            # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π URL —Å–µ–≥–º–µ–Ω—Ç–∞
            page_url = segment_url
            next_url, page_hrefs = scrape_page(page_url)
        else:
            # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if next_url is None:
                # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏, —Ñ–æ—Ä–º–∏—Ä—É–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                separator = "&" if "?" in segment_url else "?"
                page_url = f"{segment_url}{separator}_pgn={current_page}"
                if "rt=nc" not in page_url:
                    page_url += "&rt=nc"
                next_url, page_hrefs = scrape_page(page_url)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_url, page_hrefs = scrape_page(next_url)

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        total_processed_pages += 1

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        if page_hrefs:
            logger.info(
                f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_hrefs)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}"
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤
            try:
                success_count = get_product_th(
                    page_hrefs, brand_directory, threads=threads
                )
                total_processed_urls += success_count

                logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {success_count}/{len(page_hrefs)} —Ç–æ–≤–∞—Ä–æ–≤")
                logger.info(
                    f"üìä –°–µ–≥–º–µ–Ω—Ç #{segment_number} | –í—Å–µ–≥–æ: {total_processed_urls} —Ç–æ–≤–∞—Ä–æ–≤, {total_processed_pages} —Å—Ç—Ä–∞–Ω–∏—Ü"
                )

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–≤–∞—Ä–æ–≤: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if next_url:
            current_page += 1

        else:
            logger.info(
                f"üèÅ –°–µ–≥–º–µ–Ω—Ç #{segment_number} –∑–∞–≤–µ—Ä—à–µ–Ω | {total_processed_pages} —Å—Ç—Ä–∞–Ω–∏—Ü | {total_processed_urls} —Ç–æ–≤–∞—Ä–æ–≤"
            )
            break

    return {
        "segment_number": segment_number,
        "brand_name": brand_name,
        "total_pages": total_processed_pages,
        "total_products": total_processed_urls,
    }


def get_product_th(urls, brand_directory, threads):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ.
    –í–µ–¥–µ—Ç —É—á–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ.
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL —Å–æ —Å—Ç–∞—Ç—É—Å–∞–º–∏ 'failed' –∏ 'error'.

    Args:
        urls (list): –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        brand_directory (Path): –ü–∞–ø–∫–∞ –±—Ä–µ–Ω–¥–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        threads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 40.

    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ URL
    if not urls:
        logger.warning("–°–ø–∏—Å–æ–∫ URL –ø—É—Å—Ç")
        return 0

    total_urls = len(urls)
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {total_urls} —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤")

    # –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    mapping_file = brand_directory / "processed_urls.csv"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL –∏–∑ —Ñ–∞–π–ª–∞
    processed_urls_success = set()  # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    processed_urls_data = {}  # –î–∞–Ω–Ω—ã–µ –æ–±–æ –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL

    if mapping_file.exists():
        try:
            existing_df = pd.read_csv(mapping_file)
            if "url" in existing_df.columns:
                for _, row in existing_df.iterrows():
                    url = row["url"]
                    status = row.get("status", "unknown")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ URL
                    processed_urls_data[url] = {
                        "status": status,
                        "timestamp": row.get("timestamp", ""),
                        "error": row.get("error", ""),
                    }

                    # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ URL –¥–æ–±–∞–≤–ª—è–µ–º –≤ set –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞
                    if status == "success":
                        processed_urls_success.add(url)
                # count_urls = int(len(processed_urls_data))
                # count_urls_success = int(len(processed_urls_success))
                difference_threshold = 5

                logger.info(
                    f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_urls_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {mapping_file.name}"
                )
                logger.info(f"‚úÖ –ò–∑ –Ω–∏—Ö —É—Å–ø–µ—à–Ω—ã—Ö: {len(processed_urls_success)}")
                # difference = abs(count_urls - count_urls_success)
                # if difference <= difference_threshold:
                #     logger.info("–í—Å–µ —Å–∫–∞—á–∞–ª–∏")
                #     exit()
                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                failed_count = sum(
                    1
                    for data in processed_urls_data.values()
                    if data["status"] in ["failed", "error"]
                )
                if failed_count > 0:
                    logger.info(
                        f"üîÑ –ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {failed_count} –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL"
                    )

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
            processed_urls_success = set()
            processed_urls_data = {}

    # –§–∏–ª—å—Ç—Ä—É–µ–º URL - –∏—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
    urls_to_process = []
    retry_urls = []

    for url in urls:
        if url in processed_urls_success:
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        elif url in processed_urls_data:
            # URL –µ—Å—Ç—å –≤ –±–∞–∑–µ, –Ω–æ —Å—Ç–∞—Ç—É—Å –Ω–µ 'success' - –¥–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            retry_urls.append(url)
            urls_to_process.append(url)
        else:
            # –ù–æ–≤—ã–π URL
            urls_to_process.append(url)

    skipped_count = len(urls) - len(urls_to_process)

    if skipped_count > 0:
        logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {skipped_count} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL")

    if retry_urls:
        logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(retry_urls)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL")

    if not urls_to_process:
        logger.info("‚úÖ –í—Å–µ URL —É–∂–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return 0

    logger.info(
        f"üîÑ –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(urls_to_process)} URL ({len(urls_to_process) - len(retry_urls)} –Ω–æ–≤—ã—Ö + {len(retry_urls)} –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö)"
    )

    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    newly_processed_urls = []

    # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    processed_lock = Lock()
    log_lock = Lock()

    # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    processed_counter = {"count": 0}
    counter_lock = Lock()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ URL
    def process_url(url):
        try:
            # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤–æ–∑–º–æ–∂–Ω–æ URL –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ –¥—Ä—É–≥–æ–º –ø–æ—Ç–æ–∫–µ
            with processed_lock:
                if url in processed_urls_success:
                    return False

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            src = make_request(url)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä
            result = scrap_online(src, brand_directory)

            if result:
                # URL —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                with processed_lock:
                    newly_processed_urls.append(
                        {
                            "url": url,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "status": "success",
                        }
                    )
                    processed_urls_success.add(url)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ set —É—Å–ø–µ—à–Ω—ã—Ö

                    # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –ª–æ–≥–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                    if (
                        url in processed_urls_data
                        and processed_urls_data[url]["status"] != "success"
                    ):
                        old_status = processed_urls_data[url]["status"]
                        logger.info(
                            f"üîÑ URL {url} - —Å—Ç–∞—Ç—É—Å –∏–∑–º–µ–Ω–µ–Ω —Å '{old_status}' –Ω–∞ 'success'"
                        )

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
                with counter_lock:
                    processed_counter["count"] += 1
                    count = processed_counter["count"]

                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
                    if count % 50 == 0:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 URL
                        save_processed_urls(
                            mapping_file, newly_processed_urls, processed_lock
                        )
                        logger.info(
                            f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {count}/{len(urls_to_process)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"
                        )

                return True
            else:
                # –ù–µ—É–¥–∞—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                with processed_lock:
                    newly_processed_urls.append(
                        {
                            "url": url,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "status": "failed",
                        }
                    )
                return False

        except Exception as e:
            with log_lock:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
            with processed_lock:
                newly_processed_urls.append(
                    {
                        "url": url,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "status": "error",
                        "error": str(e),
                    }
                )
            return False

    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    start_time = time.time()
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ {threads} –ø–æ—Ç–æ–∫–æ–≤")

    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        results = list(executor.map(process_url, urls_to_process))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
    save_processed_urls(mapping_file, newly_processed_urls, processed_lock)

    end_time = time.time()
    total_time = end_time - start_time
    success_count = sum(1 for r in results if r)

    logger.info(
        f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count}/{len(urls_to_process)} —É—Å–ø–µ—à–Ω–æ"
    )
    logger.info(f"‚è±Ô∏è –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω—ã—Ö –≤ –±–∞–∑–µ: {len(processed_urls_success)} URL")

    return success_count


def save_processed_urls(mapping_file, newly_processed_urls, lock):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –≤ CSV —Ñ–∞–π–ª
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ

    Args:
        mapping_file (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–∞–ø–ø–∏–Ω–≥–∞
        newly_processed_urls (list): –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö URL
        lock (Lock): –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    """
    try:
        with lock:
            if not newly_processed_urls:
                return

            # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö URL
            new_df = pd.DataFrame(newly_processed_urls)

            # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            if mapping_file.exists():
                try:
                    existing_df = pd.read_csv(mapping_file)

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)

                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL, –æ—Å—Ç–∞–≤–ª—è—è –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å (—Å–∞–º—É—é —Å–≤–µ–∂—É—é)
                    combined_df = combined_df.drop_duplicates(
                        subset=["url"], keep="last"
                    )

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞: {e}")
                    combined_df = new_df
            else:
                combined_df = new_df

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            combined_df.to_csv(mapping_file, index=False)
            logger.debug(
                f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(combined_df)} –∑–∞–ø–∏—Å–µ–π –≤ {mapping_file.name}"
            )

            # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            newly_processed_urls.clear()

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")


def main(base_url, threads):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    load_proxies()

    try:
        # –°–æ–±–∏—Ä–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É
        collect_segment_urls(base_url, threads, max_results=10000)

        logger.info("üéâ –í–°–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        return False
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description="–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL eBay")
    parser.add_argument(
        "--base_url",
        type=str,
        default="https://www.ebay.com",
        help="–ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏",
    )
    parser.add_argument("--threads", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤")
    parser.add_argument("--count", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")

    args = parser.parse_args()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.threads <= 0:
        parser.error("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")
    if args.count <= 0:
        parser.error("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")

    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    count = 0
    while count < args.count:
        try:
            logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {count + 1} –∏–∑ {args.count}")
            if main(args.base_url, args.threads):
                logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã")
                break
            count += 1
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            logger.info("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥...")
            time.sleep(10)
    else:
        logger.info(f"üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({args.count})")
