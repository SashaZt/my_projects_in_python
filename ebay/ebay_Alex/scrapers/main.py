import concurrent.futures
import json
import random
import re
import time
from pathlib import Path
from threading import Lock

import pandas as pd
import requests
import urllib3
from bs4 import BeautifulSoup
from config.logger import logger
from requests.exceptions import HTTPError
from scrap import scrap_online
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
current_directory = Path.cwd()
data_directory = current_directory / "data"
config_directory = current_directory / "config"
progress_directory = current_directory / "progress"
temp_directory = current_directory / "temp"
json_directory = temp_directory / "json"

html_directory = temp_directory / "html"
config_directory = current_directory / "config"
temp_directory.mkdir(parents=True, exist_ok=True)
json_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)


config_file = config_directory / "config.json"

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤
headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
    "sec-ch-ua-full-version": '"135.0.7049.115"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-model": '""',
    "sec-ch-ua-platform": '"Windows"',
    "sec-ch-ua-platform-version": '"19.0.0"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
}
cookies = {
    "__uzma": "5dd6f251-16bd-43eb-adf6-a5fc68d31ba5",
    "__uzmb": "1747417341",
    "__uzme": "1319",
    "_fbp": "fb.1.1747646769982.102243040132258771",
    "_scid": "cuIKE6Gb42pNRDXP15V5qWD3xpGVppFH",
    "_gcl_au": "1.1.634085916.1747646770",
    "_pin_unauth": "dWlkPVkyUTNPVE16TURRdE1UZGxaQzAwT0dNekxXSmxZV010WldOak1HTTJZV1V3TURZMA",
    "_ScCbts": "%5B%5D",
    "__ssds": "2",
    "__ssuzjsr2": "a9be0cd8e",
    "__uzmaj2": "7771b6a6-eefb-4d2a-a09d-f7fb10017a47",
    "__uzmbj2": "1748265263",
    "__uzmcj2": "443031050084",
    "__uzmdj2": "1748265263",
    "__uzmlj2": "zjLRJT/evn5PESJdSC48NwLEf2CBMPC0tH+ijBtVZrw=",
    "__uzmfj2": "7f600089ee2927-1811-4b12-b3dc-d0f942520fff17482652639160-f46a16705fbe7ee410",
    "_uetsid": "e25c1d803a3211f090abb9d7ff70d114|917pju|2|fw8|0|1972",
    "_uetvid": "4cc0c570349311f09a166b26e8c6d101|j6jwo6|1748265274665|5|1|bat.bing.com/p/insights/c/f",
    "ak_bmsc": "767FF8D816C408BED2CCC6AA0201713F~000000000000000000000000000000~YAAQbEx1aOR6wfaWAQAADEHDDRtLfxAtDj33sKlK78O4LUQcdBFOVSyyjvJRTn/drlzQxgZ12JybDFrGSC5L55p23YiCcNbwxMga/B3AP4DRy2R8/0JOaoMr0QTwBSu+6xDzVcJswPgNG/u7bFyEZNZJjLyzIi8UGklEK/7VV4UBPrK0Dj4PznRyMh0kpaZ2nAHdNBFC22PQqECeEFaXy/rWN2vixKzIzvsJ6u8KMF3ihw5pcjIfYofsLxGkPT/QXcaEe3mBiuu2L6uXN8VmTaoxfbYBs2C0KkCDaHE4RlRf8l+CiGFVH479c/qMkYNwRqcVt21snTEBqc320xYorghCow0MoVDOn7yZvJaklfTgp0Gz23cHhjKMxTcj1Tqp5EtL2rnQsr0=",
    "utag_main__sn": "8",
    "_scid_r": "j2IKE6Gb42pNRDXP15V5qWD3xpGVppFHko2G-w",
    "_rdt_uuid": "1747646770138.eb01c0f4-d653-4e0e-9119-5db930f013a4",
    "cto_bundle": "QG9_X19IM05DWEZjaThLMDRDRlBFSnQ1ZGZmUjJZcWFNWE1sM1RuTldXVFFmUFhBazBCMXFxU0lDWW8lMkYxdjRBdGVrRGJXU2FEa2cwM1d4YW8xTVhMNGJXTzk2enlVSFV2RjM0UnFFOTd4bG1nS1kwQ290V1VjOTZVUU92VWtBYkZQTnhGSWtVVzE0NW1PJTJGMHloMUU0dWhVOUtFWXZWcnBEdGM3c25zalAzb0MyMVhZTVBvbU80UmVzNVY3bmxzYUVtWGFPSHIlMkZZQkx4aGFaJTJCR1BqR0g5SiUyQkZoUSUzRCUzRA",
    "s": "CgADuAKloNbqrMTQGaHR0cHM6Ly93d3cuZWJheS5jb20vYi9CTVctQ2FyLWFuZC1UcnVjay1FQ1VzLzMzNTk2L2JuXzU3NDk4OT9Db3VudHJ5JTI1MkZSZWdpb24lMjUyMG9mJTI1MjBNYW51ZmFjdHVyZT1BdXN0cmFsaWEmTEhfSXRlbUNvbmRpdGlvbj0zMDAwJlR5cGU9JTIxJl91ZGhpPTc1Jm1hZz0xJnJ0PW5jBwD4ACBoNge8ZGEzMDllNDYxOTYwYWI3MmEzZTRhMWMzZmZmN2ZhZWVGN8WG",
    "__uzmc": "605359415463",
    "__uzmd": "1748286525",
    "__uzmf": "7f600089ee2927-1811-4b12-b3dc-d0f942520fff1747417341482869183688-f03ba45e2b53978694",
    "__gads": "ID=23db798a068a3c28:T=1747417343:RT=1748286526:S=ALNI_MYw30Zys92W6lwpOzLctDwMCzY9Gw",
    "__eoi": "ID=57cc6f271819f5ee:T=1747417343:RT=1748286526:S=AA-AfjYAnNJe27z1lol8qcfzopMj",
    "ebay": "%5Ejs%3D1%5Esbf%3D%23000000%5E",
    "bm_sv": "ED4CBFA2F3C0778C0D585778F777E338~YAAQZkx1aK/mYdGWAQAASR0ADhsO0va6bOcVq9JTj5i82if2afIZePNeWSF/6W4AO38sSzntAtfGNFG9ULaV9VTZfQ+qhjZ8ClkaNjsG8glm37aSqhli4lRwC8OcxVn3OEWB6Hz0yD/OdNDRFz7OGvRE8oo7Vd4DdhBiQvnZiK+jen0vMebdTG3bdYcOJI8mMQxlAOdz2jNAftvM2DC1G7fxmdjNEPOdl9zs+aWf1FMiKIXsJQRyumoaJWlbBhU=~1",
    "dp1": "bpbf/%23e000000000000000006a15f028^bl/UA6bf723a8^",
    "nonsession": "BAQAAAZZay1gcAAaAADMABGoV8CgsUE9MAMoAIGv3I6hkYTMwOWU0NjE5NjBhYjcyYTNlNGExYzNmZmY3ZmFlZQDLAAJoNMOwMzMyKKYGCGkeVo1SLa4/ZyQwYNxJ4Q**",
}
# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏
proxy_list = []


def load_proxies():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json"""
    global proxy_list
    try:
        if config_file.exists():
            with open(config_file, "r", encoding="utf-8") as f:
                config = json.load(f)
                if "proxy_server" in config and isinstance(
                    config["proxy_server"], list
                ):
                    proxy_list = config["proxy_server"]
                    logger.info(
                        f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json"
                    )
                else:
                    logger.warning("–í config.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤")
        else:
            logger.warning("–§–∞–π–ª config.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏: {str(e)}")


def get_random_proxy():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞"""
    if not proxy_list:
        return None
    proxy_url = random.choice(proxy_list)
    proxy_url = proxy_url.strip()
    return {"http": proxy_url, "https": proxy_url}


@retry(
    stop=stop_after_attempt(10),
    wait=wait_fixed(10),
    retry=retry_if_exception_type(
        (HTTPError, requests.exceptions.ConnectionError, requests.exceptions.Timeout)
    ),
)
def make_request(url):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å"""
    try:
        proxies = get_random_proxy()
        # proxies = {
        #     "http": "http://scraperapi:6c54502fd688c7ce737f1c650444884a@proxy-server.scraperapi.com:8001",
        #     "https": "http://scraperapi:6c54502fd688c7ce737f1c650444884a@proxy-server.scraperapi.com:8001",
        # }
        response = requests.get(
            url,
            cookies=cookies,
            proxies=proxies,
            headers=headers,
            timeout=30,
            verify=False,
        )
        response.raise_for_status()
        return response.text
    except requests.exceptions.ProxyError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è {url}: {e}")
        raise

    except requests.exceptions.ConnectionError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è {url}: {e}")
        raise

    except requests.exceptions.Timeout as e:
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        raise

    except requests.exceptions.HTTPError as e:
        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {url}: {e}")
        raise

    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
        raise


def make_soup(html_content):
    """–°–æ–∑–¥–∞–µ—Ç BeautifulSoup –æ–±—ä–µ–∫—Ç"""
    return BeautifulSoup(html_content, "lxml")


def extract_filter_options(soup, filter_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ü–∏–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞"""
    options = []
    try:
        table = soup.find("section", attrs={"id": "brw-refinement-root"})
        filter_containers = table.find_all("span", class_="filter-menu-button")

        target_container = None
        for container in filter_containers:
            filter_label = container.find("span", class_="filter-label")
            if filter_label and filter_label.get_text().strip() == filter_name:
                target_container = container
                break

        if not target_container:
            logger.warning(f"‚ö†Ô∏è  –§–∏–ª—å—Ç—Ä '{filter_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return options

        option_items = target_container.find_all("li", class_="brwr__inputs")
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(option_items)} –æ–ø—Ü–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ '{filter_name}'")

        for item in option_items:
            link = item.find("a", class_="brwr__inputs__actions")
            if not link:
                continue

            href = link.get("href")
            if not href:
                continue

            option_span = link.find("span", class_="textual-display")
            if not option_span:
                continue

            option_name = option_span.get_text().strip()
            clean_url = href.replace("&amp;", "&")

            options.append({"name": option_name, "url": clean_url})

        return options
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞ '{filter_name}': {e}")
        return []


def get_results_count(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        count_element = soup.find("h2", class_="textual-display brw-controls__count")

        if not count_element:
            count_element = soup.find("span", class_="brw-controls__count")

        if count_element:
            count_text = count_element.get_text()
            match = re.search(r"([\d,]+)", count_text)
            if match:
                count_str = match.group(1).replace(",", "")
                count = int(count_str)
                return count

        return 0
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: {e}")
        return 0


def get_final_segment_urls(url, applied_filters=None, max_results=10000):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç URL –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã ‚â§ max_results
    –°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤

    Args:
        url (str): URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        applied_filters (list): –£–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        max_results (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ

    Yields:
        tuple: (url, brand_name) - –§–∏–Ω–∞–ª—å–Ω—ã–µ URL —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
    """
    if applied_filters is None:
        applied_filters = []

    filter_path = " ‚Üí ".join(applied_filters) if applied_filters else "–ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
    logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º: {filter_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    response_text = make_request(url)
    if not response_text:
        return

    soup = make_soup(response_text)
    count = get_results_count(soup)

    logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {count:,}")

    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–µ–º–ª–µ–º–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º URL —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –±—Ä–µ–Ω–¥–µ
    if count <= max_results:
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç: {count:,} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        brand_name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                # –û—á–∏—â–∞–µ–º –∏–º—è –±—Ä–µ–Ω–¥–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–∞–ø–∫–∏
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
    logger.info(
        f"‚ö†Ô∏è  –ú–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({count:,} > {max_results:,}). –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é."
    )

    filters_sequence = [
        "Brand",
        "Condition",
        "Price",
        "Type",
        "Brand Type",
        "Programming Required",
        "Country/Region of Manufacture",
    ]

    # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π —Ñ–∏–ª—å—Ç—Ä
    next_filter = None
    for filter_name in filters_sequence:
        if not any(filter_name in af for af in applied_filters):
            next_filter = filter_name
            break

    if not next_filter:
        logger.warning(
            f"‚ö†Ô∏è  –í—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {count:,}. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å."
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        brand_name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    logger.info(f"üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä: {next_filter}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ü–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞
    filter_options = extract_filter_options(soup, next_filter)

    if not filter_options:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–ø—Ü–∏–∏ –¥–ª—è '{next_filter}'")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
        brand_name = "no_brand"
        for filter_item in applied_filters:
            if filter_item.startswith("Brand: "):
                brand_name = filter_item.replace("Brand: ", "").strip()
                brand_name = re.sub(r'[<>:"/\\|?*]', "_", brand_name)
                break

        yield url, brand_name
        return

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –æ–ø—Ü–∏—é
    for i, option in enumerate(filter_options):
        option_name = option["name"]
        option_url = option["url"]

        logger.info(f"üìå –û–ø—Ü–∏—è {i+1}/{len(filter_options)}: {option_name}")

        new_applied_filters = applied_filters + [f"{next_filter}: {option_name}"]

        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ–º URL –∏–∑ —ç—Ç–æ–π –æ–ø—Ü–∏–∏
        yield from get_final_segment_urls(option_url, new_applied_filters, max_results)


def scrape_page(full_url, params=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã

    Args:
        url (str): –ë–∞–∑–æ–≤—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        tuple: (next_url, page_hrefs) - URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    """
    page_hrefs = []

    try:

        logger.info(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º: {full_url}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        src = make_request(full_url)
        soup = BeautifulSoup(src, "lxml")

        # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ JSON-LD
        json_ld_scripts = soup.find_all("script", {"type": "application/ld+json"})

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫ JSON-LD
        for script in json_ld_scripts:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–≥–∞ script
                json_data = json.loads(script.string)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
                if (
                    "about" in json_data
                    and "offers" in json_data["about"]
                    and "itemOffered" in json_data["about"]["offers"]
                ):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
                    items = json_data["about"]["offers"]["itemOffered"]

                    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —Ç–æ–≤–∞—Ä—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º URL
                    for item in items:
                        if "url" in item:
                            # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                            item_url = item["url"]
                            if "?" in item_url:
                                cleaned_url = item_url.split("?")[0]
                                page_hrefs.append(cleaned_url)
                            else:
                                page_hrefs.append(item_url)
            except json.JSONDecodeError:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON-LD")
                continue
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON-LD: {str(e)}")
                continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_button = soup.select_one("a.pagination__next")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–Ω–æ–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏–º–µ–µ—Ç –∞—Ç—Ä–∏–±—É—Ç href –∏ –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω–∞
        if next_button and "href" in next_button.attrs:
            disabled_next = soup.select_one('a.pagination__next[aria-disabled="true"]')

            if not disabled_next:
                next_url = next_button["href"]
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {next_url}")
                return next_url, page_hrefs

        return None, page_hrefs
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {full_url}: {str(e)}")
        return None, []


def collect_segment_urls(base_url, max_results=10000):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ URL —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –ø–æ –æ–¥–Ω–æ–º—É
    –°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤

    Args:
        base_url (str): –ë–∞–∑–æ–≤—ã–π URL eBay
        max_results (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
    """
    logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –°–ë–û–† –ò –û–ë–†–ê–ë–û–¢–ö–£ –°–ï–ì–ú–ï–ù–¢–û–í")
    logger.info("=" * 60)
    logger.info(f"üéØ –¶–µ–ª—å: URL —Å —Ç–æ–≤–∞—Ä–∞–º–∏ ‚â§ {max_results:,}")
    logger.info("=" * 60)

    segment_counter = 0

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π URL —Å—Ä–∞–∑—É –∫–∞–∫ —Ç–æ–ª—å–∫–æ –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    for segment_url, brand_name in get_final_segment_urls(
        base_url, max_results=max_results
    ):
        segment_counter += 1
        logger.info(
            f"üîó –ü–æ–ª—É—á–µ–Ω —Å–µ–≥–º–µ–Ω—Ç #{segment_counter} –¥–ª—è –±—Ä–µ–Ω–¥–∞ '{brand_name}': {segment_url}"
        )

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –±—Ä–µ–Ω–¥–∞
        brand_directory = json_directory / brand_name
        brand_directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ –ø–∞–ø–∫–∞: {brand_directory}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç —Å—Ä–∞–∑—É
        process_single_segment(
            segment_url, segment_counter, brand_name, brand_directory
        )

    logger.info(f"‚úÖ –ì–û–¢–û–í–û! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {segment_counter} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")


def process_single_segment(
    segment_url, segment_number, brand_name, brand_directory, threads=40
):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç - –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–≤–∞—Ä—ã

    Args:
        segment_url (str): URL —Å–µ–≥–º–µ–Ω—Ç–∞
        segment_number (int): –ù–æ–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        brand_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
        brand_directory (Path): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –±—Ä–µ–Ω–¥–∞
        threads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
    """
    logger.info(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –°–ï–ì–ú–ï–ù–¢–ê #{segment_number}")
    logger.info(f"üìå URL: {segment_url}")

    total_processed_urls = 0
    total_processed_pages = 0
    current_page = 1
    next_url = None

    # –¶–∏–∫–ª –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Ç–µ–∫—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    while True:
        logger.info(f"üìÑ –°–µ–≥–º–µ–Ω—Ç #{segment_number} | –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_page == 1:
            # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π URL —Å–µ–≥–º–µ–Ω—Ç–∞
            page_url = segment_url
            next_url, page_hrefs = scrape_page(page_url)
        else:
            # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if next_url is None:
                # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏, —Ñ–æ—Ä–º–∏—Ä—É–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                separator = "&" if "?" in segment_url else "?"
                page_url = f"{segment_url}{separator}_pgn={current_page}"
                if "rt=nc" not in page_url:
                    page_url += "&rt=nc"
                next_url, page_hrefs = scrape_page(page_url)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_url, page_hrefs = scrape_page(next_url)

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        total_processed_pages += 1

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        if page_hrefs:
            logger.info(
                f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_hrefs)} —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}"
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤
            try:
                success_count = get_product_th(
                    page_hrefs, brand_directory, threads=threads
                )
                total_processed_urls += success_count

                logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {success_count}/{len(page_hrefs)} —Ç–æ–≤–∞—Ä–æ–≤")
                logger.info(
                    f"üìä –°–µ–≥–º–µ–Ω—Ç #{segment_number} | –í—Å–µ–≥–æ: {total_processed_urls} —Ç–æ–≤–∞—Ä–æ–≤, {total_processed_pages} —Å—Ç—Ä–∞–Ω–∏—Ü"
                )

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–≤–∞—Ä–æ–≤: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if next_url:
            current_page += 1

        else:
            logger.info(
                f"üèÅ –°–µ–≥–º–µ–Ω—Ç #{segment_number} –∑–∞–≤–µ—Ä—à–µ–Ω | {total_processed_pages} —Å—Ç—Ä–∞–Ω–∏—Ü | {total_processed_urls} —Ç–æ–≤–∞—Ä–æ–≤"
            )
            break

    return {
        "segment_number": segment_number,
        "brand_name": brand_name,
        "total_pages": total_processed_pages,
        "total_products": total_processed_urls,
    }


def get_product_th(urls, brand_directory, threads=40):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ.
    –í–µ–¥–µ—Ç —É—á–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ.
    –ü–æ–≤—Ç–æ—Ä–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL —Å–æ —Å—Ç–∞—Ç—É—Å–∞–º–∏ 'failed' –∏ 'error'.

    Args:
        urls (list): –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        brand_directory (Path): –ü–∞–ø–∫–∞ –±—Ä–µ–Ω–¥–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        threads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 40.

    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ URL
    if not urls:
        logger.warning("–°–ø–∏—Å–æ–∫ URL –ø—É—Å—Ç")
        return 0

    total_urls = len(urls)
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {total_urls} —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤")

    # –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    mapping_file = brand_directory / "processed_urls.csv"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL –∏–∑ —Ñ–∞–π–ª–∞
    processed_urls_success = set()  # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    processed_urls_data = {}  # –î–∞–Ω–Ω—ã–µ –æ–±–æ –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL

    if mapping_file.exists():
        try:
            existing_df = pd.read_csv(mapping_file)
            if "url" in existing_df.columns:
                for _, row in existing_df.iterrows():
                    url = row["url"]
                    status = row.get("status", "unknown")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ URL
                    processed_urls_data[url] = {
                        "status": status,
                        "timestamp": row.get("timestamp", ""),
                        "error": row.get("error", ""),
                    }

                    # –¢–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ URL –¥–æ–±–∞–≤–ª—è–µ–º –≤ set –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞
                    if status == "success":
                        processed_urls_success.add(url)

                logger.info(
                    f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_urls_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {mapping_file.name}"
                )
                logger.info(f"‚úÖ –ò–∑ –Ω–∏—Ö —É—Å–ø–µ—à–Ω—ã—Ö: {len(processed_urls_success)}")

                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                failed_count = sum(
                    1
                    for data in processed_urls_data.values()
                    if data["status"] in ["failed", "error"]
                )
                if failed_count > 0:
                    logger.info(
                        f"üîÑ –ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {failed_count} –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL"
                    )

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
            processed_urls_success = set()
            processed_urls_data = {}

    # –§–∏–ª—å—Ç—Ä—É–µ–º URL - –∏—Å–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
    urls_to_process = []
    retry_urls = []

    for url in urls:
        if url in processed_urls_success:
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        elif url in processed_urls_data:
            # URL –µ—Å—Ç—å –≤ –±–∞–∑–µ, –Ω–æ —Å—Ç–∞—Ç—É—Å –Ω–µ 'success' - –¥–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            retry_urls.append(url)
            urls_to_process.append(url)
        else:
            # –ù–æ–≤—ã–π URL
            urls_to_process.append(url)

    skipped_count = len(urls) - len(urls_to_process)

    if skipped_count > 0:
        logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {skipped_count} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL")

    if retry_urls:
        logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(retry_urls)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL")

    if not urls_to_process:
        logger.info("‚úÖ –í—Å–µ URL —É–∂–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return 0

    logger.info(
        f"üîÑ –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(urls_to_process)} URL ({len(urls_to_process) - len(retry_urls)} –Ω–æ–≤—ã—Ö + {len(retry_urls)} –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö)"
    )

    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    newly_processed_urls = []

    # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    processed_lock = Lock()
    log_lock = Lock()

    # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
    processed_counter = {"count": 0}
    counter_lock = Lock()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ URL
    def process_url(url):
        try:
            # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤–æ–∑–º–æ–∂–Ω–æ URL –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ –¥—Ä—É–≥–æ–º –ø–æ—Ç–æ–∫–µ
            with processed_lock:
                if url in processed_urls_success:
                    return False

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            src = make_request(url)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä
            result = scrap_online(src, brand_directory)

            if result:
                # URL —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                with processed_lock:
                    newly_processed_urls.append(
                        {
                            "url": url,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "status": "success",
                        }
                    )
                    processed_urls_success.add(url)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ set —É—Å–ø–µ—à–Ω—ã—Ö

                    # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –ª–æ–≥–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                    if (
                        url in processed_urls_data
                        and processed_urls_data[url]["status"] != "success"
                    ):
                        old_status = processed_urls_data[url]["status"]
                        logger.info(
                            f"üîÑ URL {url} - —Å—Ç–∞—Ç—É—Å –∏–∑–º–µ–Ω–µ–Ω —Å '{old_status}' –Ω–∞ 'success'"
                        )

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
                with counter_lock:
                    processed_counter["count"] += 1
                    count = processed_counter["count"]

                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
                    if count % 50 == 0:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 URL
                        save_processed_urls(
                            mapping_file, newly_processed_urls, processed_lock
                        )
                        logger.info(
                            f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {count}/{len(urls_to_process)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"
                        )

                return True
            else:
                # –ù–µ—É–¥–∞—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                with processed_lock:
                    newly_processed_urls.append(
                        {
                            "url": url,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "status": "failed",
                        }
                    )
                return False

        except Exception as e:
            with log_lock:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
            with processed_lock:
                newly_processed_urls.append(
                    {
                        "url": url,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "status": "error",
                        "error": str(e),
                    }
                )
            return False

    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    start_time = time.time()
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ {threads} –ø–æ—Ç–æ–∫–æ–≤")

    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        results = list(executor.map(process_url, urls_to_process))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
    save_processed_urls(mapping_file, newly_processed_urls, processed_lock)

    end_time = time.time()
    total_time = end_time - start_time
    success_count = sum(1 for r in results if r)

    logger.info(
        f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count}/{len(urls_to_process)} —É—Å–ø–µ—à–Ω–æ"
    )
    logger.info(f"‚è±Ô∏è –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –í—Å–µ–≥–æ —É—Å–ø–µ—à–Ω—ã—Ö –≤ –±–∞–∑–µ: {len(processed_urls_success)} URL")

    return success_count


def save_processed_urls(mapping_file, newly_processed_urls, lock):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –≤ CSV —Ñ–∞–π–ª
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ

    Args:
        mapping_file (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–∞–ø–ø–∏–Ω–≥–∞
        newly_processed_urls (list): –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö URL
        lock (Lock): –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    """
    try:
        with lock:
            if not newly_processed_urls:
                return

            # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö URL
            new_df = pd.DataFrame(newly_processed_urls)

            # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            if mapping_file.exists():
                try:
                    existing_df = pd.read_csv(mapping_file)

                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)

                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL, –æ—Å—Ç–∞–≤–ª—è—è –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å (—Å–∞–º—É—é —Å–≤–µ–∂—É—é)
                    combined_df = combined_df.drop_duplicates(
                        subset=["url"], keep="last"
                    )

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞: {e}")
                    combined_df = new_df
            else:
                combined_df = new_df

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            combined_df.to_csv(mapping_file, index=False)
            logger.debug(
                f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(combined_df)} –∑–∞–ø–∏—Å–µ–π –≤ {mapping_file.name}"
            )

            # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            newly_processed_urls.clear()

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")


# def get_product_th(urls, brand_directory, threads=40):
#     """
#     –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ.
#     –í–µ–¥–µ—Ç —É—á–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ.

#     Args:
#         urls (list): –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
#         brand_directory (Path): –ü–∞–ø–∫–∞ –±—Ä–µ–Ω–¥–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
#         threads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.

#     Returns:
#         int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
#     """
#     # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ URL
#     if not urls:
#         logger.warning("–°–ø–∏—Å–æ–∫ URL –ø—É—Å—Ç")
#         return 0

#     total_urls = len(urls)
#     logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {total_urls} —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤")

#     # –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
#     mapping_file = brand_directory / "processed_urls.csv"

#     # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL –∏–∑ —Ñ–∞–π–ª–∞
#     processed_urls = set()
#     if mapping_file.exists():
#         try:
#             existing_df = pd.read_csv(mapping_file)
#             if "url" in existing_df.columns:
#                 processed_urls = set(existing_df["url"].tolist())
#                 logger.info(
#                     f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_urls)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏–∑ {mapping_file.name}"
#                 )
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
#             processed_urls = set()

#     # –§–∏–ª—å—Ç—Ä—É–µ–º URL - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
#     urls_to_process = [url for url in urls if url not in processed_urls]
#     skipped_count = len(urls) - len(urls_to_process)

#     if skipped_count > 0:
#         logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {skipped_count} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL")

#     if not urls_to_process:
#         logger.info("‚úÖ –í—Å–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
#         return 0

#     logger.info(f"üîÑ –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(urls_to_process)} –Ω–æ–≤—ã—Ö URL")

#     # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
#     newly_processed_urls = []

#     # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
#     processed_lock = Lock()
#     log_lock = Lock()

#     # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
#     processed_counter = {"count": 0}
#     counter_lock = Lock()

#     # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ URL
#     def process_url(url):
#         try:
#             # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤–æ–∑–º–æ–∂–Ω–æ URL –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ –¥—Ä—É–≥–æ–º –ø–æ—Ç–æ–∫–µ
#             with processed_lock:
#                 if url in processed_urls:
#                     return False

#             # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
#             src = make_request(url)

#             # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä
#             result = scrap_online(src, brand_directory)

#             if result:
#                 # –î–æ–±–∞–≤–ª—è–µ–º URL –≤ —Å–ø–∏—Å–æ–∫ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
#                 with processed_lock:
#                     newly_processed_urls.append(
#                         {
#                             "url": url,
#                             "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
#                             "status": "success",
#                         }
#                     )
#                     processed_urls.add(url)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ set –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏

#                 # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
#                 with counter_lock:
#                     processed_counter["count"] += 1
#                     count = processed_counter["count"]

#                     # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
#                     if count % 50 == 0:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 URL
#                         save_processed_urls(
#                             mapping_file, newly_processed_urls, processed_lock
#                         )
#                         logger.info(
#                             f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {count}/{len(urls_to_process)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"
#                         )

#                 return True
#             else:
#                 # –î–∞–∂–µ –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å
#                 with processed_lock:
#                     newly_processed_urls.append(
#                         {
#                             "url": url,
#                             "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
#                             "status": "failed",
#                         }
#                     )

#                 return False

#         except Exception as e:
#             with log_lock:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")

#             # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
#             with processed_lock:
#                 newly_processed_urls.append(
#                     {
#                         "url": url,
#                         "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
#                         "status": "error",
#                         "error": str(e),
#                     }
#                 )
#             return False

#     # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
#     start_time = time.time()
#     logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ {threads} –ø–æ—Ç–æ–∫–æ–≤")

#     with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
#         results = list(executor.map(process_url, urls_to_process))

#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
#     save_processed_urls(mapping_file, newly_processed_urls, processed_lock)

#     end_time = time.time()
#     total_time = end_time - start_time
#     success_count = sum(1 for r in results if r)

#     logger.info(
#         f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count}/{len(urls_to_process)} —É—Å–ø–µ—à–Ω–æ"
#     )
#     logger.info(f"‚è±Ô∏è –ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
#     logger.info(f"üìä –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(processed_urls)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL")

#     return success_count


# def save_processed_urls(mapping_file, newly_processed_urls, lock):
#     """
#     –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –≤ CSV —Ñ–∞–π–ª

#     Args:
#         mapping_file (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–∞–ø–ø–∏–Ω–≥–∞
#         newly_processed_urls (list): –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
#         lock (Lock): –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
#     """
#     try:
#         with lock:
#             if not newly_processed_urls:
#                 return

#             # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –Ω–æ–≤—ã—Ö URL
#             new_df = pd.DataFrame(newly_processed_urls)

#             # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –¥–∞–Ω–Ω—ã–º
#             if mapping_file.exists():
#                 try:
#                     existing_df = pd.read_csv(mapping_file)
#                     combined_df = pd.concat([existing_df, new_df], ignore_index=True)
#                 except Exception as e:
#                     logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞: {e}")
#                     combined_df = new_df
#             else:
#                 combined_df = new_df

#             # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å)
#             combined_df = combined_df.drop_duplicates(subset=["url"], keep="last")

#             # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
#             combined_df.to_csv(mapping_file, index=False)

#             # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
#             newly_processed_urls.clear()

#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    load_proxies()

    # –ë–∞–∑–æ–≤—ã–π URL eBay
    base_url = "https://www.ebay.com/b/Car-Truck-ECUs-Computer-Modules/33596/bn_584314?Type=AC%2520Motor%2520Controllers&mag=1&rt=nc"

    try:
        # –°–æ–±–∏—Ä–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É
        collect_segment_urls(base_url, max_results=10000)

        logger.info("üéâ –í–°–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        return False
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    while True:
        try:
            main()
            break  # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            logger.info("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥...")

    # main()
