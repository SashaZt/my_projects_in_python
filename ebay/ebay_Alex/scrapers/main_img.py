import io
import json
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import pandas as pd
import urllib3
import argparse
from requests.exceptions import (
    ConnectionError,
    HTTPError,
    ProxyError,
    RequestException,
    Timeout,
)
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)
from urllib3.exceptions import ProtocolError, ReadTimeoutError
from typing import List, Dict, Any
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import requests
from config.logger import logger
from PIL import Image
from tenacity import retry, stop_after_attempt, wait_exponential

current_directory = Path.cwd()
config_directory = current_directory / "config"
temp_directory = current_directory / "temp"
xlsx_directory = temp_directory / "xlsx"


temp_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)



product_details = xlsx_directory / "product_details.xlsx"
product_details_updated = xlsx_directory / "product_details_updated.xlsx"
config_file = config_directory / "config.json"

headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
    "sec-ch-ua-full-version": '"135.0.7049.115"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-model": '""',
    "sec-ch-ua-platform": '"Windows"',
    "sec-ch-ua-platform-version": '"19.0.0"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
}


def load_proxies():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json
    """
    global proxy_list
    try:
        if config_file.exists():
            with open(config_file, "r") as f:
                config = json.load(f)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –≤ config.json
                if "proxy_server" in config and isinstance(
                    config["proxy_server"], list
                ):
                    proxy_list = config["proxy_server"]
                    logger.info(
                        f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ config.json"
                    )
                else:
                    logger.warning("–í config.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä–æ–≤")
        else:
            logger.warning("–§–∞–π–ª config.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏: {str(e)}")


def get_random_proxy():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
    """
    if not proxy_list:
        return None

    proxy_url = random.choice(proxy_list)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ URL –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
    proxy_url = proxy_url.strip()

    return {"http": proxy_url, "https": proxy_url}


@retry(
    stop=stop_after_attempt(10),
    wait=wait_exponential(
        multiplier=1, min=4, max=60
    ),  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: 4, 8, 16, 32, 60, 60...
    retry=(
        retry_if_exception_type(HTTPError)
        | retry_if_exception_type(ConnectionError)
        | retry_if_exception_type(Timeout)
        | retry_if_exception_type(ProxyError)
        | retry_if_exception_type(ProtocolError)
        | retry_if_exception_type(ReadTimeoutError)
        | retry_if_exception_type(OSError)
    ),
)
def download_single_image(image_url, file_path, thread_id):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ WebP
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
    if file_path.exists():
        logger.info(f"–ü–æ—Ç–æ–∫ {thread_id}: –§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {file_path}")
        return True

    # –ü–æ–ª—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
    proxies = get_random_proxy()
    # logger.info(f"–ü–æ—Ç–æ–∫ {thread_id}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏ {proxies}")

    response = requests.get(
        image_url,
        proxies=proxies,
        headers=headers,
        timeout=30,
        verify=False,  # –û—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
    )
    response.raise_for_status()  # –í—ã–∑—ã–≤–∞–µ—Ç HTTPError, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –Ω–µ 200

    try:
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –±–∞–π—Ç–æ–≤
        image = Image.open(io.BytesIO(response.content))

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ (–¥–ª—è WebP —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é)
        if image.mode in ("RGBA", "LA", "P"):
            # –°–æ–∑–¥–∞–µ–º –±–µ–ª—ã–π —Ñ–æ–Ω
            background = Image.new("RGB", image.size, (255, 255, 255))
            if image.mode == "P":
                image = image.convert("RGBA")
            background.paste(
                image, mask=image.split()[-1] if image.mode == "RGBA" else None
            )
            image = background
        elif image.mode != "RGB":
            image = image.convert("RGB")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JPG
        image.save(file_path, "WEBP", quality=90)

    except Exception as e:
        logger.error(f"–ü–æ—Ç–æ–∫ {thread_id}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        # –ï—Å–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è WebP –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª
        raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ

    return True


def download_image_task(task_data):
    """
    –ó–∞–¥–∞—á–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    task_data: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –∑–∞–¥–∞—á–µ
    """
    image_url = task_data["image_url"]
    file_path = task_data["file_path"]
    product_id = task_data["product_id"]
    image_key = task_data["image_key"]
    thread_id = threading.current_thread().ident

    try:

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        download_single_image(image_url, file_path, thread_id)

        return {
            "success": True,
            "product_id": product_id,
            "image_key": image_key,
            "file_path": str(file_path),
            "relative_path": f"images/{product_id}/{file_path.name}",
        }

    except Exception as e:
        logger.error(
            f"–ü–æ—Ç–æ–∫ {thread_id}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {image_key} –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {product_id}: {e}"
        )
        return {
            "success": False,
            "product_id": product_id,
            "image_key": image_key,
            "error": str(e),
        }


def download_images_and_update_json_threaded(json_data, max_workers=5):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.

    Args:
        json_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö
        max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤

    Returns:
        list: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    load_proxies()

    if not proxy_list:
        logger.warning("–ü—Ä–æ–∫—Å–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ")

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –ø–∞–ø–∫—É –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    base_images_dir = Path("images")
    base_images_dir.mkdir(exist_ok=True)

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    download_tasks = []
    product_data_map = {}
    results = {}

    for item in json_data:
        # –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
        updated_item = item.copy()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º product_id
        product_id = item.get("product_id", "")
        if not product_id:
            continue

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
        product_data_map[product_id] = updated_item

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ç–æ–≤–∞—Ä–∞
        product_dir = base_images_dir / str(product_id)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        if product_id not in results:
            results[product_id] = {}

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_keys = ["url_image_1", "url_image_2", "url_image_3"]

        for i, key in enumerate(image_keys, 1):
            image_url = item.get(key, "")

            if image_url and image_url.strip():
                # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º .webp —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, —Ç–∞–∫ –∫–∞–∫ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                filename = f"{i:02d}.webp"

                file_path = product_dir / filename

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
                if file_path.exists():
                    logger.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {file_path}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    results[product_id][key] = f"images/{product_id}/{filename}"
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                download_tasks.append(
                    {
                        "image_url": image_url,
                        "file_path": file_path,
                        "product_id": product_id,
                        "image_key": key,
                    }
                )
            else:
                # –ï—Å–ª–∏ URL –ø—É—Å—Ç–æ–π, —Å—Ç–∞–≤–∏–º None
                results[product_id][key] = None

    logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(download_tasks)} –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        future_to_task = {
            executor.submit(download_image_task, task): task for task in download_tasks
        }

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        for future in as_completed(future_to_task):
            result = future.result()
            product_id = result["product_id"]
            image_key = result["image_key"]

            if product_id not in results:
                results[product_id] = {}

            if result["success"]:
                results[product_id][image_key] = result["relative_path"]
            else:
                # –ï—Å–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, —Å—Ç–∞–≤–∏–º None
                results[product_id][image_key] = None

    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    updated_data = []
    for product_id, product_data in product_data_map.items():
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º - –ò–°–ü–†–ê–í–õ–Ø–ï–ú –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ï –ö–õ–Æ–ß–ï–ô
        image_mapping = {
            "url_image_1": "image_1",
            "url_image_2": "image_2",
            "url_image_3": "image_3",
        }

        for url_key, local_key in image_mapping.items():
            if product_id in results and url_key in results[product_id]:
                product_data[local_key] = results[product_id][url_key]
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å—Ç–∞–≤–∏–º None
                product_data[local_key] = None

        updated_data.append(product_data)

    logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {len(updated_data)} —Ç–æ–≤–∞—Ä–æ–≤")
    return updated_data

def extract_product_data_compact(file_path):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å —É—Å–ª–æ–≤–∏—è–º–∏:
    - –ï—Å–ª–∏ –Ω–µ—Ç product_id - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É
    - –ï—Å–ª–∏ –Ω–µ—Ç url_image - —Å—Ç–∞–≤–∏–º None
    
    Args:
        file_path (str): –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å product_id)
    """
    df = pd.read_excel(file_path)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ product_id
    if 'product_id' not in df.columns:
        raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'product_id' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ")
    
    # –í—Å–µ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    all_columns = ['product_id', 'url_image_1', 'url_image_2', 'url_image_3']
    
    result = []
    
    for index, row in df.iterrows():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ product_id –∏ —á—Ç–æ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π/None
        product_id = row.get('product_id')
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ product_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—É—Å—Ç–æ–π –∏–ª–∏ NaN
        if pd.isna(product_id) or product_id is None or str(product_id).strip() == '':
            continue
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏
        product_dict = {'product_id': str(product_id).strip()}
        
        # –î–æ–±–∞–≤–ª—è–µ–º url_image –∫–æ–ª–æ–Ω–∫–∏ (None –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)
        for col in ['url_image_1', 'url_image_2', 'url_image_3']:
            if col in df.columns:
                value = row.get(col)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∏–ª–∏ –ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):
                    product_dict[col] = None
                else:
                    product_dict[col] = str(value).strip()
            else:
                product_dict[col] = None
        
        result.append(product_dict)
    
    return result

def process_json_file_threaded(input_file,max_workers):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç JSON —Ñ–∞–π–ª: —Å–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π JSON.

    Args:
        input_file: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
        output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
        max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ
        # with open(input_file, "r", encoding="utf-8") as f:
        #     json_data = json.load(f)
        
        xlsx_data = extract_product_data_compact(input_file)

        # logger.info(xlsx_data)
        # exit()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        updated_data = download_images_and_update_json_threaded(xlsx_data, max_workers)
        update_excel_with_images(
            product_details, 
            updated_data, 
            product_details_updated
        )

    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    except json.JSONDecodeError:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ JSON –∏–∑ —Ñ–∞–π–ª–∞ {input_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")

def update_excel_with_images(file_path: str, products_data: List[Dict[str, Any]], output_path: str = None):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç Excel —Ñ–∞–π–ª, –¥–æ–±–∞–≤–ª—è—è –∫–æ–ª–æ–Ω–∫–∏ image_1, image_2, image_3 
    –∏ –∑–∞–ø–æ–ª–Ω—è—è –¥–∞–Ω–Ω—ã–µ –ø–æ product_id
    
    Args:
        file_path (str): –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É Excel —Ñ–∞–π–ª—É
        products_data (List[Dict]): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        output_path (str, optional): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ï—Å–ª–∏ None, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
    """
    try:
        # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π Excel —Ñ–∞–π–ª
        df = pd.read_excel(file_path)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        new_columns = ['image_1', 'image_2', 'image_3']
        for col in new_columns:
            if col not in df.columns:
                df[col] = None
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ product_id
        products_dict = {str(item['product_id']): item for item in products_data}
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        updated_count = 0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º vectorized –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        for index, row in df.iterrows():
            product_id = str(row['product_id'])
            
            if product_id in products_dict:
                product_data = products_dict[product_id]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ image_1, image_2, image_3
                for img_col in ['image_1', 'image_2', 'image_3']:
                    if img_col in product_data and product_data[img_col]:
                        df.at[index, img_col] = product_data[img_col]
                
                updated_count += 1
                logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–¥—É–∫—Ç —Å ID: {product_id}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        save_path = output_path if output_path else file_path
        df.to_excel(save_path, index=False)
        
        logger.info(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {updated_count}")
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {len(df)}")
        
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return False

def add_image_columns_only(file_path: str, output_path: str = None):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ image_1, image_2, image_3 –±–µ–∑ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        file_path (str): –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
        output_path (str, optional): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    try:
        df = pd.read_excel(file_path)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        new_columns = ['image_1', 'image_2', 'image_3']
        for col in new_columns:
            if col not in df.columns:
                df[col] = None
                logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {col}")
        
        save_path = output_path if output_path else file_path
        df.to_excel(save_path, index=False)
        
        logger.info(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Å –Ω–æ–≤—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏: {save_path}")
        return True
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return False

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL eBay")
    parser.add_argument("--max_workers", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤")
    parser.add_argument("--count", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)")

    args = parser.parse_args()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.max_workers <= 0:
        parser.error("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")
    if args.count <= 0:
        parser.error("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")


    
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    count = 0
    while count < args.count:
        try:
            logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {count + 1} –∏–∑ {args.count}")
            if process_json_file_threaded(product_details, args.max_workers):
                logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã")
                break
            count += 1
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            logger.info("üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥...")
    else:
        logger.info(f"üõë –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({args.count})")
