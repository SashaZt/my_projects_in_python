import csv
import json
import re
import sys
import time
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import parse_qs, urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_search_directory = current_directory / "html_search"
html_product_directory = current_directory / "html_product"
config_directory = current_directory / "config"
log_directory = current_directory / "log"
log_directory.mkdir(parents=True, exist_ok=True)
html_product_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)

output_json_file = data_directory / "output.json"
output_xlsx_file = data_directory / "output.xlsx"
output_csv_file = data_directory / "output.csv"
log_file_path = log_directory / "log_message.log"
CONFIG_PATH = config_directory / "config.json"


BASE_URL = "https://auburnmaine.patriotproperties.com/"
logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def load_config():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ JSON-—Ñ–∞–π–ª–∞"""

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if not CONFIG_PATH.exists():
            logger.error(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {CONFIG_PATH}")
            logger.error("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–∞.")
            sys.exit(1)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = json.load(f)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
        required_sections = ["cookies"]
        for section in required_sections:
            if section not in config:
                logger.error(
                    f"–û—à–∏–±–∫–∞: –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–¥–µ–ª '{section}'"
                )
                sys.exit(1)

        return config

    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)


def get_html():
    timeout = 60
    max_attempts = 10
    delay_seconds = 5
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    config = load_config()
    cookies = config["cookies"]
    headers = config["headers"]
    data = config["data"]
    for page in range(1, 194):
        output_html_file = html_search_directory / f"auburnmaine_0{page}.html"

        for attempt in range(max_attempts):
            try:
                if page == 1:
                    response = requests.post(
                        "https://auburnmaine.patriotproperties.com/SearchResults.asp",
                        cookies=cookies,
                        headers=headers,
                        data=data,
                        timeout=timeout,
                    )
                else:
                    params = {
                        "page": page,
                    }
                    response = requests.get(
                        "https://auburnmaine.patriotproperties.com/SearchResults.asp",
                        params=params,
                        cookies=cookies,
                        headers=headers,
                        timeout=timeout,
                    )

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
                if response.status_code == 200:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ü–µ–ª–∏–∫–æ–º
                    with open(output_html_file, "w", encoding="utf-8") as file:
                        file.write(response.text)
                    logger.info(f"Successfully saved {output_html_file}")
                    break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                else:
                    logger.warning(
                        f"Attempt {attempt + 1} failed for page {page} with status {response.status_code}"
                    )
                    if attempt < max_attempts - 1:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        time.sleep(delay_seconds)
                    continue

            except requests.RequestException as e:
                logger.error(
                    f"Error on attempt {attempt + 1} for page {page}: {str(e)}"
                )
                if attempt < max_attempts - 1:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    time.sleep(delay_seconds)
                continue

        else:  # –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –µ—Å–ª–∏ —Ü–∏–∫–ª –ø–æ–ø—ã—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ break
            logger.error(f"Failed to get page {page} after {max_attempts} attempts")


def scrap_html():
    # Extract row data
    property_data_list = []

    for html_file in html_search_directory.glob("*.html"):
        with open(html_file, "r", encoding="utf-8") as file:
            content = file.read()

        soup = BeautifulSoup(content, "lxml")
        table = soup.find("table", attrs={"id": "T1"})

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        if not table:
            print(f"–¢–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue

        for row in table.find_all("tr", valign="top"):
            cells = row.find_all("td")

            # Skip if not enough cells
            if len(cells) < 8:  # –ú–∏–Ω–∏–º—É–º 8 —è—á–µ–µ–∫ –≤ —Å—Ç—Ä–æ–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
                continue

            try:
                # Extract Parcel ID and Account Number from first cell
                parcel_cell = cells[0]
                parcel_link = parcel_cell.find("a")

                if not parcel_link:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Å—Å—ã–ª–∫–∏

                parcel_id = parcel_link.get_text(strip=True)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Account Number –∏ URL –∏–∑ href
                href = parcel_link.get("href")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ href —Å / –∏–ª–∏ –Ω–µ—Ç
                url = f"{BASE_URL}{href}"

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Account Number –∏–∑ URL
                account_match = None
                if url:
                    account_match_raw = re.search(r"AccountNumber=(\d+)", url)
                    if account_match_raw:
                        account_match = account_match_raw.group(1)

                # Location
                location_text = ""
                if len(cells) > 1:
                    location_text = cells[1].get_text(strip=True).replace("\xa0", " ")

                # Owner - –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤
                owners = ""
                if len(cells) > 2:
                    owner_links = cells[2].find_all("a")
                    if owner_links:
                        owners = ";".join(a.get_text(strip=True) for a in owner_links)

                # LUC Description
                luc_links = ""
                if len(cells) > 7:
                    luc_cell = cells[7]
                    luc_links_raw = luc_cell.find_all("a")
                    if len(luc_links_raw) > 1:
                        luc_links = luc_links_raw[1].get_text(strip=True)

                property_data = {
                    "Parcel ID": parcel_id,
                    "Location": location_text,
                    "Owner": owners,
                    "LUC Description": luc_links,
                    "Account Number": account_match,
                    "URL": url,
                }
                property_data_list.append(property_data)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–µ {html_file}: {e}")
                continue

    with open(output_json_file, "w", encoding="utf-8") as f:
        json.dump(property_data_list, f, ensure_ascii=False, indent=4)

    df = pd.DataFrame(property_data_list)
    df.to_csv(output_csv_file, index=False, encoding="utf-8")


def read_cities_from_csv():
    df = pd.read_csv(output_csv_file)
    return df["URL"].tolist()


def create_session():
    session = requests.Session()
    config = load_config()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    if "headers" in config and isinstance(config["headers"], dict):
        session.headers.update(config["headers"])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫—É–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ (—É—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–±–µ–ª–∞ –≤ –∫–ª—é—á–µ)
    cookies_key = "cookies"
    if cookies_key not in config:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç —Å –ø—Ä–æ–±–µ–ª–æ–º
        cookies_key = "cookies "

    if cookies_key in config and isinstance(config[cookies_key], dict):
        # –î–æ–±–∞–≤–ª—è–µ–º –∫—É–∫–∏ –≤ —Å–µ—Å—Å–∏—é
        for cookie_name, cookie_value in config[cookies_key].items():
            session.cookies.set(cookie_name, cookie_value)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é, –ø–æ—Å–µ—Ç–∏–≤ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫—É–∫–∏
    try:
        response = session.get("https://auburnmaine.patriotproperties.com/")
        if response.status_code != 200:
            logger.info(
                f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏: {response.status_code}"
            )
        return session
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–µ—Å—Å–∏–∏: {e}")
        return None


def process_url(session, url, account_number):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL —Å–≤–æ–π—Å—Ç–≤–∞, —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

    Args:
        session: –û–±—ä–µ–∫—Ç —Å–µ—Å—Å–∏–∏ requests
        url: URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        account_number: –ù–æ–º–µ—Ä –∞–∫–∫–∞—É–Ω—Ç–∞
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
        retry_delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)

    Returns:
        bool: True –≤ —Å–ª—É—á–∞–µ —É—Å–ø–µ—Ö–∞, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    config = load_config()
    MAX_RETRIES = config["MAX_RETRIES"]
    RETRY_DELAY = config["RETRY_DELAY"]
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            card_number = 1
            output_html_file = (
                html_product_directory / f"{account_number}_{card_number}.html"
            )
            if output_html_file.exists():
                return True
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π URL –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Å–µ—Å—Å–∏–∏
            main_response = session.get(url)

            if (
                "Either no search has been executed or your session has timed out"
                in main_response.text
                or main_response.status_code != 200
            ):
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )
                return False

            # –¢–µ–ø–µ—Ä—å –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–µ–π–º–∞ summary-bottom.asp
            bottom_url = f"{BASE_URL}/summary-bottom.asp"
            response = session.get(bottom_url)

            if (
                "Either no search has been executed or your session has timed out"
                in response.text
            ):
                logger.error(
                    f"–û—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—É–∫–∏."
                )
                # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ True/False
                raise SessionExpiredException(
                    f"–°–µ—Å—Å–∏—è –∏—Å—Ç–µ–∫–ª–∞ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            save_response(response.text, output_html_file)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –∫–∞—Ä—Ç–æ—á–µ–∫
            soup = BeautifulSoup(response.text, "lxml")
            card_info = soup.select_one(
                "body > table > tbody > tr > td:nth-child(3) > p"
            )

            # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
            total_cards = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞

            try:
                card_text = card_info.text.strip()

                # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ "Card 1 of 4"
                if "of" in card_text:
                    try:
                        current_card, total_cards = map(
                            int, re.findall(r"\d+", card_text)
                        )
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö: {e}")
                        if attempt < MAX_RETRIES:
                            logger.info(
                                f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥..."
                            )
                            time.sleep(RETRY_DELAY)
                            continue
                        return False
                else:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{card_text}'"
                    )
                    return True  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ –µ–¥–∏–Ω–∏—á–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞
            except AttributeError:
                logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )
                return True  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ –µ–¥–∏–Ω–∏—á–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞

            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞, —Ç–æ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏
            if total_cards == 1:
                return True

            # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏, —Å–∫–∞—á–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            all_cards_success = True
            for card_num in range(2, total_cards + 1):
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ä—Ç–æ—á–∫–∏
                params = {"ValCard": "0", "Card": str(card_num)}
                card_output_html_file = (
                    html_product_directory / f"{account_number}_{card_num}.html"
                )
                if card_output_html_file.exists():
                    continue

                card_success = False
                for card_attempt in range(1, MAX_RETRIES + 1):
                    # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –∫–∞—Ä—Ç–æ—á–∫—É
                    logger.info(
                        f"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {card_num}, –ø–æ–ø—ã—Ç–∫–∞ {card_attempt}/{MAX_RETRIES}"
                    )
                    next_response = session.get(
                        f"{BASE_URL}/Summary-bottom.asp", params=params
                    )

                    if (
                        "Either no search has been executed or your session has timed out"
                        in next_response.text
                    ):
                        logger.error(
                            f"–û—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ {card_num} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                        )
                        if card_attempt < MAX_RETRIES:
                            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π URL —Å–Ω–æ–≤–∞
                            session = create_session()
                            if not session:
                                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é")
                                break
                            session.get(url)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                            logger.info(
                                f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–µ–π —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥..."
                            )
                            time.sleep(RETRY_DELAY)
                            continue
                        break

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç–æ—á–∫—É
                    save_response(next_response.text, card_output_html_file)
                    logger.info(
                        f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞ {card_num} –∏–∑ {total_cards} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                    )
                    card_success = True
                    break

                if not card_success:
                    all_cards_success = False
                    logger.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É {card_num} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫"
                    )

            return all_cards_success

        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}: {e}"
            )
            if attempt < MAX_RETRIES:
                logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥...")
                time.sleep(RETRY_DELAY)
                continue
            return False

    return False


def save_response(html_content, file_name):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª
    """
    with open(file_name, "w", encoding="utf-8") as file:
        file.write(html_content)


class SessionExpiredException(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –∏—Å—Ç–µ—á–µ–Ω–∏–∏ —Å–µ—Å—Å–∏–∏"""

    pass


def process_url_list():
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ CSV —Ñ–∞–π–ª–∞
    """
    session = create_session()

    if not session:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
        return False

    success_count = 0
    failed_count = 0
    urls = read_cities_from_csv()
    for url in urls:
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º AccountNumber –∏–∑ URL
            account_match = re.search(r"AccountNumber=(\d+)", url)
            if not account_match:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –∞–∫–∫–∞—É–Ω—Ç–∞ –∏–∑ URL: {url}")
                failed_count += 1
                continue

            account_number = account_match.group(1)

            try:
                if process_url(session, url, account_number):
                    success_count += 1
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω URL –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}")
                else:
                    failed_count += 1
                    logger.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                    )
            except SessionExpiredException as see:
                logger.critical(f"{see}. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                logger.critical(
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±–Ω–æ–≤–∏—Ç–µ –∫—É–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç."
                )
                return False  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—Å—é –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–µ—Å—Å–∏–∏

        except Exception as e:
            failed_count += 1
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url}: {e}")

    logger.info(
        f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {success_count}, –û—à–∏–±–æ–∫: {failed_count}"
    )
    return success_count > 0


if __name__ == "__main__":
    # get_html()
    # scrap_html()
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∏–∑ CSV —Ñ–∞–π–ª–∞

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
    process_url_list()
