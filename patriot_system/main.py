import csv
import json
import re
import sys
import time
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import parse_qs, urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_search_directory = current_directory / "html_search"
html_product_directory = current_directory / "html_product"
config_directory = current_directory / "config"
log_directory = current_directory / "log"
log_directory.mkdir(parents=True, exist_ok=True)
html_product_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)

output_json_file = data_directory / "output.json"
output_xlsx_file = data_directory / "output.xlsx"
output_csv_file = data_directory / "output.csv"
log_file_path = log_directory / "log_message.log"
CONFIG_PATH = config_directory / "config.json"
PROXIES_PATH = config_directory / "proxies.json"
cache_file = data_directory / "downloaded_accounts.json"


BASE_URL = "https://auburnmaine.patriotproperties.com/"
logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)


# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


class SessionExpiredException(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –∏—Å—Ç–µ—á–µ–Ω–∏–∏ —Å–µ—Å—Å–∏–∏"""

    pass


class StopProcessingException(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö URL"""

    pass


def load_config():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ JSON-—Ñ–∞–π–ª–∞"""

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if not CONFIG_PATH.exists():
            logger.error(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {CONFIG_PATH}")
            logger.error("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–∞.")
            sys.exit(1)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = json.load(f)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
        required_sections = ["cookies"]
        for section in required_sections:
            if section not in config:
                logger.error(
                    f"–û—à–∏–±–∫–∞: –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–¥–µ–ª '{section}'"
                )
                sys.exit(1)

        return config

    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)


def load_proxies():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞ JSON"""
    try:
        if not PROXIES_PATH.exists():
            logger.error(f"–§–∞–π–ª —Å –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {PROXIES_PATH}")
            return []

        with open(PROXIES_PATH, "r", encoding="utf-8") as f:
            proxies = json.load(f)
            return proxies
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ JSON –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø—Ä–æ–∫—Å–∏: {e}")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø—Ä–æ–∫—Å–∏: {e}")
        return []


def get_html():
    timeout = 60
    max_attempts = 10
    delay_seconds = 5
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    config = load_config()
    cookies = config["cookies"]
    headers = config["headers"]
    data = config["data"]
    for page in range(1, 194):
        output_html_file = html_search_directory / f"auburnmaine_0{page}.html"

        for attempt in range(max_attempts):
            try:
                if page == 1:
                    response = requests.post(
                        "https://auburnmaine.patriotproperties.com/SearchResults.asp",
                        cookies=cookies,
                        headers=headers,
                        data=data,
                        timeout=timeout,
                    )
                else:
                    params = {
                        "page": page,
                    }
                    response = requests.get(
                        "https://auburnmaine.patriotproperties.com/SearchResults.asp",
                        params=params,
                        cookies=cookies,
                        headers=headers,
                        timeout=timeout,
                    )

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
                if response.status_code == 200:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ü–µ–ª–∏–∫–æ–º
                    with open(output_html_file, "w", encoding="utf-8") as file:
                        file.write(response.text)
                    logger.info(f"Successfully saved {output_html_file}")
                    break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                else:
                    logger.warning(
                        f"Attempt {attempt + 1} failed for page {page} with status {response.status_code}"
                    )
                    if attempt < max_attempts - 1:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        time.sleep(delay_seconds)
                    continue

            except requests.RequestException as e:
                logger.error(
                    f"Error on attempt {attempt + 1} for page {page}: {str(e)}"
                )
                if attempt < max_attempts - 1:  # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    time.sleep(delay_seconds)
                continue

        else:  # –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –µ—Å–ª–∏ —Ü–∏–∫–ª –ø–æ–ø—ã—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ break
            logger.error(f"Failed to get page {page} after {max_attempts} attempts")


def scrap_html():
    # Extract row data
    property_data_list = []

    for html_file in html_search_directory.glob("*.html"):
        with open(html_file, "r", encoding="utf-8") as file:
            content = file.read()

        soup = BeautifulSoup(content, "lxml")
        table = soup.find("table", attrs={"id": "T1"})

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        if not table:
            print(f"–¢–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue

        for row in table.find_all("tr", valign="top"):
            cells = row.find_all("td")

            # Skip if not enough cells
            if len(cells) < 8:  # –ú–∏–Ω–∏–º—É–º 8 —è—á–µ–µ–∫ –≤ —Å—Ç—Ä–æ–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
                continue

            try:
                # Extract Parcel ID and Account Number from first cell
                parcel_cell = cells[0]
                parcel_link = parcel_cell.find("a")

                if not parcel_link:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Å—Å—ã–ª–∫–∏

                parcel_id = parcel_link.get_text(strip=True)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Account Number –∏ URL –∏–∑ href
                href = parcel_link.get("href")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ href —Å / –∏–ª–∏ –Ω–µ—Ç
                url = f"{BASE_URL}{href}"

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Account Number –∏–∑ URL
                account_match = None
                if url:
                    account_match_raw = re.search(r"AccountNumber=(\d+)", url)
                    if account_match_raw:
                        account_match = account_match_raw.group(1)

                # Location
                location_text = ""
                if len(cells) > 1:
                    location_text = cells[1].get_text(strip=True).replace("\xa0", " ")

                # Owner - –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤
                owners = ""
                if len(cells) > 2:
                    owner_links = cells[2].find_all("a")
                    if owner_links:
                        owners = ";".join(a.get_text(strip=True) for a in owner_links)

                # LUC Description
                luc_links = ""
                if len(cells) > 7:
                    luc_cell = cells[7]
                    luc_links_raw = luc_cell.find_all("a")
                    if len(luc_links_raw) > 1:
                        luc_links = luc_links_raw[1].get_text(strip=True)

                property_data = {
                    "Parcel ID": parcel_id,
                    "Location": location_text,
                    "Owner": owners,
                    "LUC Description": luc_links,
                    "Account Number": account_match,
                    "URL": url,
                }
                property_data_list.append(property_data)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–µ {html_file}: {e}")
                continue

    with open(output_json_file, "w", encoding="utf-8") as f:
        json.dump(property_data_list, f, ensure_ascii=False, indent=4)

    df = pd.DataFrame(property_data_list)
    df.to_csv(output_csv_file, index=False, encoding="utf-8")


def read_cities_from_csv():
    df = pd.read_csv(output_csv_file)
    return df["URL"].tolist()


def update_download_cache():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–µ—à-—Ñ–∞–π–ª —Å–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º–∏ –∞–∫–∫–∞—É–Ω—Ç–∞–º–∏"""
    downloaded = set()
    pattern = re.compile(r"(\d+)_\d+\.html")

    for file_path in html_product_directory.glob("*.html"):
        match = pattern.match(file_path.name)
        if match:
            downloaded.add(match.group(1))

    with open(cache_file, "w") as f:
        json.dump(list(downloaded), f)

    logger.info(f"–ö–µ—à –æ–±–Ω–æ–≤–ª–µ–Ω: {len(downloaded)} –∞–∫–∫–∞—É–Ω—Ç–æ–≤")


def get_downloaded_accounts():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫–∫–∞—É–Ω—Ç–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ —Å–∫–∞—á–∞–Ω—ã —Ñ–∞–π–ª—ã"""

    # –ï—Å–ª–∏ –∫–µ—à-—Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∞–∫—Ç—É–∞–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if cache_file.exists():
        cache_mtime = cache_file.stat().st_mtime
        dir_mtime = max(
            [p.stat().st_mtime for p in html_product_directory.glob("*.html")],
            default=0,
        )

        # –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –∏–∑–º–µ–Ω—è–ª–∞—Å—å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∫–µ—à–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à
        if dir_mtime <= cache_mtime:
            try:
                with open(cache_file, "r") as f:
                    downloaded = set(json.load(f))
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(downloaded)} –∞–∫–∫–∞—É–Ω—Ç–æ–≤ –∏–∑ –∫–µ—à–∞")
                return downloaded
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à-—Ñ–∞–π–ª–∞: {e}")

    # –ò–Ω–∞—á–µ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    downloaded = set()
    pattern = re.compile(r"(\d+)_\d+\.html")

    for file_path in html_product_directory.glob("*.html"):
        match = pattern.match(file_path.name)
        if match:
            downloaded.add(match.group(1))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à
    try:
        with open(cache_file, "w") as f:
            json.dump(list(downloaded), f)
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∫–µ—à-—Ñ–∞–π–ª–∞: {e}")

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(downloaded)} —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤")
    return downloaded


def create_session(proxy=None):
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)"""
    session = requests.Session()
    config = load_config()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    if "headers" in config and isinstance(config["headers"], dict):
        session.headers.update(config["headers"])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫—É–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    cookies_key = "cookies"
    if cookies_key not in config:
        cookies_key = "cookies "

    if cookies_key in config and isinstance(config[cookies_key], dict):
        for cookie_name, cookie_value in config[cookies_key].items():
            session.cookies.set(cookie_name, cookie_value)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏, –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω
    if proxy:
        session.proxies = {"http": proxy, "https": proxy}

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é, –ø–æ—Å–µ—Ç–∏–≤ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    try:
        response = session.get("https://auburnmaine.patriotproperties.com/", timeout=30)
        if response.status_code != 200:
            logger.warning(
                f"–°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏: {response.status_code}"
            )
        return session
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–µ—Å—Å–∏–∏ (–ø—Ä–æ–∫—Å–∏: {proxy}): {e}")
        return None


def process_url(session, url, account_number):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL —Å–≤–æ–π—Å—Ç–≤–∞, —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏

    Args:
        session: –û–±—ä–µ–∫—Ç —Å–µ—Å—Å–∏–∏ requests
        url: URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        account_number: –ù–æ–º–µ—Ä –∞–∫–∫–∞—É–Ω—Ç–∞
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)
        retry_delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)

    Returns:
        bool: True –≤ —Å–ª—É—á–∞–µ —É—Å–ø–µ—Ö–∞, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    config = load_config()
    MAX_RETRIES = config["MAX_RETRIES"]
    RETRY_DELAY = config["RETRY_DELAY"]
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            card_number = 1
            output_html_file = (
                html_product_directory / f"{account_number}_{card_number}.html"
            )
            # if output_html_file.exists():
            #     return True
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π URL –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Å–µ—Å—Å–∏–∏
            main_response = session.get(url)
            logger.info(main_response.status_code)
            if (
                "Either no search has been executed or your session has timed out"
                in main_response.text
            ):
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )
                raise StopProcessingException(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )
            if main_response.status_code == 403:
                time.sleep(300)

            # –¢–µ–ø–µ—Ä—å –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–µ–π–º–∞ summary-bottom.asp
            bottom_url = f"{BASE_URL}/summary-bottom.asp"
            response = session.get(bottom_url)

            if (
                "Either no search has been executed or your session has timed out"
                in response.text
            ):
                logger.error(
                    f"–û—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—É–∫–∏."
                )
                # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ True/False
                raise SessionExpiredException(
                    f"–°–µ—Å—Å–∏—è –∏—Å—Ç–µ–∫–ª–∞ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            save_response(response.text, output_html_file)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –∫–∞—Ä—Ç–æ—á–µ–∫
            soup = BeautifulSoup(response.text, "lxml")
            card_info = soup.select_one(
                "body > table > tbody > tr > td:nth-child(3) > p"
            )

            # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ª—é–±–æ–≥–æ —Ç–µ–≥–∞ <p>, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ —Ç–µ–∫—Å—Ç "Card X of Y"
            if not card_info:
                for p_tag in soup.find_all("p"):
                    if "Card" in p_tag.text and "of" in p_tag.text:
                        card_info = p_tag
                        break

            # –ú–µ—Ç–æ–¥ 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–∫–∏ Next Card
            has_next_card = False
            next_card_link = soup.find("a", string="Next Card")
            if not next_card_link:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—É—é —Å—Å—ã–ª–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é "Next Card"
                for a_tag in soup.find_all("a"):
                    if "Next Card" in a_tag.text:
                        next_card_link = a_tag
                        has_next_card = True
                        break
            else:
                has_next_card = True

            # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
            total_cards = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞

            try:
                if card_info:
                    card_text = card_info.text.strip()

                    # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ "Card 1 of 4"
                    if "of" in card_text:
                        try:
                            current_card, total_cards = map(
                                int, re.findall(r"\d+", card_text)
                            )
                            logger.info(
                                f"–ù–∞–π–¥–µ–Ω–æ {total_cards} –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                            )
                        except Exception as e:
                            logger.error(
                                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö: {e}"
                            )
                            if attempt < MAX_RETRIES:
                                logger.info(
                                    f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥..."
                                )
                                time.sleep(RETRY_DELAY)
                                continue
                            return False
                    else:
                        logger.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{card_text}'"
                        )

                        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –∫–∞—Ä—Ç–æ—á–∫—É, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ —á–∏—Å–ª–æ –∫–∞—Ä—Ç–æ—á–µ–∫
                        if has_next_card:
                            total_cards = 2  # –ú–∏–Ω–∏–º—É–º 2 –∫–∞—Ä—Ç–æ—á–∫–∏
                            logger.info(
                                f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ 'Next Card', –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–∏–Ω–∏–º—É–º 2 –∫–∞—Ä—Ç–æ—á–∫–∏"
                            )
                else:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–∫–∏ Next Card
                    if has_next_card:
                        total_cards = 2  # –ú–∏–Ω–∏–º—É–º 2 –∫–∞—Ä—Ç–æ—á–∫–∏
                        logger.info(
                            f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ 'Next Card', –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–∏–Ω–∏–º—É–º 2 –∫–∞—Ä—Ç–æ—á–∫–∏"
                        )
                    else:
                        logger.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                        )
            except AttributeError:
                logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                )

            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞, —Ç–æ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏
            if total_cards == 1:
                return True

            # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏, —Å–∫–∞—á–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            all_cards_success = True
            for card_num in range(2, total_cards + 1):
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ä—Ç–æ—á–∫–∏
                params = {"ValCard": "0", "Card": str(card_num)}
                card_output_html_file = (
                    html_product_directory / f"{account_number}_{card_num}.html"
                )
                if card_output_html_file.exists():
                    continue

                card_success = False
                for card_attempt in range(1, MAX_RETRIES + 1):
                    # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –∫–∞—Ä—Ç–æ—á–∫—É
                    logger.info(
                        f"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {card_num}, –ø–æ–ø—ã—Ç–∫–∞ {card_attempt}/{MAX_RETRIES}"
                    )
                    next_response = session.get(
                        f"{BASE_URL}/Summary-bottom.asp", params=params
                    )

                    if (
                        "Either no search has been executed or your session has timed out"
                        in next_response.text
                    ):
                        logger.error(
                            f"–û—à–∏–±–∫–∞ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ {card_num} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                        )
                        if card_attempt < MAX_RETRIES:
                            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π URL —Å–Ω–æ–≤–∞
                            session = create_session()
                            if not session:
                                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é")
                                break
                            session.get(url)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                            logger.info(
                                f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–µ–π —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥..."
                            )
                            time.sleep(RETRY_DELAY)
                            continue
                        break

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç–æ—á–∫—É
                    save_response(next_response.text, card_output_html_file)
                    logger.info(
                        f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞ {card_num} –∏–∑ {total_cards} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                    )
                    card_success = True
                    break

                if not card_success:
                    all_cards_success = False
                    logger.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É {card_num} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫"
                    )

            return all_cards_success

        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url} –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}: {e}"
            )
            if attempt < MAX_RETRIES:
                logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥...")
                time.sleep(RETRY_DELAY)
                continue
            return False

    return False


def save_response(html_content, file_name):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª
    """
    with open(file_name, "w", encoding="utf-8") as file:
        file.write(html_content)


def process_url_list(skip_downloaded=True):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ CSV —Ñ–∞–π–ª–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–∫—Å–∏
    """
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤
    downloaded_accounts = get_downloaded_accounts() if skip_downloaded else set()
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    proxies = load_proxies()
    if not proxies:
        logger.warning(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–∫—Å–∏, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"
        )

    proxy_index = 0
    proxy_count = len(proxies)

    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏
    current_proxy = proxies[proxy_index] if proxy_count > 0 else None
    session = create_session(current_proxy)

    if not session:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
        return False

    success_count = 0
    failed_count = 0
    urls = read_cities_from_csv()

    for url_index, url in enumerate(urls):
        try:
            # –ú–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –Ω–æ–≤–æ–º URL, –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–∫—Å–∏
            if proxy_count > 0 and url_index > 0:
                proxy_index = (proxy_index + 1) % proxy_count
                current_proxy = proxies[proxy_index]
                session = create_session(current_proxy)

                if not session:
                    logger.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é —Å –ø—Ä–æ–∫—Å–∏ {current_proxy}. –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π."
                    )
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –∏—Ç–µ—Ä–∞—Ü–∏—é –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é
                    failed_count += 1
                    continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º AccountNumber –∏–∑ URL
            account_match = re.search(r"AccountNumber=(\d+)", url)
            if not account_match:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –∞–∫–∫–∞—É–Ω—Ç–∞ –∏–∑ URL: {url}")
                failed_count += 1
                continue

            account_number = account_match.group(1)
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –∞–∫–∫–∞—É–Ω—Ç—ã
            if account_number in downloaded_accounts:
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–∫–∫–∞—É–Ω—Ç {account_number}, —É–∂–µ —Å–∫–∞—á–∞–Ω")
                success_count += 1
                continue
            try:
                if process_url(session, url, account_number):
                    success_count += 1
                    logger.info(
                        f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω URL –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number} (–ø—Ä–æ–∫—Å–∏: {current_proxy})"
                    )
                else:
                    failed_count += 1
                    logger.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL –¥–ª—è –∞–∫–∫–∞—É–Ω—Ç–∞ {account_number}"
                    )
            except SessionExpiredException as see:
                logger.critical(f"{see}. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                logger.critical(
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±–Ω–æ–≤–∏—Ç–µ –∫—É–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç."
                )
                return False  # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–∏ –∏—Å—Ç–µ—á–µ–Ω–∏–∏ —Å–µ—Å—Å–∏–∏
            except StopProcessingException as spe:
                logger.critical(f"{spe}. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                return False  # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ

        except Exception as e:
            logger.critical(
                f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url}: {e}. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."
            )
            return False

    logger.info(
        f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {success_count}, –û—à–∏–±–æ–∫: {failed_count}"
    )
    return success_count > 0


if __name__ == "__main__":
    # get_html()
    # scrap_html()
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∏–∑ CSV —Ñ–∞–π–ª–∞

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
    process_url_list()
