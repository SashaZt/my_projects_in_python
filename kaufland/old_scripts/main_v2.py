import json
import re
import sys
import traceback
from html import unescape
from pathlib import Path

from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
log_directory = current_directory / "log"
log_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def clean_html(html_text):
    """–û—á–∏—â–∞–µ—Ç HTML-—Ä–∞–∑–º–µ—Ç–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç"""
    text = re.sub(r"\u003C\u002F?[^>]+\u003E", "", html_text)  # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    text = unescape(text)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏
    return text.strip()


def extract_breadcrumb(script_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏ —Å –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    breadcrumb = []
    breadcrumb_pattern = r"breadcrumb:\[(.*?)\]"
    match = re.search(breadcrumb_pattern, script_content, re.DOTALL)

    if match:
        items = match.group(1)
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        item_sections = items.split("},{")

        for section in item_sections:
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∫–æ–±–æ–∫ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
            section = section.strip("{").strip("}")
            item = {}

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–æ–ª—è
            # ID (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            id_match = re.search(r"id:([^,]+)", section)
            if id_match:
                id_value = id_match.group(1)
                try:
                    item["id"] = int(id_value)
                except ValueError:
                    item["id"] = id_value

            # Name
            name_match = re.search(r'name:"([^"]+)"', section)
            if name_match:
                item["name"] = name_match.group(1)

            # Title
            title_match = re.search(r'title:"([^"]+)"', section)
            if title_match:
                item["title"] = title_match.group(1)

            # URL
            url_match = re.search(r'url:"([^"]+)"', section)
            if url_match:
                item["url"] = url_match.group(1).replace("\\u002F", "/")

            # isMasked
            masked_match = re.search(r"isMasked:([a-zA-Z])", section)
            if masked_match:
                item["isMasked"] = masked_match.group(1)

            if item:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–ª–µ
                breadcrumb.append(item)

    return breadcrumb


def extract_description(script_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞"""
    desc_pattern = r'descriptionHtml:"([^"]+)"'
    match = re.search(desc_pattern, script_content)

    if not match:
        return {}

    html_content = (
        match.group(1)
        .replace("\\u003C", "<")
        .replace("\\u003E", ">")
        .replace("\\u002F", "/")
    )

    specs = {}
    dt_dd_pattern = r"<dt>([^<]+)</dt><dd>([^<]+)</dd>"
    for dt, dd in re.findall(dt_dd_pattern, html_content):
        key = clean_html(dt.strip(":"))
        value = clean_html(dd)
        specs[key] = value

    return {"specifications": specs}


def extract_attributes(script_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã –ø—Ä–æ–¥—É–∫—Ç–∞ —Å –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    # –ò—â–µ–º –≤—Å—é —Å–µ–∫—Ü–∏—é –∞—Ç—Ä–∏–±—É—Ç–æ–≤
    attr_section_pattern = r"attributes:{default:\[(.*?)\],highlighted:"
    section_match = re.search(attr_section_pattern, script_content, re.DOTALL)

    if not section_match:
        return {}

    attrs_section = section_match.group(1)
    attributes = []

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–∂–¥—ã–π –∞—Ç—Ä–∏–±—É—Ç
    attr_pattern = r'{id:"([^"]+)",name:"([^"]+)",values:\[([^\]]*)\]'
    for attr_match in re.finditer(attr_pattern, attrs_section):
        attr_id = attr_match.group(1)
        attr_name = attr_match.group(2)
        values_text = attr_match.group(3)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        values = []
        value_pattern = r'{text:"([^"]+)"(?:,link:"([^"]+)")?}'
        for value_match in re.finditer(value_pattern, values_text):
            value = {"text": value_match.group(1)}
            if value_match.group(2):
                value["link"] = value_match.group(2).replace("\\u002F", "/")
            values.append(value)

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        if not values:
            continue

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
        flags = {}
        flag_patterns = {
            "isCategoryRelevant": r"isCategoryRelevant:(true|false)",
            "isDefaultRelevant": r"isDefaultRelevant:(true|false)",
            "isPartOfRadioEquipmentAct": r"isPartOfRadioEquipmentAct:(true|false)",
        }

        for flag_name, pattern in flag_patterns.items():
            flag_match = re.search(pattern, attrs_section)
            if flag_match:
                flags[flag_name] = flag_match.group(1) == "true"

        attributes.append(
            {"id": attr_id, "name": attr_name, "values": values, "flags": flags}
        )

    return attributes


def extract_image_hashes(script_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–µ—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Å–∫—Ä–∏–ø—Ç–∞"""
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å —Å–∫—Ä–∏–ø—Ç–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ñ—É–Ω–∫—Ü–∏–∏
    end_pattern = r"\)\((.*?)\)\);$"
    match = re.search(end_pattern, script_content)
    if not match:
        return []

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    params = match.group(1).split(",")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ö–µ—à–∏ (—Å—Ç—Ä–æ–∫–∏ –∏–∑ 32 —Å–∏–º–≤–æ–ª–æ–≤ –≤ —à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç–µ—Ä–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
    hashes = []
    hash_pattern = r'"([a-f0-9]{32})"'

    for param in params:
        hash_match = re.search(hash_pattern, param)
        if hash_match:
            hashes.append(hash_match.group(1))

    return hashes


def extract_product_data(script_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ"""
    product_data = {
        "id": None,
        "title": None,
        "manufacturer": None,
        "price": None,
        "currency": None,
        "breadcrumb": [],
        "description": {},
        "attributes": [],
        "reviews": {},
        "delivery": {},
        "image_hashes": [],  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –ø–æ–ª–µ
    }

    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    title_match = re.search(r'title:"([^"]+)"', script_content)
    if title_match:
        product_data["title"] = title_match.group(1)

    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
    manufacturer_pattern = r'"manufacturer".*?"text":"([^"]+)"'
    manufacturer_match = re.search(manufacturer_pattern, script_content)
    if manufacturer_match:
        product_data["manufacturer"] = manufacturer_match.group(1)

    # –¶–µ–Ω–∞ –∏ –≤–∞–ª—é—Ç–∞
    price_match = re.search(r"B\.offerNetPrice=([\d.]+)", script_content)
    if price_match:
        product_data["price"] = float(price_match.group(1))
        product_data["currency"] = "EUR"

    # –•–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏
    product_data["breadcrumb"] = extract_breadcrumb(script_content)

    # –û–ø–∏—Å–∞–Ω–∏–µ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    description_data = extract_description(script_content)
    product_data.update(description_data)

    # –ê—Ç—Ä–∏–±—É—Ç—ã
    product_data["attributes"] = extract_attributes(script_content)

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç–∞–≤–∫–µ
    delivery_pattern = r"delivery:{([^}]+)}"
    delivery_match = re.search(delivery_pattern, script_content)
    if delivery_match:
        delivery_text = delivery_match.group(1)
        delivery_info = {}
        for field in ["datePhrase", "deliveryTime"]:
            field_match = re.search(rf'{field}:"([^"]+)"', delivery_text)
            if field_match:
                delivery_info[field] = field_match.group(1)
        product_data["delivery"] = delivery_info

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—Ç–∑—ã–≤–∞—Ö
    reviews_pattern = r"reviewMetaData:{numberOfReviews:(\d+),numberOfStars:([\d.]+)}"
    reviews_match = re.search(reviews_pattern, script_content)
    if reviews_match:
        product_data["reviews"] = {
            "count": int(reviews_match.group(1)),
            "rating": float(reviews_match.group(2)),
        }
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–µ—à–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    product_data["image_hashes"] = extract_image_hashes(script_content)
    return product_data


def extract_script_content(html_file_path):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∫—Ä–∏–ø—Ç–∞, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ 'APP_SHELL_SSR_STATE_@mf\u002Fpdp-frontend' –∏–∑ HTML-—Ñ–∞–π–ª–∞.

    Args:
        html_file_path (str): –ü—É—Ç—å –∫ HTML-—Ñ–∞–π–ª—É

    Returns:
        str: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∫—Ä–∏–ø—Ç–∞ –∏–ª–∏ None, –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    try:
        # –ß—Ç–µ–Ω–∏–µ HTML-—Ñ–∞–π–ª–∞
        with open(html_file_path, "r", encoding="utf-8") as file:
            html_content = file.read()

        # –ú–µ—Ç–æ–¥ 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
        soup = BeautifulSoup(html_content, "html.parser")
        scripts = soup.find_all("script")

        target_script = None
        target_pattern = r'window\["APP_SHELL_SSR_STATE_@mf\\u002Fpdp-frontend"\]'

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Å–∫—Ä–∏–ø—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        for script in scripts:
            if script.string and re.search(target_pattern, script.string):
                target_script = script.string
                break

        # –ú–µ—Ç–æ–¥ 2: –ï—Å–ª–∏ BeautifulSoup –Ω–µ –Ω–∞—à–µ–ª —Å–∫—Ä–∏–ø—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        if target_script is None:
            pattern = r'<script[^>]*>(.*?window\["APP_SHELL_SSR_STATE_@mf\\u002Fpdp-frontend"\].*?)</script>'
            match = re.search(pattern, html_content, re.DOTALL)
            if match:
                target_script = match.group(1)

        return target_script

    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞: {e}")
        return None


def main():
    try:
        logger.info("–ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å–∫—Ä–∏–ø—Ç–∞...")
        # with open("found_script.txt", "r", encoding="utf-8") as f:
        #     script_content = f.read()
        script_content = extract_script_content(
            "Astschere, Baumschere, Gartenschere, _ Kaufland.de.html"
        )
        product_data = extract_product_data(script_content)

        logger.info("\n–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
        with open("product_data.json", "w", encoding="utf-8") as f:
            json.dump(product_data, f, ensure_ascii=False, indent=2)

        logger.info("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'product_data.json'")

        # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        logger.info("\n–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ:")
        logger.info(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {product_data['title']}")
        logger.info(f"–¶–µ–Ω–∞: {product_data['price']} {product_data['currency']}")

        if product_data["breadcrumb"]:
            logger.info("\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for item in product_data["breadcrumb"]:
                title = item.get("title") or item.get("name", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                logger.info(f"- {title}")

        if product_data.get("specifications"):
            logger.info("\n–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
            for key, value in list(product_data["specifications"].items())[:5]:
                logger.info(f"{key}: {value}")

        if product_data.get("attributes"):
            logger.info("\n–ê—Ç—Ä–∏–±—É—Ç—ã:")
            for attr in product_data["attributes"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                values_text = ", ".join(v["text"] for v in attr["values"])
                logger.info(f"{attr['name']}: {values_text}")

        if product_data.get("reviews"):
            logger.info(
                f"\n–û—Ç–∑—ã–≤—ã: {product_data['reviews']['count']} —à—Ç., "
                f"—Ä–µ–π—Ç–∏–Ω–≥: {product_data['reviews']['rating']}"
            )

        if product_data.get("delivery"):
            logger.info(f"\n–î–æ—Å—Ç–∞–≤–∫–∞: {product_data['delivery'].get('datePhrase')}")

    except FileNotFoundError:
        logger.error("–û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª found_script.txt")
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")

        traceback.print_exc()


if __name__ == "__main__":
    main()
