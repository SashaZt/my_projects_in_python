import json
import sys
import time
import urllib.parse
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from config import COOKIES, HEADERS
from loguru import logger

# API_KEY = "6c54502fd688c7ce737f1c650444884a"
API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö


current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def make_request_with_retries(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")

    return None


def get_all_page_html(id_product):
    url = "https://rrr.lt/ru/poisk"
    headers = HEADERS
    cookies = COOKIES
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    query_params = {"q": id_product, "prs": "2", "page": "1"}

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "url": url,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    }
    json_file = json_product_directory / f"{id_product}.json"
    if json_file.exists():
        logger.info(f"–§–∞–π–ª {json_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
    payload["url"] = f"{url}?{urllib.parse.urlencode(query_params)}"
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É Cookie –∏–∑ —Å–ª–æ–≤–∞—Ä—è cookies
    cookie_string = "; ".join(f"{key}={value}" for key, value in cookies.items())
    headers["Cookie"] = cookie_string
    response = make_request_with_retries(
        "https://api.scraperapi.com/",
        payload,
        MAX_RETRIES,
        RETRY_DELAY,
        headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    )

    if not response:
        raise Exception(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
        )

    src = response.text
    with open(json_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–°–∫–∞—á–∞–Ω–æ {json_file}")


def make_request_code(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            logger.info(f"response.status_code: {response.status_code}")
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")

    return None


def get_all_code(pages):
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "x-requested-with": "XMLHttpRequest",
    }
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        # 'render': 'true'  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
    }
    for page in range(0, pages + 1):
        if page == 0:
            url = "https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey"
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
            payload["url"] = f"{url}"
        else:
            url = f"https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey?page={page}"
            params = {
                "page": page,
            }
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
            payload["url"] = f"{url}?{urllib.parse.urlencode(params)}"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏

        html_file = html_code_directory / f"page_{page}.html"
        if html_file.exists():
            logger.info(f"–§–∞–π–ª {html_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        response = make_request_code(
            "https://api.scraperapi.com/",
            payload,
            MAX_RETRIES,
            RETRY_DELAY,
            headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
        )

        if not response:
            raise Exception(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
            )

        src = response.text
        with open(html_file, "w", encoding="utf-8") as file:
            file.write(src)

        logger.info(f"–°–∫–∞—á–∞–Ω–æ {html_file}")


def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["code"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return (
            df["code"].dropna().tolist()
        )  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def extract_data_product():
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_data = {}

    for json_file in json_product_directory.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file}")

        parts = data.get("parts", [])
        if not parts:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –≤ JSON.")
            continue

        min_price_part = min(
            parts, key=lambda x: float(x.get("price", float("inf"))), default=None
        )
        sku = data.get("search_query", None)

        categories = data.get("categories", {})
        category_name = next(
            (
                category["name"]
                for category in categories.values()
                if category.get("part_count", 0) > 0
            ),
            None,
        )

        if min_price_part and category_name:
            manufacturer_code = min_price_part.get("manufacturer_code", None)
            if not manufacturer_code:
                continue
            delivery_price_str = min_price_part.get("delivery_price", None)
            if delivery_price_str:
                delivery_price_str = delivery_price_str.replace(" ‚Ç¨", "")
            else:
                delivery_price_str = "0"

            price_str = min_price_part.get("price", "0")
            if not price_str:
                price_str = "0"

            delivery_price = float(delivery_price_str)
            price = float(price_str)

            result = {
                "–ë—Ä–µ–Ω–¥": min_price_part.get("car", {}).get("manufacturer", None),
                "–ö–æ–¥": sku,
                "K–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è": manufacturer_code,
                "–û–ø–∏—Å–∞–Ω–∏–µ": f"{category_name} | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ì–∞—Ä–∞–Ω—Ç—ñ—è  –Ω–∞ –≤–µ—Å—å —Ç–æ–≤–∞—Ä | –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏ —É –Ω–∞—Å –≤ –°–¢–û | –ó–∞–ø—á–∞—Å—Ç–∏–Ω–∏ –∑ –Ñ–≤—Ä–æ-—Ä–æ–∑–±–æ—Ä—ñ–≤ | –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å | –¢–µ–ª–µ—Ñ–æ–Ω—É–π—Ç–µ | –ú–∏—Ä–Ω–æ–≥–æ –¥–Ω—è.",
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price + price,
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞": price,
                "–¶–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price,
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –®–¢.": "1",
                "–ë/–£": "1",
                "–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞": None,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if category_name not in category_data:
                category_data[category_name] = []
            category_data[category_name].append(result)
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é {category_name}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category_name, data in category_data.items():
        if data:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –∑–∞–º–µ–Ω—è—è –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            safe_category_name = "".join(
                c for c in category_name if c.isalnum() or c in (" ", "-", "_")
            )

            file_name = data_directory / f"{safe_category_name}.xlsx"
            df = pd.DataFrame(data)
            df.to_excel(file_name, index=False)
            logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category_name}': {file_name}")


def extract_data_code():
    all_data = []
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_code_directory.glob("*.html"):
        with open(html_file, encoding="utf-8") as file:
            src = file.read()
        soup = BeautifulSoup(src, "lxml")
        code_tag = soup.find_all("button", attrs={"data-testid": "part-code"})
        if not code_tag:
            logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–¥ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue
        for code in code_tag:
            code_text = code.text.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            all_data.append(code_text)
    save_code_csv(all_data)


def save_code_csv(data):
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º "code"
    df = pd.DataFrame(data, columns=["code"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV —Ñ–∞–π–ª
    output_file = output_csv_file  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∏ –∏–º—è —Ñ–∞–π–ª–∞
    df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(
        f"–í—Å–µ –∫–æ–¥—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}"
    )


if __name__ == "__main__":
    # # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–¥—ã –∑–∞–ø—á–∞—Å—Ç–µ–π
    get_all_code(3)
    # # # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–¥—ã –∑–∞–ø—á–∞—Å—Ç–µ–π
    # extract_data_code()
    # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
    urls = read_urls(output_csv_file)
    for url in urls[:1]:
        get_all_page_html(url)
    # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
    extract_data_product()
