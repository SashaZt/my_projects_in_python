import json
import re
import sys
import time
import urllib.parse
from collections import OrderedDict
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

# API_KEY = "6c54502fd688c7ce737f1c650444884a"
API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö


current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def main_config():
    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    headers, cookies = parse_curl_from_file("config.txt")

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
    filtered_headers, filtered_cookies = filter_required_data(headers, cookies)

    return filtered_headers, filtered_cookies


def parse_curl_from_file(file_path="config.txt"):
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª config.txt
    with open(file_path, "r", encoding="utf-8") as f:
        curl_data = f.read().strip()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è headers –∏ cookies
    headers = {}
    cookies = {}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (-H)
    header_matches = re.findall(r"-H\s+'([^']+)'", curl_data)
    for header in header_matches:
        key, value = header.split(": ", 1)  # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª—é—á –∏ –∑–Ω–∞—á–µ–Ω–∏–µ
        headers[key.lower()] = value  # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—É–∫–∏ (-b)
    cookie_match = re.search(r"-b\s+'([^']+)'", curl_data)
    if cookie_match:
        cookie_string = cookie_match.group(1)
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –∫—É–∫ –≤ —Å–ª–æ–≤–∞—Ä—å
        cookie_pairs = cookie_string.split("; ")
        for pair in cookie_pairs:
            if "=" in pair:
                key, value = pair.split("=", 1)
                cookies[key] = value

    return headers, cookies


def filter_required_data(headers, cookies):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è headers
    required_headers_keys = {
        "accept",
        "accept-language",
        "x-requested-with",
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è cookies
    required_cookies_keys = {
        "ci_session",
        "ff_ux_sid",
        "cart_session",
        "CookieConsent",
        "soundestID",
        "omnisendSessionID",
        "disable_ovoko_modal",
        "wishlist",
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º headers
    filtered_headers = {
        key: headers[key] for key in required_headers_keys if key in headers
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º cookies
    filtered_cookies = {
        key: cookies[key] for key in required_cookies_keys if key in cookies
    }

    return filtered_headers, filtered_cookies


def submit_batch_jobs(product_ids):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç batch-–∑–∞–ø—Ä–æ—Å –≤ ScraperAPI –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
    """
    headers, cookies = main_config()
    cookie_string = "; ".join(f"{key}={value}" for key, value in cookies.items())
    headers["Cookie"] = cookie_string

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    urls = []
    for id_product in product_ids:
        json_file = json_product_directory / f"{id_product}.json"
        if json_file.exists():
            logger.info(f"–§–∞–π–ª {json_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {id_product}.")
            continue

        base_url = "https://rrr.lt/ru/poisk"
        query_params = {"q": id_product, "prs": "2", "page": "1"}
        full_url = f"{base_url}?{urllib.parse.urlencode(query_params)}"
        urls.append(full_url)

    if not urls:
        logger.info("–í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç. –ù–µ—á–µ–≥–æ —Å–∫–∞—á–∏–≤–∞—Ç—å.")
        return []

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è batch-–∑–∞–ø—Ä–æ—Å–∞
    payload = {
        "apiKey": API_KEY,
        "urls": urls,  # –°–ø–∏—Å–æ–∫ URL
        "apiParams": {
            "keep_headers": "true",  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        },
    }
    logger.info(payload)
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = requests.post(
                "https://async.scraperapi.com/batchjobs",
                json=payload,
                headers=headers,
                timeout=60,
            )
            if response.status_code == 200:
                return response.json()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ batch-–∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{MAX_RETRIES}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ batch-–∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{MAX_RETRIES}."
            )
        retries += 1
        time.sleep(RETRY_DELAY)

    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å batch-–∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫.")
    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å batch-–∑–∞–ø—Ä–æ—Å –∫ ScraperAPI.")


def check_batch_status(status_urls):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –∑–∞–¥–∞—á –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ–≥–¥–∞ –æ–Ω–∏ –≥–æ—Ç–æ–≤—ã.
    """
    results = {}
    for status_url in status_urls:
        retries = 0
        while retries < MAX_RETRIES:
            try:
                response = requests.get(status_url, timeout=60)
                if response.status_code != 200:
                    logger.warning(
                        f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {status_url}."
                    )
                    retries += 1
                    time.sleep(RETRY_DELAY)
                    continue

                job_data = response.json()
                job_id = job_data["id"]
                status = job_data["status"]
                original_url = job_data["url"]

                # –ò–∑–≤–ª–µ–∫–∞–µ–º id_product –∏–∑ URL
                id_product = urllib.parse.parse_qs(
                    urllib.parse.urlparse(original_url).query
                )["q"][0]
                json_file = json_product_directory / f"{id_product}.json"

                if status == "finished":
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    with open(json_file, "w", encoding="utf-8") as file:
                        json.dump(
                            job_data["response"], file, ensure_ascii=False, indent=4
                        )
                    logger.info(f"–°–∫–∞—á–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {json_file}")
                    results[id_product] = job_data["response"]
                    break
                elif status == "running":
                    logger.info(f"–ó–∞–¥–∞—á–∞ –¥–ª—è {id_product} –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –ñ–¥–µ–º...")
                else:
                    logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å {status} –¥–ª—è {id_product}.")
                    break

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ç—É—Å–∞ {status_url}: {e}")

            retries += 1
            time.sleep(RETRY_DELAY)

        if retries >= MAX_RETRIES:
            logger.error(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {status_url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫."
            )
            results[id_product] = None

    return results


def get_pages_html_batch(product_ids):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è batch-–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º batch-–∑–∞–ø—Ä–æ—Å
    batch_response = submit_batch_jobs(product_ids)
    if not batch_response:
        return {}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º statusUrl –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
    status_urls = [job["statusUrl"] for job in batch_response]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = check_batch_status(status_urls)
    return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    product_ids = ["5802243444", "735513182", "7422R9"]
    json_product_directory.mkdir(
        parents=True, exist_ok=True
    )  # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç

    results = get_pages_html_batch(product_ids)
    for product_id, result in results.items():
        if result:
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {product_id}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {product_id}")
