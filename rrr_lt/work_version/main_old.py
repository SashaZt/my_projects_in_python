import json
import re
import sys
import time
import urllib.parse
from collections import OrderedDict
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

# API_KEY = "6c54502fd688c7ce737f1c650444884a"
API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö


current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def make_request_page_html(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")

    return None


def get_page_html(id_product):
    url = "https://rrr.lt/ru/poisk"
    headers, cookies = main_config()
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    query_params = {"q": id_product, "prs": "2", "page": "1"}

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "url": url,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    }
    json_file = json_product_directory / f"{id_product}.json"
    if json_file.exists():
        logger.info(f"–§–∞–π–ª {json_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
    payload["url"] = f"{url}?{urllib.parse.urlencode(query_params)}"
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É Cookie –∏–∑ —Å–ª–æ–≤–∞—Ä—è cookies
    cookie_string = "; ".join(f"{key}={value}" for key, value in cookies.items())
    headers["Cookie"] = cookie_string
    response = make_request_page_html(
        "https://api.scraperapi.com/",
        payload,
        MAX_RETRIES,
        RETRY_DELAY,
        headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    )

    if not response:
        raise Exception(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
        )

    src = response.text
    with open(json_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–°–∫–∞—á–∞–Ω–æ {json_file}")


def make_request_code(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            logger.info(f"response.status_code: {response.status_code}")
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")

    return None


def get_all_code(pages):
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "x-requested-with": "XMLHttpRequest",
    }
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        # 'render': 'true'  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
    }
    for page in range(0, pages + 1):
        if page == 0:
            url = "https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey"
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
            payload["url"] = f"{url}"
        else:
            url = f"https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey?page={page}"
            params = {
                "page": page,
            }
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
            payload["url"] = f"{url}?{urllib.parse.urlencode(params)}"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏

        html_file = html_code_directory / f"page_{page}.html"
        if html_file.exists():
            logger.info(f"–§–∞–π–ª {html_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        response = make_request_code(
            "https://api.scraperapi.com/",
            payload,
            MAX_RETRIES,
            RETRY_DELAY,
            headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
        )

        if not response:
            raise Exception(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
            )

        src = response.text
        with open(html_file, "w", encoding="utf-8") as file:
            file.write(src)

        logger.info(f"–°–∫–∞—á–∞–Ω–æ {html_file}")


def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["code"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return (
            df["code"].dropna().tolist()
        )  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def extract_data_product():
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_data = {}

    for json_file in json_product_directory.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file}")

        parts = data.get("parts", [])
        if not parts:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –≤ JSON.")
            continue

        min_price_part = min(
            parts, key=lambda x: float(x.get("price", float("inf"))), default=None
        )
        sku = data.get("search_query", None)

        categories = data.get("categories", {})
        category_name = next(
            (
                category["name"]
                for category in categories.values()
                if category.get("part_count", 0) > 0
            ),
            None,
        )

        if min_price_part and category_name:
            manufacturer_code = min_price_part.get("manufacturer_code", None)
            if not manufacturer_code:
                continue
            delivery_price_str = min_price_part.get("delivery_price", None)
            if delivery_price_str:
                delivery_price_str = delivery_price_str.replace(" ‚Ç¨", "")
            else:
                delivery_price_str = "0"

            price_str = min_price_part.get("price", "0")
            if not price_str:
                price_str = "0"

            delivery_price = float(delivery_price_str)
            price = float(price_str)

            result = {
                "–ë—Ä–µ–Ω–¥": min_price_part.get("car", {}).get("manufacturer", None),
                "–ö–æ–¥": sku,
                "K–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è": manufacturer_code,
                "–û–ø–∏—Å–∞–Ω–∏–µ": f"{category_name} | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ì–∞—Ä–∞–Ω—Ç—ñ—è  –Ω–∞ –≤–µ—Å—å —Ç–æ–≤–∞—Ä | –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏ —É –Ω–∞—Å –≤ –°–¢–û | –ó–∞–ø—á–∞—Å—Ç–∏–Ω–∏ –∑ –Ñ–≤—Ä–æ-—Ä–æ–∑–±–æ—Ä—ñ–≤ | –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å | –¢–µ–ª–µ—Ñ–æ–Ω—É–π—Ç–µ | –ú–∏—Ä–Ω–æ–≥–æ –¥–Ω—è.",
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price + price,
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞": price,
                "–¶–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price,
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –®–¢.": "1",
                "–ë/–£": "1",
                "–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞": None,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if category_name not in category_data:
                category_data[category_name] = []
            category_data[category_name].append(result)
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é {category_name}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category_name, data in category_data.items():
        if data:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –∑–∞–º–µ–Ω—è—è –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            safe_category_name = "".join(
                c for c in category_name if c.isalnum() or c in (" ", "-", "_")
            )

            file_name = data_directory / f"{safe_category_name}.xlsx"
            df = pd.DataFrame(data)
            df.to_excel(file_name, index=False)
            logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category_name}': {file_name}")


def extract_data_code():
    all_data = []
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_code_directory.glob("*.html"):
        with open(html_file, encoding="utf-8") as file:
            src = file.read()
        soup = BeautifulSoup(src, "lxml")
        code_tag = soup.find_all("button", attrs={"data-testid": "part-code"})
        if not code_tag:
            logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–¥ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue
        for code in code_tag:
            code_text = code.text.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            all_data.append(code_text)
    save_code_csv(all_data)


def save_code_csv(data):
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º "code"
    df = pd.DataFrame(data, columns=["code"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV —Ñ–∞–π–ª
    output_file = output_csv_file  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∏ –∏–º—è —Ñ–∞–π–ª–∞
    df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(
        f"–í—Å–µ –∫–æ–¥—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}"
    )


def parse_curl_from_file(file_path="config.txt"):
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª config.txt
    with open(file_path, "r", encoding="utf-8") as f:
        curl_data = f.read().strip()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è headers –∏ cookies
    headers = {}
    cookies = {}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (-H)
    header_matches = re.findall(r"-H\s+'([^']+)'", curl_data)
    for header in header_matches:
        key, value = header.split(": ", 1)  # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª—é—á –∏ –∑–Ω–∞—á–µ–Ω–∏–µ
        headers[key.lower()] = value  # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—É–∫–∏ (-b)
    cookie_match = re.search(r"-b\s+'([^']+)'", curl_data)
    if cookie_match:
        cookie_string = cookie_match.group(1)
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –∫—É–∫ –≤ —Å–ª–æ–≤–∞—Ä—å
        cookie_pairs = cookie_string.split("; ")
        for pair in cookie_pairs:
            if "=" in pair:
                key, value = pair.split("=", 1)
                cookies[key] = value

    return headers, cookies


def filter_required_data(headers, cookies):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è headers
    required_headers_keys = {
        "accept",
        "accept-language",
        "x-requested-with",
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è cookies
    required_cookies_keys = {
        "ci_session",
        "ff_ux_sid",
        "cart_session",
        "CookieConsent",
        "soundestID",
        "omnisendSessionID",
        "disable_ovoko_modal",
        "wishlist",
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º headers
    filtered_headers = {
        key: headers[key] for key in required_headers_keys if key in headers
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º cookies
    filtered_cookies = {
        key: cookies[key] for key in required_cookies_keys if key in cookies
    }

    return filtered_headers, filtered_cookies


def main_config():
    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    headers, cookies = parse_curl_from_file("config.txt")

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
    filtered_headers, filtered_cookies = filter_required_data(headers, cookies)

    return filtered_headers, filtered_cookies


if __name__ == "__main__":
    # # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–¥—ã –∑–∞–ø—á–∞—Å—Ç–µ–π
    # get_all_code(3)
    # # # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–¥—ã –∑–∞–ø—á–∞—Å—Ç–µ–π
    # extract_data_code()
    # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
    urls = read_urls(output_csv_file)
    for url in urls[:1]:
        get_all_page_html(url)
    # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
    # extract_data_product()
