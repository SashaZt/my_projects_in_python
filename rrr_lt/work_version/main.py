import asyncio
import os
import re
import sys
from pathlib import Path

import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from loguru import logger

# –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤
from database import export_to_excel

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤
from main_th import process_products_with_threads

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤ –∑–∞–ø—á–∞—Å—Ç–µ–π
from main_th_queue import process_pages_with_threads_code

current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
config_directory = current_directory / "config"
data_directory = current_directory / "data"
config_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"
env_file_path = config_directory / ".env"
config_file_path = config_directory / "config.txt"

load_dotenv(env_file_path)
API_KEY = os.getenv("API_KEY")
MAX_RETRIES = int(os.getenv("MAX_RETRIES"))
RETRY_DELAY = int(os.getenv("RETRY_DELAY"))
TOTAL_PAGES = int(os.getenv("TOTAL_PAGES"))
NUM_THREADS = int(os.getenv("NUM_THREADS"))

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["code"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return (
            df["code"].dropna().tolist()
        )  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def extract_data_code():
    all_data = []
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_code_directory.glob("*.html"):
        with open(html_file, encoding="utf-8") as file:
            src = file.read()
        soup = BeautifulSoup(src, "lxml")
        code_tag = soup.find_all("button", attrs={"data-testid": "part-code"})
        if not code_tag:
            logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–¥ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue
        for code in code_tag:
            code_text = code.text.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            all_data.append(code_text)
    save_code_csv(all_data)


def save_code_csv(data):
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º "code"
    df = pd.DataFrame(data, columns=["code"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV —Ñ–∞–π–ª
    output_file = output_csv_file  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∏ –∏–º—è —Ñ–∞–π–ª–∞
    df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(
        f"–í—Å–µ –∫–æ–¥—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}"
    )


def main_config():
    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
    headers, cookies = parse_curl_from_file()

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
    filtered_headers, filtered_cookies = filter_required_data(headers, cookies)

    return filtered_headers, filtered_cookies


def parse_curl_from_file():
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª config.txt
    with open(config_file_path, "r", encoding="utf-8") as f:
        curl_data = f.read().strip()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è headers –∏ cookies
    headers = {}
    cookies = {}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (-H)
    header_matches = re.findall(r"-H\s+'([^']+)'", curl_data)
    for header in header_matches:
        key, value = header.split(": ", 1)  # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª—é—á –∏ –∑–Ω–∞—á–µ–Ω–∏–µ
        headers[key.lower()] = value  # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—É–∫–∏ (-b)
    cookie_match = re.search(r"-b\s+'([^']+)'", curl_data)
    if cookie_match:
        cookie_string = cookie_match.group(1)
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –∫—É–∫ –≤ —Å–ª–æ–≤–∞—Ä—å
        cookie_pairs = cookie_string.split("; ")
        for pair in cookie_pairs:
            if "=" in pair:
                key, value = pair.split("=", 1)
                cookies[key] = value

    return headers, cookies


def filter_required_data(headers, cookies):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è headers
    required_headers_keys = {
        "accept",
        "accept-language",
        "x-requested-with",
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è cookies
    required_cookies_keys = {
        "ci_session",
        "ff_ux_sid",
        "cart_session",
        "CookieConsent",
        "soundestID",
        "omnisendSessionID",
        "disable_ovoko_modal",
        "wishlist",
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º headers
    filtered_headers = {
        key: headers[key] for key in required_headers_keys if key in headers
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º cookies
    filtered_cookies = {
        key: cookies[key] for key in required_cookies_keys if key in cookies
    }

    return filtered_headers, filtered_cookies


def main_loop():
    while True:
        print(
            "\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:\n"
            "1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤ —Å —Å–∞–π—Ç–∞\n"
            "2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–æ–≤ –∑–∞–ø—á–∞—Å—Ç–µ–π\n"
            "3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–ø—á–∞—Å—Ç–µ–π\n"
            "4. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ï–∫—Å–µ–ª—å\n"
            "5. –£–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n"
            "0. –í—ã—Ö–æ–¥"
        )
        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è: ")
        if choice == "1":
            process_pages_with_threads_code(
                total_pages=TOTAL_PAGES,
                num_threads=NUM_THREADS,
                api_key=API_KEY,
                html_code_directory=html_code_directory,
                max_retries=MAX_RETRIES,
                delay=RETRY_DELAY,
            )

        elif choice == "2":
            # –∏–∑–≤–ª–µ—á—å_–∫–æ–¥–æ–≤ –∑–∞–ø—á–∞—Å—Ç–µ–π
            extract_data_code()

        elif choice == "3":
            urls = read_urls(output_csv_file)
            headers, cookies = main_config()
            process_products_with_threads(
                id_products=urls,
                num_threads=NUM_THREADS,
                api_key=API_KEY,
                base_url="https://rrr.lt/ru/poisk",
                headers=headers,
                cookies=cookies,
                json_product_directory=json_product_directory,
                max_retries=MAX_RETRIES,
                delay=RETRY_DELAY,
            )
        elif choice == "4":
            # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–µ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(export_to_excel())
            loop.close()
        elif choice == "5":
            for file in html_code_directory.glob("*.html"):
                file.unlink()
            for file in json_product_directory.glob("*.json"):
                file.unlink()
            print("–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")
        elif choice == "0":
            break
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")


if __name__ == "__main__":
    main_loop()
