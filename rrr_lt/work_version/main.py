import json
import sys
import time
import urllib.parse
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

from config import COOKIES, HEADERS
from main_th import process_products_with_threads
from main_th_queue import process_pages_with_threads_code
from scrap_product import extract_data_product

current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"


API_KEY = "6c54502fd688c7ce737f1c650444884a"
# API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["code"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return (
            df["code"].dropna().tolist()
        )  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


# # –°–∫–∞—á–∏–≤–∞–µ–º –∫–æ–¥—ã —Ç–æ–≤–∞—Ä–æ–≤
# process_pages_with_threads_code(
#     total_pages=3,
#     num_threads=50,
#     api_key=API_KEY,
#     html_code_directory=html_code_directory,
#     max_retries=MAX_RETRIES,
#     delay=RETRY_DELAY,
# )


def extract_data_code():
    all_data = []
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_code_directory.glob("*.html"):
        with open(html_file, encoding="utf-8") as file:
            src = file.read()
        soup = BeautifulSoup(src, "lxml")
        code_tag = soup.find_all("button", attrs={"data-testid": "part-code"})
        if not code_tag:
            logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–¥ –≤ —Ñ–∞–π–ª–µ {html_file}")
            continue
        for code in code_tag:
            code_text = code.text.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            all_data.append(code_text)
    save_code_csv(all_data)


def save_code_csv(data):
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º "code"
    df = pd.DataFrame(data, columns=["code"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV —Ñ–∞–π–ª
    output_file = output_csv_file  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∏ –∏–º—è —Ñ–∞–π–ª–∞
    df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(
        f"–í—Å–µ –∫–æ–¥—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}"
    )


# # –ó–∞–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ç–æ–≤–∞—Ä–∞–º–∏
# urls = read_urls(output_csv_file)
# process_products_with_threads(
#     id_products=urls,
#     num_threads=10,
#     api_key=API_KEY,
#     base_url="https://rrr.lt/ru/poisk",
#     headers=HEADERS,
#     cookies=COOKIES,
#     json_product_directory=json_product_directory,
#     max_retries=MAX_RETRIES,
#     delay=RETRY_DELAY,
# )
if __name__ == "__main__":
    extract_data_product()
