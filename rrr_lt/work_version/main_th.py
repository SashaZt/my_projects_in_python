import asyncio
import sys
import threading
import time
import urllib.parse
from pathlib import Path
from queue import Empty, Queue
from typing import Dict, List, Optional

import requests
from database import (
    create_database,
    extract_and_save_product,
    get_all_codes,
    get_all_codes_products,
)
from loguru import logger
from tqdm import tqdm

current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


class ThreadedScraper:
    def __init__(
        self,
        num_threads: int,
        api_key: str,
        base_url: str,
        headers: Dict,
        cookies: Dict,
        json_product_directory: Path,
        max_retries: int = 95,
        delay: int = 30,
    ):
        self.num_threads = num_threads
        self.api_key = api_key
        self.base_url = base_url
        self.headers = headers.copy()
        self.cookies = cookies
        self.json_product_directory = json_product_directory
        self.max_retries = max_retries
        self.delay = delay
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        self.total_tasks = 0
        self.progress_bar = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
        self.task_queue = Queue()
        self.active_threads = []
        self.stop_event = threading.Event()

        # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        self.processed_count = 0
        self.processed_lock = threading.Lock()

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É Cookie –∏–∑ —Å–ª–æ–≤–∞—Ä—è cookies
        cookie_string = "; ".join(f"{key}={value}" for key, value in cookies.items())
        self.headers["Cookie"] = cookie_string
        # –°–æ–∑–¥–∞–µ–º –ë–î –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        asyncio.run(create_database())
        codes_data = asyncio.run(get_all_codes_products())
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.existing_search_queries = set(row[0] for row in codes_data if row[0])
        self.existing_codes = set(row[1] for row in codes_data if row[1])

    def make_request_with_retries(
        self, url: str, params: Dict, headers: Optional[Dict] = None
    ) -> Optional[requests.Response]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        retries = 0
        while retries < self.max_retries and not self.stop_event.is_set():
            try:
                response = requests.get(url, params=params, headers=headers, timeout=60)
                if response.status_code == 200:
                    return response
                else:
                    logger.warning(
                        f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. "
                        f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                    )
            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. "
                    f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                )
            retries += 1
            if not self.stop_event.is_set():
                time.sleep(self.delay)

        logger.error(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}"
        )
        return None

    def process_product(self, id_product: str) -> None:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç"""
        try:
            # json_file = self.json_product_directory / f"{id_product}.json"
            # if json_file.exists():
            #     with self.processed_lock:
            #         self.processed_count += 1
            #         if self.progress_bar:
            #             self.progress_bar.update(1)
            #     return
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ code –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–∞
            if (
                id_product in self.existing_search_queries
                or id_product in self.existing_codes
            ):
                # logger.info(f"–ü—Ä–æ–¥—É–∫—Ç {id_product} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            query_params = {"q": id_product, "prs": "2", "page": "1"}
            full_url = f"{self.base_url}?{urllib.parse.urlencode(query_params)}"

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
            payload = {
                "api_key": self.api_key,
                "url": full_url,
                "keep_headers": "true",
            }

            response = self.make_request_with_retries(
                "https://api.scraperapi.com/", payload, headers=self.headers
            )
            if response:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(
                    extract_and_save_product(response.text, id_product)
                )
                loop.close()
                # if response:
                #     with open(json_file, "w", encoding="utf-8") as file:
                #         file.write(response.text)
                #     loop = asyncio.new_event_loop()
                #     asyncio.set_event_loop(loop)
                #     loop.run_until_complete(
                #         extract_and_save_product(response.text, json_file)
                #     )
                #     loop.close()
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∫–æ–¥ –≤ —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                self.existing_codes.add(id_product)

                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {id_product}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞ {id_product}: {e}")

    def worker(self) -> None:
        """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–¥–∞—á–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        thread_name = threading.current_thread().name
        # logger.debug(f"–ó–∞–ø—É—â–µ–Ω –ø–æ—Ç–æ–∫ {thread_name}")

        while not self.stop_event.is_set():
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                try:
                    id_product = self.task_queue.get(timeout=1)
                except Empty:
                    continue

                try:
                    self.process_product(id_product)
                except Exception as e:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ –ø–æ—Ç–æ–∫–µ {thread_name}: {e}"
                    )
                finally:
                    self.task_queue.task_done()

            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ {thread_name}: {e}")
                if not self.stop_event.is_set():
                    time.sleep(1)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π

        # logger.debug(f"–ü–æ—Ç–æ–∫ {thread_name} –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")

    def start(self) -> None:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—É–ª —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤"""
        self.stop_event.clear()
        self.processed_count = 0

        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏
        for _ in range(self.num_threads):
            thread = threading.Thread(target=self.worker, daemon=True)
            thread.start()
            self.active_threads.append(thread)

        logger.info(f"–ó–∞–ø—É—â–µ–Ω–æ {self.num_threads} —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤")

    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏"""
        self.stop_event.set()

        for thread in self.active_threads:
            thread.join()

        self.active_threads.clear()
        if self.progress_bar:
            self.progress_bar.close()

    def add_task(self, id_product: str) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥—å"""
        self.task_queue.put(id_product)

    def add_tasks(self, id_products: List[str]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥—å"""
        self.total_tasks = len(id_products)
        self.progress_bar = tqdm(
            total=self.total_tasks,
            desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤",
            bar_format="{l_bar}{bar} | –í—Ä–µ–º—è: {elapsed} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining} | –°–∫–æ—Ä–æ—Å—Ç—å: {rate_fmt}",
        )
        for id_product in id_products:
            self.task_queue.put(id_product)

    def wait_completion(self) -> None:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥–∏"""
        self.task_queue.join()

    @property
    def tasks_processed(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        return self.processed_count

    @property
    def queue_size(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏"""
        return self.task_queue.qsize()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def process_products_with_threads(
    id_products: List[str], num_threads: int = 95, **kwargs
) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤

    Args:
        id_products: –°–ø–∏—Å–æ–∫ ID –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        num_threads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ThreadedScraper
    """
    scraper = ThreadedScraper(num_threads=num_threads, **kwargs)

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏
        scraper.start()

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å
        scraper.add_tasks(id_products)

        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        scraper.wait_completion()

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {scraper.tasks_processed}")

    finally:
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏
        scraper.stop()
