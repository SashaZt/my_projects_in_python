import asyncio
import sys
import threading
import time
import urllib.parse
from pathlib import Path
from queue import Empty, Queue
from typing import Dict, Optional

import requests
from database import create_database, extract_and_save_codes
from loguru import logger
from tqdm import tqdm

current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


class ThreadedScraperCode:
    def __init__(
        self,
        num_threads: int,
        api_key: str,
        html_code_directory: Path,
        max_retries: int = 95,
        delay: int = 30,
    ):
        self.num_threads = num_threads
        self.api_key = api_key
        self.html_code_directory = html_code_directory
        self.max_retries = max_retries
        self.delay = delay

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
        self.task_queue = Queue()
        self.active_threads = []
        self.stop_event = threading.Event()

        # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        self.processed_count = 0
        self.processed_lock = threading.Lock()

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        self.total_tasks = 0
        self.progress_bar = None

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
        self.headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-language": "ru,en;q=0.9,uk;q=0.8",
            "x-requested-with": "XMLHttpRequest",
        }
        asyncio.run(create_database())

    def make_request_with_retries(
        self, url: str, params: Dict, headers: Optional[Dict] = None
    ) -> Optional[requests.Response]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        retries = 0
        while retries < self.max_retries and not self.stop_event.is_set():
            try:
                response = requests.get(url, params=params, headers=headers, timeout=60)
                if response.status_code == 200:
                    return response
                else:
                    logger.warning(
                        f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. "
                        f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                    )
            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. "
                    f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                )
            retries += 1
            if not self.stop_event.is_set():
                time.sleep(self.delay)

        logger.error(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}"
        )
        return None

    def process_page(self, page: int) -> None:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        # logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
        try:
            html_file = self.html_code_directory / f"page_{page}.html"
            if html_file.exists():
                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
                return
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
            payload = {
                "api_key": self.api_key,
                "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                # 'render': 'true'  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
            }
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if page == 0:
                url = "https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey"
                payload["url"] = f"{url}"
            else:
                url = f"https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey?page={page}"
                params = {
                    "page": page,
                }
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
                payload["url"] = f"{url}?{urllib.parse.urlencode(params)}"

            response = self.make_request_with_retries(
                "https://api.scraperapi.com/", payload, headers=self.headers
            )

            if response:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(extract_and_save_codes(response.text))
                loop.close()
                with open(html_file, "w", encoding="utf-8") as file:
                    file.write(response.text)

                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

    def worker(self) -> None:
        """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–¥–∞—á–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        while not self.stop_event.is_set():
            try:
                page = self.task_queue.get(timeout=1)
                try:
                    self.process_page(page)
                finally:
                    self.task_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ {threading.current_thread().name}: {e}")
                time.sleep(1)

    def start(self) -> None:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—É–ª —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤"""
        self.stop_event.clear()
        self.processed_count = 0

        for _ in range(self.num_threads):
            thread = threading.Thread(target=self.worker, daemon=True)
            thread.start()
            self.active_threads.append(thread)

    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏"""
        self.stop_event.set()

        for thread in self.active_threads:
            thread.join()

        self.active_threads.clear()
        if self.progress_bar:
            self.progress_bar.close()

    def add_pages(self, total_pages: int) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.total_tasks = total_pages + 1  # +1 –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã 0
        self.progress_bar = tqdm(
            total=self.total_tasks,
            desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü",
            bar_format="{l_bar}{bar} | –í—Ä–µ–º—è: {elapsed} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining} | –°–∫–æ—Ä–æ—Å—Ç—å: {rate_fmt}",
        )
        for page in range(total_pages + 1):
            self.task_queue.put(page)

    def wait_completion(self) -> None:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥–∏"""
        self.task_queue.join()

    @property
    def tasks_processed(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        return self.processed_count


def process_pages_with_threads_code(
    total_pages: int, num_threads: int = 95, **kwargs
) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤

    Args:
        total_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        num_threads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ThreadedScraperCode
    """
    scraper = ThreadedScraperCode(num_threads=num_threads, **kwargs)

    try:
        scraper.start()
        scraper.add_pages(total_pages)
        scraper.wait_completion()
    finally:
        scraper.stop()
