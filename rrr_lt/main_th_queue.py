import json
import sys
import threading
import time
import urllib.parse
from pathlib import Path
from queue import Empty, Queue
from typing import Dict, List, Optional

import pandas as pd
import requests
from bs4 import BeautifulSoup
from config import COOKIES, HEADERS
from loguru import logger

# API_KEY = "6c54502fd688c7ce737f1c650444884a"
API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö


current_directory = Path.cwd()
html_code_directory = current_directory / "html_code"
json_product_directory = current_directory / "json_product"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_product_directory.mkdir(parents=True, exist_ok=True)
html_code_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


class ThreadedScraperCode:
    def __init__(
        self,
        num_threads: int,
        api_key: str,
        html_code_directory: Path,
        max_retries: int = 10,
        delay: int = 30,
    ):
        self.num_threads = num_threads
        self.api_key = api_key
        self.html_code_directory = html_code_directory
        self.max_retries = max_retries
        self.delay = delay

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
        self.task_queue = Queue()
        self.active_threads = []
        self.stop_event = threading.Event()

        # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        self.processed_count = 0
        self.processed_lock = threading.Lock()

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        self.total_tasks = 0
        self.progress_bar = None

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
        self.headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-language": "ru,en;q=0.9,uk;q=0.8",
            "x-requested-with": "XMLHttpRequest",
        }

    def make_request_with_retries(
        self, url: str, params: Dict, headers: Optional[Dict] = None
    ) -> Optional[requests.Response]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        retries = 0
        while retries < self.max_retries and not self.stop_event.is_set():
            try:
                response = requests.get(url, params=params, headers=headers, timeout=60)
                if response.status_code == 200:
                    return response
                else:
                    logger.warning(
                        f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. "
                        f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                    )
            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. "
                    f"–ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{self.max_retries}."
                )
            retries += 1
            if not self.stop_event.is_set():
                time.sleep(self.delay)

        logger.error(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}"
        )
        return None

    def process_page(self, page: int) -> None:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            html_file = self.html_code_directory / f"page_{page}.html"
            if html_file.exists():
                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if page == 0:
                url = "https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey"
            else:
                url = f"https://rrr.lt/ru/spisok-kodov-zapasnykh-chastey?page={page}"

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
            payload = {
                "api_key": self.api_key,
                "url": url,
                "keep_headers": "true",
            }

            response = self.make_request_with_retries(
                "https://api.scraperapi.com/", payload, headers=self.headers
            )

            if response:
                with open(html_file, "w", encoding="utf-8") as file:
                    file.write(response.text)

                with self.processed_lock:
                    self.processed_count += 1
                    if self.progress_bar:
                        self.progress_bar.update(1)
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

    def worker(self) -> None:
        """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∑–∞–¥–∞—á–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        while not self.stop_event.is_set():
            try:
                page = self.task_queue.get(timeout=1)
                try:
                    self.process_page(page)
                finally:
                    self.task_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ {threading.current_thread().name}: {e}")
                time.sleep(1)

    def start(self) -> None:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—É–ª —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤"""
        self.stop_event.clear()
        self.processed_count = 0

        for _ in range(self.num_threads):
            thread = threading.Thread(target=self.worker, daemon=True)
            thread.start()
            self.active_threads.append(thread)

    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏"""
        self.stop_event.set()

        for thread in self.active_threads:
            thread.join()

        self.active_threads.clear()
        if self.progress_bar:
            self.progress_bar.close()

    def add_pages(self, total_pages: int) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.total_tasks = total_pages + 1  # +1 –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã 0
        self.progress_bar = tqdm(
            total=self.total_tasks,
            desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü",
            bar_format="{l_bar}{bar} | –í—Ä–µ–º—è: {elapsed} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining} | –°–∫–æ—Ä–æ—Å—Ç—å: {rate_fmt}",
        )
        for page in range(total_pages + 1):
            self.task_queue.put(page)

    def wait_completion(self) -> None:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥–∏"""
        self.task_queue.join()

    @property
    def tasks_processed(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        return self.processed_count


def process_pages_with_threads_code(
    total_pages: int, num_threads: int = 50, **kwargs
) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—É–ª–∞ –ø–æ—Ç–æ–∫–æ–≤

    Args:
        total_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        num_threads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ThreadedScraperCode
    """
    scraper = ThreadedScraperCode(num_threads=num_threads, **kwargs)

    try:
        scraper.start()
        scraper.add_pages(total_pages)
        scraper.wait_completion()
    finally:
        scraper.stop()
