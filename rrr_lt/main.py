import asyncio
import json
from pathlib import Path
from typing import Optional, Dict
import random
import logging
import pandas as pd
import json
import time
import urllib.parse
import requests
import time
from urllib.parse import urlparse, urlunparse
import sys
import pandas as pd
import requests
from loguru import logger
from pathlib import Path
from bs4 import BeautifulSoup



API_KEY = "6c54502fd688c7ce737f1c650444884a"
# API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö



current_directory = Path.cwd()
html_directory = current_directory / "html"
json_directory = current_directory / "json"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
def make_request_with_retries(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")
    
    return None

def get_all_page_html(id_product):
    url = f"https://rrr.lt/ru/poisk"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = {
        'accept': 'application/json, text/javascript, */*; q=0.01',
        'accept-language': 'ru,en;q=0.9,uk;q=0.8',
        'x-requested-with': 'XMLHttpRequest'
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    query_params = {
        'q': id_product,
        'prs': '2',
        'page': '1'
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "url": url,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        # 'render': 'true'  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
    }
    json_file = json_directory / f"{id_product}.json"
    if json_file.exists():
        logger.info(f"–§–∞–π–ª {json_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
    payload["url"] = f"{url}?{urllib.parse.urlencode(query_params)}"

    response = make_request_with_retries(
        "https://api.scraperapi.com/", 
        payload, 
        MAX_RETRIES, 
        RETRY_DELAY,
        headers=headers  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    )
    
    if not response:
        raise Exception(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
        )
        
    
    src = response.text
    with open(json_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–°–∫–∞—á–∞–Ω–æ {json_file}")



def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["id"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return df["url"].dropna().tolist()  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []
def extract_data():
    all_data = []
    for json_file in json_directory.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file}")
        # –ù–∞–π—Ç–∏ –¥–µ—Ç–∞–ª—å "–û—Ö–ª–∞–¥–∏—Ç–µ–ª—å EGR" —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–Ω–æ–π
        parts = data.get("parts", [])
        if not parts:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –≤ JSON.")
            continue
        min_price_part = min(
            parts,
            key=lambda x: float(x.get("price", float("inf"))),
            default=None
        )
        sku = data.get("search_query", None)
        
        # –ù–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é "–°–∏—Å—Ç–µ–º–∞ –≤—ã–±—Ä–æ—Å–∞ –≥–∞–∑–æ–≤" —Å part_count > 0
        categories = data.get("categories", {})
        category_name = next(
            (category["name"] for category in categories.values() if category.get("part_count", 0) > 0),
            None
        )

        if min_price_part and category_name:
            result = {
                "–ë—Ä–µ–Ω–¥": min_price_part.get("car", {}).get("manufacturer", None),
                "–ö–æ–¥": sku,
                "K–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è": data.get("manufacturer_code", None),
                "–û–ø–∏—Å–∞–Ω–∏–µ": f"{category_name} | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ì–∞—Ä–∞–Ω—Ç—ñ—è  –Ω–∞ –≤–µ—Å—å —Ç–æ–≤–∞—Ä | –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏ —É –Ω–∞—Å –≤ –°–¢–û | –ó–∞–ø—á–∞—Å—Ç–∏–Ω–∏ –∑ –Ñ–≤—Ä–æ-—Ä–æ–∑–±–æ—Ä—ñ–≤ | –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å | –¢–µ–ª–µ—Ñ–æ–Ω—É–π—Ç–µ | –ú–∏—Ä–Ω–æ–≥–æ –¥–Ω—è.",
                "–¶–µ–Ω–∞": min_price_part["price"],
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –®–¢.": "1",
                "–ë/–£": "1",
                "–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞": None,
                # "part_name": min_price_part["part_name"],
                
                
            }
            logger.info(f"–ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {result}")
            all_data.append(result)
    if all_data:
        df = pd.DataFrame(all_data)
        df.to_excel(xlsx_result, index=False)
        logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {xlsx_result}")
if __name__ == "__main__":
    # urls = read_urls(output_csv_file)
    # for url in urls[:101]:
    #     get_all_page_html(url)
    extract_data()