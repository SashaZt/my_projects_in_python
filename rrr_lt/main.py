import asyncio
import json
from pathlib import Path
from typing import Optional, Dict
import random
import logging
import pandas as pd
import json
import time
import urllib.parse
import requests
import time
from urllib.parse import urlparse, urlunparse
import sys
import pandas as pd
import requests
from loguru import logger
from pathlib import Path
from bs4 import BeautifulSoup



API_KEY = "6c54502fd688c7ce737f1c650444884a"
# API_KEY = "b7141d2b54426945a9f0bf6ab4c7bc54"
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
MAX_RETRIES = 10
RETRY_DELAY = 30  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö



current_directory = Path.cwd()
html_directory = current_directory / "html"
json_directory = current_directory / "json"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
json_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
xlsx_result = data_directory / "result.xlsx"
output_csv_file = data_directory / "output.csv"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
def make_request_with_retries(url, params, max_retries=10, delay=30, headers=None):
    """
    –î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.
        max_retries (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫.
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        headers (dict): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.

    Returns:
        Response | None: –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers, timeout=60)
            if response.status_code == 200:
                return response
            else:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
                )
        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ü–æ–ø—ã—Ç–∫–∞ {retries + 1}/{max_retries}."
            )
        retries += 1
        time.sleep(delay)

    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")
    
    return None

def get_all_page_html(id_product):
    url = f"https://rrr.lt/ru/poisk"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = {
        'accept': 'application/json, text/javascript, */*; q=0.01',
        'accept-language': 'ru,en;q=0.9,uk;q=0.8',
        'x-requested-with': 'XMLHttpRequest'
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    query_params = {
        'q': id_product,
        'prs': '2',
        'page': '1'
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ScraperAPI
    payload = {
        "api_key": API_KEY,
        "url": url,
        "keep_headers": "true",  # –í–∞–∂–Ω–æ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        # 'render': 'true'  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ JavaScript
    }
    json_file = json_directory / f"{id_product}.json"
    if json_file.exists():
        logger.info(f"–§–∞–π–ª {json_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ URL
    payload["url"] = f"{url}?{urllib.parse.urlencode(query_params)}"

    response = make_request_with_retries(
        "https://api.scraperapi.com/", 
        payload, 
        MAX_RETRIES, 
        RETRY_DELAY,
        headers=headers  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    )
    
    if not response:
        raise Exception(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."
        )
        
    
    src = response.text
    with open(json_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–°–∫–∞—á–∞–Ω–æ {json_file}")



def read_urls(csv_path):
    """–ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL."""
    try:
        df = pd.read_csv(csv_path, usecols=["id"])  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É "url"
        return df["url"].dropna().tolist()  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []
# def extract_data():
#     all_data = []
    
#     for json_file in json_directory.glob("*.json"):
#         with open(json_file, "r", encoding="utf-8") as file:
#             data = json.load(file)
#         logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file}")
#         # –ù–∞–π—Ç–∏ –¥–µ—Ç–∞–ª—å "–û—Ö–ª–∞–¥–∏—Ç–µ–ª—å EGR" —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–Ω–æ–π
#         parts = data.get("parts", [])
#         if not parts:
#             logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –≤ JSON.")
#             continue
#         min_price_part = min(
#             parts,
#             key=lambda x: float(x.get("price", float("inf"))),
#             default=None
#         )
#         sku = data.get("search_query", None)
        
#         # –ù–∞–π—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é "–°–∏—Å—Ç–µ–º–∞ –≤—ã–±—Ä–æ—Å–∞ –≥–∞–∑–æ–≤" —Å part_count > 0
#         categories = data.get("categories", {})
#         category_name = next(
#             (category["name"] for category in categories.values() if category.get("part_count", 0) > 0),
#             None
#         )
        
#         if min_price_part and category_name:
#             delivery_price_str = min_price_part.get("delivery_price", None).replace(" ‚Ç¨", "")
#             price_str = min_price_part["price"]
#             # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (None) –∏–ª–∏ –ø—É—Å—Ç–æ–µ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º "0"
#             if not delivery_price_str:
#                 delivery_price_str = "0"
#             else:
#                 delivery_price_str = delivery_price_str.replace(" ‚Ç¨", "")

#             if not price_str:
#                 price_str = "0"

#             # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–∞ —Ç–∏–ø–∞ float –∏ —Å—É–º–º–∏—Ä—É–µ–º
#             delivery_price = float(delivery_price_str)
#             price = float(price_str)
#             result = {
#                 "–ë—Ä–µ–Ω–¥": min_price_part.get("car", {}).get("manufacturer", None),
#                 "–ö–æ–¥": sku,
#                 "K–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è": data.get("manufacturer_code", None),
#                 "–û–ø–∏—Å–∞–Ω–∏–µ": f"{category_name} | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ì–∞—Ä–∞–Ω—Ç—ñ—è  –Ω–∞ –≤–µ—Å—å —Ç–æ–≤–∞—Ä | –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏ —É –Ω–∞—Å –≤ –°–¢–û | –ó–∞–ø—á–∞—Å—Ç–∏–Ω–∏ –∑ –Ñ–≤—Ä–æ-—Ä–æ–∑–±–æ—Ä—ñ–≤ | –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å | –¢–µ–ª–µ—Ñ–æ–Ω—É–π—Ç–µ | –ú–∏—Ä–Ω–æ–≥–æ –¥–Ω—è.",
#                 "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price + price,
#                 "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞": price,
#                 "–¶–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price,
#                 "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –®–¢.": "1",
#                 "–ë/–£": "1",
#                 "–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞": None,
#                 # "part_name": min_price_part["part_name"],
                
                
#             }
#             logger.info(f"–ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {result}")
#             all_data.append(result)
#     if all_data:
#         df = pd.DataFrame(all_data)
#         df.to_excel(xlsx_result, index=False)
#         logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {xlsx_result}")
def extract_data():
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    category_data = {}
    
    for json_file in json_directory.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {json_file}")
        
        parts = data.get("parts", [])
        if not parts:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –≤ JSON.")
            continue
            
        min_price_part = min(
            parts,
            key=lambda x: float(x.get("price", float("inf"))),
            default=None
        )
        sku = data.get("search_query", None)
        
        categories = data.get("categories", {})
        category_name = next(
            (category["name"] for category in categories.values() if category.get("part_count", 0) > 0),
            None
        )
        
        if min_price_part and category_name:
            manufacturer_code = min_price_part.get("manufacturer_code", None)
            if not manufacturer_code:
                continue
            delivery_price_str = min_price_part.get("delivery_price", None)
            if delivery_price_str:
                delivery_price_str = delivery_price_str.replace(" ‚Ç¨", "")
            else:
                delivery_price_str = "0"

            price_str = min_price_part.get("price", "0")
            if not price_str:
                price_str = "0"

            delivery_price = float(delivery_price_str)
            price = float(price_str)
            
            result = {
                "–ë—Ä–µ–Ω–¥": min_price_part.get("car", {}).get("manufacturer", None),
                "–ö–æ–¥": sku,
                "K–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è": manufacturer_code,
                "–û–ø–∏—Å–∞–Ω–∏–µ": f"{category_name} | –û—Ä–∏–≥—ñ–Ω–∞–ª | –ì–∞—Ä–∞–Ω—Ç—ñ—è  –Ω–∞ –≤–µ—Å—å —Ç–æ–≤–∞—Ä | –ì–∞—Ä–∞–Ω—Ç—ñ–π–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø—á–∞—Å—Ç–∏–Ω–∏ —É –Ω–∞—Å –≤ –°–¢–û | –ó–∞–ø—á–∞—Å—Ç–∏–Ω–∏ –∑ –Ñ–≤—Ä–æ-—Ä–æ–∑–±–æ—Ä—ñ–≤ | –í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å | –¢–µ–ª–µ—Ñ–æ–Ω—É–π—Ç–µ | –ú–∏—Ä–Ω–æ–≥–æ –¥–Ω—è.",
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price + price,
                "–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞": price,
                "–¶–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞–≤–∫–∏": delivery_price,
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ, –®–¢.": "1",
                "–ë/–£": "1",
                "–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞": None,
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if category_name not in category_data:
                category_data[category_name] = []
            category_data[category_name].append(result)
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é {category_name}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category_name, data in category_data.items():
        if data:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –∑–∞–º–µ–Ω—è—è –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            safe_category_name = "".join(c for c in category_name if c.isalnum() or c in (' ', '-', '_'))
            file_name = f"{safe_category_name}.xlsx"
            df = pd.DataFrame(data)
            df.to_excel(file_name, index=False)
            logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category_name}': {file_name}")



if __name__ == "__main__":
    # urls = read_urls(output_csv_file)
    # for url in urls[:1]:
    #     url = "19070006031"
    #     get_all_page_html(url)
    extract_data()