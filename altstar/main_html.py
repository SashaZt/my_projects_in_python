import json
import random
import re
import sys
import time
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_directory = current_directory / "html"
log_directory = current_directory / "log"
img_directory = current_directory / "img"

img_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
output_html_file = html_directory / "output.html"
output_csv_file = data_directory / "output.csv"
output_json_file = data_directory / "output.json"
categories_file = current_directory / "category.txt"
BASE_URL = "https://altstar.ua/"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


class AltStarScraper:
    def __init__(self, base_url="https://altstar.ua", category_url="/salniki"):
        self.base_url = base_url
        self.category_url = category_url
        self.full_url = base_url + category_url
        self.cookies = {
            "PHPSESSID": "4u34v9cjpr22r449jev8j5ho40",
            "PHPSESSID": "4u34v9cjpr22r449jev8j5ho40",
            "PHPSESSID": "4u34v9cjpr22r449jev8j5ho40",
            "language": "uk-ua",
            "currency": "UAH",
        }

        self.headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-language": "ru,en;q=0.9,uk;q=0.8",
            "cache-control": "no-cache",
            "dnt": "1",
            "pragma": "no-cache",
            "priority": "u=0, i",
            "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "none",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
            # 'cookie': 'PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; language=uk-ua; currency=UAH',
        }
        self.products_links = []

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_name = category_url.strip("/")
        self.category_dir = data_directory / category_name
        self.category_dir.mkdir(exist_ok=True)

        self.links_file = self.category_dir / "product_links.json"

    def get_page(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            response = requests.get(
                url, cookies=self.cookies, headers=self.headers, timeout=30
            )
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None

    def extract_product_links(self, html):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if not html:
            return []

        soup = BeautifulSoup(html, "lxml")
        links = []

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
        product_elements = soup.find_all("span", class_="crr-cnt")
        for element in product_elements:
            product_url = element.get("data-crr-url")
            if product_url:
                links.append(product_url)
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä: {product_url}")

        return links

    def get_total_pages(self, html):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        if not html:
            return 1

        soup = BeautifulSoup(html, "lxml")
        pagination_info = soup.find("div", class_="col-sm-4 text-right")

        if pagination_info:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ "–í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–æ 1 –ø–æ 20 –∑ 121"
            text = pagination_info.text
            match = re.search(r"–∑ (\d+)", text)
            if match:
                total_items = int(match.group(1))
                items_per_page = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                total_pages = (total_items // items_per_page) + (
                    1 if total_items % items_per_page > 0 else 0
                )
                logger.info(f"–í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {total_items}, —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
                return total_pages

        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 1")
        return 1

    def collect_all_product_links(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ —Ç–æ–≤–∞—Ä—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–≥–æ
        if self.links_file.exists():
            try:
                with open(self.links_file, "r", encoding="utf-8") as f:
                    self.products_links = json.load(f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.products_links)} —Å—Å—ã–ª–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞")
                return self.products_links
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ —Å—Å—ã–ª–æ–∫: {e}")
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                self.products_links = []

        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        first_page_html = self.get_page(self.full_url)
        if not first_page_html:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
            return []

        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        total_pages = self.get_total_pages(first_page_html)

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        links = self.extract_product_links(first_page_html)
        self.products_links.extend(links)

        # –û–±—Ö–æ–¥–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å–æ 2-–π –ø–æ –ø–æ—Å–ª–µ–¥–Ω—é—é)
        for page in range(2, total_pages + 1):
            page_url = f"{self.full_url}?page={page}"
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}/{total_pages}: {page_url}")

            page_html = self.get_page(page_url)
            if page_html:
                page_links = self.extract_product_links(page_html)
                self.products_links.extend(page_links)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                self.save_links()

                # –î–µ–ª–∞–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(1.0, 3.0))
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")

        logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã: {len(self.products_links)}")
        return self.products_links

    def save_links(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –≤ JSON-—Ñ–∞–π–ª"""
        try:
            with open(self.links_file, "w", encoding="utf-8") as f:
                json.dump(self.products_links, f, ensure_ascii=False)
            logger.info(f"–°—Å—ã–ª–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {self.links_file}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")

    def download_product_pages(self):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"""
        if not self.products_links:
            logger.warning("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç, —Å–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ —Å—Å—ã–ª–∫–∏")
            return

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è HTML-—Ñ–∞–π–ª–æ–≤ —Ç–µ–∫—É—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_name = self.category_url.strip("/")
        category_html_dir = html_directory / category_name
        category_html_dir.mkdir(exist_ok=True)

        total = len(self.products_links)
        for i, url in enumerate(self.products_links, 1):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
            # –ü—Ä–∏–º–µ—Ä: https://altstar.ua/f032111123/cargo -> f032111123_cargo.html
            url_parts = url.split("/")
            if len(url_parts) >= 2:
                product_id = url_parts[-2]
                brand = url_parts[-1]
                filename = f"{product_id}_{brand}.html"

                output_file = category_html_dir / filename

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
                if output_file.exists():
                    logger.info(
                        f"[{i}/{total}] –§–∞–π–ª {filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                    )
                    continue

                logger.info(f"[{i}/{total}] –°–∫–∞—á–∏–≤–∞–µ–º {url}")
                html_content = self.get_page(url)

                if html_content:
                    try:
                        with open(output_file, "w", encoding="utf-8") as f:
                            f.write(html_content)
                        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_file}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {filename}: {e}")

                # –î–µ–ª–∞–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(1.5, 4.0))
            else:
                logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL: {url}")

    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {self.category_url}")

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã...")
        self.collect_all_product_links()

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–≤–∞—Ä–æ–≤...")
        self.download_product_pages()

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {self.category_url} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


def read_categories(file_path):
    """–ß—Ç–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
    categories = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            categories = [line.strip() for line in f if line.strip()]
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Ñ–∞–π–ª–∞")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
    return categories


if __name__ == "__main__":
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏

    if not categories_file.exists():
        logger.error(f"–§–∞–π–ª —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {categories_file}")
        sys.exit(1)

    # –ß—Ç–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ —Ñ–∞–π–ª–∞
    categories = read_categories(categories_file)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for i, category in enumerate(categories, 1):
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}/{len(categories)}: {category}")

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_url = f"/{category}"

        # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        scraper = AltStarScraper(category_url=category_url)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
        scraper.run()

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if i < len(categories):
            sleep_time = random.uniform(5.0, 10.0)
            logger.info(f"–ü–∞—É–∑–∞ {sleep_time:.1f} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π...")
            time.sleep(sleep_time)

    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
