import json
import re
import sys
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger
from main_html import main_scraper

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_directory = current_directory / "html"
log_directory = current_directory / "log"
img_directory = current_directory / "img"
output_directory = current_directory / "output"


img_directory.mkdir(parents=True, exist_ok=True)
output_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)

html_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
output_json_file = data_directory / "output.json"
BASE_URL = "https://altstar.ua/"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def get_html():
    cookies = {
        "PHPSESSID": "4u34v9cjpr22r449jev8j5ho40",
        "language": "uk-ua",
        "currency": "UAH",
    }

    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "cache-control": "no-cache",
        "dnt": "1",
        "pragma": "no-cache",
        "priority": "u=0, i",
        "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        # 'cookie': 'PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; language=uk-ua; currency=UAH',
    }

    response = requests.get(
        "https://altstar.ua/1986se3754/bosch",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ü–µ–ª–∏–∫–æ–º
        with open(output_html_file, "w", encoding="utf-8") as file:
            file.write(response.text)
        logger.info(f"Successfully saved {output_html_file}")
    else:
        logger.error(f"Failed to get HTML. Status code: {response.status_code}")


# def scrap_html():
#     # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
#     # Initialize dictionaries for each section
#     left_column_data = {}
#     middle_column_data = {}
#     right_column_data = {}
#     for html_file in html_directory.glob("*.html"):
#         with html_file.open(encoding="utf-8") as file:
#             content = file.read()

#         # Create BeautifulSoup object
#         soup = BeautifulSoup(content, "lxml")

#         # Find the main product-info div
#         product_info = soup.find("div", class_="product-info")

#         if not product_info:
#             return {"error": "Product info div not found"}

#         # =============================================
#         # Extract from first column (left)
#         # =============================================
#         left_column = product_info.find("div", class_="col-sm-3 col-xs-12")

#         if left_column:
#             # Extract product name from ribbon
#             ribbon_name = left_column.find("h2", class_="ribbon_name single_product")
#             if ribbon_name:
#                 # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª—è–µ–º –∫—Ä–∞–π–Ω–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#                 product_name = ribbon_name.text.strip()

#                 # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ
#                 product_name = product_name.replace("\xa0", "_").replace(" ", "_")

#                 # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –∑–∞–º–µ–Ω—è—é—â–∏–π –≤—Å–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#                 # product_name = ''.join(c if not c.isspace() else '_' for c in product_name)

#                 left_column_data["product_name"] = product_name

#             # Extract price
#             price_elem = left_column.find("span", id="price-old")
#             if price_elem:
#                 left_column_data["price"] = price_elem.text.strip()

#             # Extract all image links using a set to avoid duplicates
#             image_links = set()

#             # Main product image
#             main_image = left_column.find("a", class_="MagicZoom")
#             if main_image and "href" in main_image.attrs:
#                 image_links.add(main_image["href"])

#             # Thumbnail images in MagicScroll
#             magic_scroll = left_column.find("div", class_="MagicScroll")
#             if magic_scroll:
#                 thumbnails = magic_scroll.find_all("a")
#                 for thumb in thumbnails:
#                     if "href" in thumb.attrs:
#                         image_links.add(f'{BASE_URL}{thumb["href"]}')

#             # Other thumbnail images
#             other_thumbs = left_column.find_all("a", class_="mz-thumb")
#             for thumb in other_thumbs:
#                 if "href" in thumb.attrs:
#                     image_links.add('{BASE_URL}{thumb["href"]}')

#             left_column_data["image_links"] = list(image_links)

#         # =============================================
#         # Extract from middle column
#         # =============================================
#         middle_column = product_info.find("div", class_="col-sm-4 col-xs-12")

#         if middle_column:
#             # Extract brand
#             # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
#             # –ò—â–µ–º span, –∫–æ—Ç–æ—Ä—ã–π –°–û–î–ï–†–ñ–ò–¢ —Ç–µ–∫—Å—Ç "–ë—Ä–µ–Ω–¥", –∞ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –µ–º—É
#             brand_span = middle_column.find(
#                 "span", string=lambda text: text and "–ë—Ä–µ–Ω–¥:" in text if text else False
#             )

#             if brand_span:
#                 # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫—É <a> –≤–Ω—É—Ç—Ä–∏ span
#                 brand_link = brand_span.find("a")
#                 if brand_link:
#                     brand = brand_link.find("b")
#                     if brand:
#                         middle_column_data["brand"] = brand.text.strip()
#                         logger.info(f"Extracted brand from link: {brand.text.strip()}")
#                 else:
#                     # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–≥ <b>
#                     brand = brand_span.find("b")
#                     if brand:
#                         middle_column_data["brand"] = brand.text.strip()
#                         logger.info(f"Extracted brand from span: {brand.text.strip()}")
#             else:
#                 # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—â–µ–º –ª—é–±–æ–π span —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –ø—Ä–æ –±—Ä–µ–Ω–¥
#                 for span in middle_column.find_all("span"):
#                     if span.text and "–ë—Ä–µ–Ω–¥:" in span.text:
#                         # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–≥ <b> –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ span
#                         brand = span.find("b")
#                         if brand:
#                             middle_column_data["brand"] = brand.text.strip()
#                             break

#             name_product = middle_column.find("span", class_="product_cat_name prodttl")
#             if name_product:
#                 middle_column_data["name_product"] = name_product.text.strip()
#             # Extract characteristics
#             chars_div = middle_column.find("div", class_="attrs table")
#             if chars_div:
#                 characteristics = {}
#                 char_rows = chars_div.find_all("div", class_="detail-chars")

#                 for row in char_rows:
#                     title_div = row.find("div", class_="detail-chars-title")
#                     field_div = row.find("div", class_="detail-chars-field")

#                     if title_div and field_div:
#                         title = title_div.find("span", class_="detail-chars-title-name")
#                         if title:
#                             char_name = title.text.strip()
#                             char_value = field_div.text.strip()
#                             characteristics[char_name] = char_value

#                 middle_column_data["characteristics"] = characteristics

#             # Extract "–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö" data
#             aggregates_div = middle_column.find("div", class_="product-applicability")
#             if aggregates_div:
#                 aggregates_parts = []

#                 for div in aggregates_div.find_all("div", recursive=False):
#                     brand_elem = div.find("span", class_="s-s")
#                     if brand_elem and brand_elem.find("b"):
#                         brand_name = brand_elem.find("b").text.strip().rstrip(":")

#                         # Get all links (part numbers) for this brand
#                         links = div.find("div", class_="more").find_all("a")
#                         part_numbers = [link.text.strip() for link in links]

#                         # Format as requested: Brand:part1!part2!part3
#                         if part_numbers:
#                             aggregates_parts.append(
#                                 f"{brand_name}:{'!'.join(part_numbers)}"
#                             )

#                 # Join all brands with /
#                 middle_column_data["–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö"] = "/".join(
#                     aggregates_parts
#                 )

#         # =============================================
#         # Extract from right column
#         # =============================================
#         right_column = product_info.find("div", class_="col-sm-5 col-xs-12")

#         if right_column:
#             # Extract "–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"
#             analogs_div = right_column.find("div", class_="analogs")
#             if analogs_div:
#                 analogs_parts = []

#                 # Find all brand sections
#                 for div in analogs_div.find_all("div", recursive=False):
#                     if div.find("span") and div.find("span").find("b"):
#                         brand_name = div.find("span").find("b").text.strip().rstrip(":")

#                         # Extract part numbers from links
#                         part_numbers = []
#                         for a_tag in div.find_all("a"):
#                             part_numbers.append(a_tag.text.strip())

#                         # Format as requested: Brand:part1!part2!part3
#                         if part_numbers:
#                             analogs_parts.append(
#                                 f"{brand_name}:{'!'.join(part_numbers)}"
#                             )

#                 # Join all brands with /
#                 right_column_data["–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = "/".join(analogs_parts)

#             # Extract "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"
#             applications_div = right_column.find("div", class_="by_car")
#             if applications_div:
#                 applications_span = applications_div.find("span")
#                 if applications_span:
#                     applications_text = applications_span.text.strip()

#                     # Process the applications text to merge by manufacturer
#                     lines = [
#                         line.strip()
#                         for line in applications_text.split("\n")
#                         if line.strip()
#                     ]

#                     # Process the applications text to merge by manufacturer
#                     # Get raw text from the span
#                     raw_text = applications_span.get_text()

#                     # Process the raw text to identify manufacturer patterns
#                     import re

#                     # Split the raw text into lines for processing
#                     lines = [
#                         line.strip() for line in raw_text.split("\n") if line.strip()
#                     ]

#                     # Dictionary to store manufacturer -> models mapping
#                     manufacturer_models = {}

#                     # Process each line
#                     for line in lines:
#                         # Identify manufacturer - first all uppercase word in the line
#                         match = re.match(r"^([A-Z]+)\s", line)
#                         if match:
#                             manufacturer = match.group(1)
#                             model_info = line[len(manufacturer) :].strip()

#                             if manufacturer not in manufacturer_models:
#                                 manufacturer_models[manufacturer] = []

#                             manufacturer_models[manufacturer].append(model_info)

#                     # Format as requested
#                     applications = []
#                     for manufacturer, models in manufacturer_models.items():
#                         models_text = "!".join(models)
#                         applications.append(f"{manufacturer} {models_text}")

#                     right_column_data["–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"] = applications


#         # Combine all data
#         result = {
#             "left_column": left_column_data,
#             "middle_column": middle_column_data,
#             "right_column": right_column_data,
#         }
#     with open(output_json_file, "w", encoding="utf-8") as f:
#         json.dump(result, f, ensure_ascii=False, indent=4)
#     return result
# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –∏–ª–∏ —Ñ–∞–π–ª, –≤–æ—Ç –ø—Ä–∏–º–µ—Ä:
def scrap_html():
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
    products_by_directory = {}

    # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    processed_files = 0
    total_files = 0

    # –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ HTML —Ñ–∞–π–ª–æ–≤ –≤–æ –≤—Å–µ—Ö –ø–∞–ø–∫–∞—Ö
    for html_path in html_directory.glob("**/*.html"):
        total_files += 1

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ HTML —Ñ–∞–π–ª–æ–≤: {total_files}")

    # –û–±—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞–ø–∫–∏ –≤–Ω—É—Ç—Ä–∏ html_directory –∏ –∏—â–µ–º HTML —Ñ–∞–π–ª—ã
    for html_path in html_directory.glob("**/*.html"):
        try:
            logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ ({processed_files + 1}/{total_files}): {html_path}"
            )

            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ñ–∞–π–ª
            directory_name = html_path.parent.name

            # –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –µ—â–µ –Ω–µ –≤ —Å–ª–æ–≤–∞—Ä–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–µ
            if directory_name not in products_by_directory:
                products_by_directory[directory_name] = []

            with html_path.open(encoding="utf-8") as file:
                content = file.read()

            # Initialize dictionaries for each section
            left_column_data = {}
            middle_column_data = {}
            right_column_data = {}

            # Create BeautifulSoup object
            soup = BeautifulSoup(content, "lxml")

            # Find the main product-info div
            product_info = soup.find("div", class_="product-info")

            if not product_info:
                logger.warning(
                    f"–ù–µ –Ω–∞–π–¥–µ–Ω div —Å –∫–ª–∞—Å—Å–æ–º product-info –≤ —Ñ–∞–π–ª–µ {html_path}"
                )
                processed_files += 1
                continue

            # =============================================
            # Extract from first column (left)
            # =============================================
            left_column = product_info.find("div", class_="col-sm-3 col-xs-12")

            if left_column:
                # Extract product name from ribbon
                ribbon_name = left_column.find(
                    "h2", class_="ribbon_name single_product"
                )
                if ribbon_name:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª—è–µ–º –∫—Ä–∞–π–Ω–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                    product_name = ribbon_name.text.strip()

                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ
                    product_name = product_name.replace("\xa0", "_").replace(" ", "_")

                    left_column_data["product_name"] = product_name

                # Extract price
                price_elem = left_column.find("span", id="price-old")
                if price_elem:
                    left_column_data["price"] = price_elem.text.strip()

                # Extract all image links using a set to avoid duplicates
                image_links = set()

                # Main product image
                main_image = left_column.find("a", class_="MagicZoom")
                if main_image and "href" in main_image.attrs:
                    image_links.add(main_image["href"])

                # Thumbnail images in MagicScroll
                magic_scroll = left_column.find("div", class_="MagicScroll")
                if magic_scroll:
                    thumbnails = magic_scroll.find_all("a")
                    for thumb in thumbnails:
                        if "href" in thumb.attrs:
                            image_links.add(f'{BASE_URL}{thumb["href"]}')

                # Other thumbnail images
                other_thumbs = left_column.find_all("a", class_="mz-thumb")
                for thumb in other_thumbs:
                    if "href" in thumb.attrs:
                        image_links.add(f'{BASE_URL}{thumb["href"]}')

                left_column_data["image_links"] = list(image_links)

            # =============================================
            # Extract from middle column
            # =============================================
            middle_column = product_info.find("div", class_="col-sm-4 col-xs-12")

            if middle_column:
                # Extract brand
                brand_span = middle_column.find(
                    "span",
                    string=lambda text: text and "–ë—Ä–µ–Ω–¥:" in text if text else False,
                )

                if brand_span:
                    # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫—É <a> –≤–Ω—É—Ç—Ä–∏ span
                    brand_link = brand_span.find("a")
                    if brand_link:
                        brand = brand_link.find("b")
                        if brand:
                            middle_column_data["brand"] = brand.text.strip()
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–≥ <b>
                        brand = brand_span.find("b")
                        if brand:
                            middle_column_data["brand"] = brand.text.strip()
                else:
                    # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—â–µ–º –ª—é–±–æ–π span —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –ø—Ä–æ –±—Ä–µ–Ω–¥
                    for span in middle_column.find_all("span"):
                        if span.text and "–ë—Ä–µ–Ω–¥:" in span.text:
                            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–≥ <b> –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ span
                            brand = span.find("b")
                            if brand:
                                middle_column_data["brand"] = brand.text.strip()
                                break

                name_product = middle_column.find(
                    "span", class_="product_cat_name prodttl"
                )
                if name_product:
                    middle_column_data["name_product"] = name_product.text.strip()

                # Extract characteristics
                chars_div = middle_column.find("div", class_="attrs table")
                if chars_div:
                    characteristics = {}
                    char_rows = chars_div.find_all("div", class_="detail-chars")

                    for row in char_rows:
                        title_div = row.find("div", class_="detail-chars-title")
                        field_div = row.find("div", class_="detail-chars-field")

                        if title_div and field_div:
                            title = title_div.find(
                                "span", class_="detail-chars-title-name"
                            )
                            if title:
                                char_name = title.text.strip()
                                char_value = field_div.text.strip()
                                characteristics[char_name] = char_value

                    middle_column_data["characteristics"] = characteristics

                # Extract "–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö" data
                aggregates_div = middle_column.find(
                    "div", class_="product-applicability"
                )
                if aggregates_div:
                    aggregates_parts = []

                    for div in aggregates_div.find_all("div", recursive=False):
                        brand_elem = div.find("span", class_="s-s")
                        if brand_elem and brand_elem.find("b"):
                            brand_name = brand_elem.find("b").text.strip().rstrip(":")

                            # Get all links (part numbers) for this brand
                            links = div.find("div", class_="more").find_all("a")
                            part_numbers = [link.text.strip() for link in links]

                            # Format as requested: Brand:part1!part2!part3
                            if part_numbers:
                                aggregates_parts.append(
                                    f"{brand_name}:{'!'.join(part_numbers)}"
                                )

                    # Join all brands with /
                    middle_column_data["aggregates"] = "/".join(aggregates_parts)

            # =============================================
            # Extract from right column
            # =============================================
            right_column = product_info.find("div", class_="col-sm-5 col-xs-12")

            if right_column:
                # Extract "–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"
                analogs_div = right_column.find("div", class_="analogs")
                if analogs_div:
                    analogs_parts = []

                    # Find all brand sections
                    for div in analogs_div.find_all("div", recursive=False):
                        if div.find("span") and div.find("span").find("b"):
                            brand_name = (
                                div.find("span").find("b").text.strip().rstrip(":")
                            )

                            # Extract part numbers from links
                            part_numbers = []
                            for a_tag in div.find_all("a"):
                                part_numbers.append(a_tag.text.strip())

                            # Format as requested: Brand:part1!part2!part3
                            if part_numbers:
                                analogs_parts.append(
                                    f"{brand_name}:{'!'.join(part_numbers)}"
                                )

                    # Join all brands with /
                    right_column_data["analogs"] = "/".join(analogs_parts)

                # Extract "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"
                applications_div = right_column.find("div", class_="by_car")
                if applications_div:
                    applications_span = applications_div.find("span")
                    if applications_span:
                        # Get raw text from the span
                        raw_text = applications_span.get_text()

                        # Split the raw text into lines for processing
                        lines = [
                            line.strip()
                            for line in raw_text.split("\n")
                            if line.strip()
                        ]

                        # Dictionary to store manufacturer -> models mapping
                        manufacturer_models = {}

                        # Process each line
                        for line in lines:
                            # Identify manufacturer - first all uppercase word in the line
                            match = re.match(r"^([A-Z]+)\s", line)
                            if match:
                                manufacturer = match.group(1)
                                model_info = line[len(manufacturer) :].strip()

                                if manufacturer not in manufacturer_models:
                                    manufacturer_models[manufacturer] = []

                                manufacturer_models[manufacturer].append(model_info)

                        # Format as requested
                        applications = []
                        for manufacturer, models in manufacturer_models.items():
                            models_text = "!".join(models)
                            applications.append(f"{manufacturer} {models_text}")

                        right_column_data["applications"] = applications

            # Combine all data
            product_data = {
                "left_column": left_column_data,
                "middle_column": middle_column_data,
                "right_column": right_column_data,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ –≤ —Å–ø–∏—Å–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            products_by_directory[directory_name].append(product_data)
            processed_files += 1

            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Å–±–æ–µ
            if processed_files % 100 == 0:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                for dir_name, products in products_by_directory.items():
                    # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    temp_output = (
                        output_directory / f"temp_{dir_name}_{processed_files}.json"
                    )
                    with open(temp_output, "w", encoding="utf-8") as f:
                        json.dump(products, f, ensure_ascii=False, indent=4)
                logger.info(
                    f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ {processed_files} —Ñ–∞–π–ª–æ–≤"
                )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {html_path}: {str(e)}")
            processed_files += 1
            continue

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for dir_name, products in products_by_directory.items():
        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        output_file = output_directory / f"{dir_name}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(products, f, ensure_ascii=False, indent=4)
        logger.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {dir_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}")

    logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}/{total_files}")
    logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º")

    return products_by_directory


def sanitize_filename(filename: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –Ω–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ."""
    # –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ Windows
    forbidden_chars = r'\/:*?"<>|'
    # –°–æ–∑–¥–∞—ë–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∑–∞–º–µ–Ω—ã –ª—é–±–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    # –ó–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª –Ω–∞ '_'
    sanitized = re.sub(f"[{re.escape(forbidden_chars)}]", "_", filename)
    return sanitized


def get_img(img_url, product_name):
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "cache-control": "no-cache",
        "dnt": "1",
        "pragma": "no-cache",
        "priority": "u=0, i",
        "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        # 'cookie': 'PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; language=uk-ua; currency=UAH',
    }
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Å–ø–∏—Å–∫—É URL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤.
    –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    """
    all_data = []

    for i, url in enumerate(img_url):
        product_result = sanitize_filename(product_name)
        output_img_file = img_directory / f"{product_result}_{i}.jpg"
        name_img = f"{product_result}_{i}.jpg"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
        if output_img_file.exists():
            logger.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {output_img_file}")
            all_data.append(name_img)
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É URL

        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–∫–∞—á–∏–≤–∞–µ–º –µ–≥–æ
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                with open(output_img_file, "wb") as file:
                    file.write(response.content)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–∏–ª: {output_img_file}")
                all_data.append(name_img)
            else:
                logger.error(
                    f"Failed to get image {i}. Status code: {response.status_code}"
                )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}: {str(e)}")

    return all_data


# def convert_json_to_csv():
#     """
#     Convert JSON product data to CSV format

#     Args:
#         json_file_path (str): Path to the JSON file
#         csv_file_path (str): Path to save the CSV file
#     """
#     # Load JSON data
#     with open(output_json_file, "r", encoding="utf-8") as f:
#         data = json.load(f)

#     # Extract data from each section
#     left_column = data.get("left_column", {})
#     middle_column = data.get("middle_column", {})
#     right_column = data.get("right_column", {})

#     # Create a dictionary for the CSV row
#     row_data = {}

#     # Add product name and price
#     product_name = left_column.get("product_name", "")
#     row_data["–ê—Ä—Ç–∏–∫—É–ª"] = product_name
#     img_url = left_column.get("image_links", "")
#     img_list = get_img(img_url, product_name)
#     # For analogs, join all entries
#     row_data["–ù–æ–º–µ—Ä–∞ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = right_column.get("–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤", "")

#     # Add aggregates, analogs, and applications
#     row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å"] = middle_column.get("–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö", "")

#     # For applications, join all entries with a separator
#     applications = right_column.get("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é", [])
#     if applications:
#         row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = "||".join(applications)
#     else:
#         row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = ""
#     name_product = middle_column.get("name_product", "")
#     row_data["–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É"] = name_product
#     brand = middle_column.get("brand", None)
#     row_data["–í–∏—Ä–æ–±–Ω–∏–∫"] = brand
#     row_data["–¶—ñ–Ω–∞"] = left_column.get("price", "")
#     row_data["–§–æ—Ç–æ"] = ",".join(img_list)
#     row_data["–ö—ñ–ª—å–∫—ñ—Å—Ç—å"] = "100"

#     # Add all characteristics as separate columns
#     characteristics = middle_column.get("characteristics", {})
#     for key, value in characteristics.items():
#         # Clean column name for CSV
#         clean_key = key.replace(",", "").strip()
#         row_data[clean_key] = value

#     # Create DataFrame with a single row
#     df = pd.DataFrame([row_data])
#     output_csv_file = data_directory / "output.csv"
#     # Save to CSV
#     df.to_csv(output_csv_file, index=False, encoding="windows-1251", sep=";")

#     logger.info(f"CSV file created: {output_csv_file}")


def convert_json_to_csv():
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ JSON —Ñ–∞–π–ª—ã –∏–∑ output_directory –≤ CSV —Ñ–æ—Ä–º–∞—Ç.
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ JSON —Ñ–∞–π–ª–∞ —Å–æ–∑–¥–∞–µ—Ç—Å—è CSV —Ñ–∞–π–ª —Å —Ç–µ–º –∂–µ –∏–º–µ–Ω–µ–º.
    –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç 4000 —Å—Ç—Ä–æ–∫, —Ñ–∞–π–ª —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏.
    """
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    json_files = list(output_directory.glob("*.json"))

    if not json_files:
        logger.warning(f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {output_directory} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤")
        return

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {len(json_files)}")

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è CSV —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    csv_output_directory = data_directory / "csv_output"
    csv_output_directory.mkdir(exist_ok=True)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π JSON —Ñ–∞–π–ª
    for json_file_path in json_files:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            file_name = json_file_path.stem

            logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–∞ {file_name}.json –≤ CSV")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫
            rows = []

            # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –≤ JSON –º–∞—Å—Å–∏–≤–µ
            for item in data:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                left_column = item.get("left_column", {})
                middle_column = item.get("middle_column", {})
                right_column = item.get("right_column", {})

                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å—Ç—Ä–æ–∫–∏ CSV
                row_data = {}

                # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è –ø—Ä–æ–¥—É–∫—Ç–∞ –∏ —Ü–µ–Ω—É
                product_name = left_column.get("product_name", "")
                row_data["–ê—Ä—Ç–∏–∫—É–ª"] = product_name

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img_url = left_column.get("image_links", "")
                img_list = get_img(img_url, product_name)

                # –î–ª—è –∞–Ω–∞–ª–æ–≥–æ–≤ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏
                analogs = right_column.get("analogs", "")
                row_data["–ù–æ–º–µ—Ä–∞ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = analogs

                # –î–æ–±–∞–≤–ª—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã, –∞–Ω–∞–ª–æ–≥–∏ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
                row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å"] = middle_column.get("aggregates", "")

                # –î–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
                applications = right_column.get("applications", [])
                if applications:
                    row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = "||".join(applications)
                else:
                    row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = ""

                name_product = middle_column.get("name_product", "")
                row_data["–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É"] = name_product
                brand = middle_column.get("brand", "")
                row_data["–í–∏—Ä–æ–±–Ω–∏–∫"] = brand
                row_data["–¶—ñ–Ω–∞"] = left_column.get("price", "")
                row_data["–§–æ—Ç–æ"] = ",".join(img_list)
                row_data["–ö—ñ–ª—å–∫—ñ—Å—Ç—å"] = "100"

                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
                characteristics = middle_column.get("characteristics", {})
                for key, value in characteristics.items():
                    # –û—á–∏—â–∞–µ–º –∏–º—è —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è CSV
                    clean_key = key.replace(",", "").strip()
                    row_data[clean_key] = value

                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫
                rows.append(row_data)

            # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫
            df = pd.DataFrame(rows)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏ DataFrame 4000 —Å—Ç—Ä–æ–∫
            rows_per_file = 4000
            if len(df) > rows_per_file:
                # –†–∞–∑–±–∏–≤–∞–µ–º DataFrame –Ω–∞ —á–∞—Å—Ç–∏
                total_files = (len(df) + rows_per_file - 1) // rows_per_file
                for i in range(total_files):
                    start_idx = i * rows_per_file
                    end_idx = min((i + 1) * rows_per_file, len(df))
                    df_chunk = df.iloc[start_idx:end_idx]

                    # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —á–∞—Å—Ç–∏
                    output_csv_file = (
                        csv_output_directory / f"{file_name}_part_{i+1}.csv"
                    )

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å –≤ CSV
                    df_chunk.to_csv(
                        output_csv_file, index=False, encoding="windows-1251", sep=";"
                    )
                    logger.info(f"CSV —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_csv_file}")
            else:
                # –û–¥–∏–Ω —Ñ–∞–π–ª, –µ—Å–ª–∏ –º–µ–Ω–µ–µ 4000 —Å—Ç—Ä–æ–∫
                output_csv_file = csv_output_directory / f"{file_name}.csv"
                df.to_csv(
                    output_csv_file, index=False, encoding="windows-1251", sep=";"
                )
                logger.info(f"CSV —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_csv_file}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {json_file_path}: {str(e)}")
            continue

    logger.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤ –≤ CSV –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    main_scraper()
    scrap_html()
    convert_json_to_csv()
