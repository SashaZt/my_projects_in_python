import json
import re
import sys
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger
from main_html import main_scraper

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_directory = current_directory / "html"
log_directory = current_directory / "log"
img_directory = current_directory / "img"

img_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)

html_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
output_json_file = data_directory / "output.json"
BASE_URL = "https://altstar.ua/"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def get_html():
    cookies = {
        "PHPSESSID": "4u34v9cjpr22r449jev8j5ho40",
        "language": "uk-ua",
        "currency": "UAH",
    }

    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "cache-control": "no-cache",
        "dnt": "1",
        "pragma": "no-cache",
        "priority": "u=0, i",
        "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        # 'cookie': 'PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; language=uk-ua; currency=UAH',
    }

    response = requests.get(
        "https://altstar.ua/1986se3754/bosch",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ü–µ–ª–∏–∫–æ–º
        with open(output_html_file, "w", encoding="utf-8") as file:
            file.write(response.text)
        logger.info(f"Successfully saved {output_html_file}")
    else:
        logger.error(f"Failed to get HTML. Status code: {response.status_code}")


# def scrap_html():
#     # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
#     # Initialize dictionaries for each section
#     left_column_data = {}
#     middle_column_data = {}
#     right_column_data = {}
#     for html_file in html_directory.glob("*.html"):
#         with html_file.open(encoding="utf-8") as file:
#             content = file.read()

#         # Create BeautifulSoup object
#         soup = BeautifulSoup(content, "lxml")

#         # Find the main product-info div
#         product_info = soup.find("div", class_="product-info")

#         if not product_info:
#             return {"error": "Product info div not found"}

#         # =============================================
#         # Extract from first column (left)
#         # =============================================
#         left_column = product_info.find("div", class_="col-sm-3 col-xs-12")

#         if left_column:
#             # Extract product name from ribbon
#             ribbon_name = left_column.find("h2", class_="ribbon_name single_product")
#             if ribbon_name:
#                 # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª—è–µ–º –∫—Ä–∞–π–Ω–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#                 product_name = ribbon_name.text.strip()

#                 # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ
#                 product_name = product_name.replace("\xa0", "_").replace(" ", "_")

#                 # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –∑–∞–º–µ–Ω—è—é—â–∏–π –≤—Å–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
#                 # product_name = ''.join(c if not c.isspace() else '_' for c in product_name)

#                 left_column_data["product_name"] = product_name

#             # Extract price
#             price_elem = left_column.find("span", id="price-old")
#             if price_elem:
#                 left_column_data["price"] = price_elem.text.strip()

#             # Extract all image links using a set to avoid duplicates
#             image_links = set()

#             # Main product image
#             main_image = left_column.find("a", class_="MagicZoom")
#             if main_image and "href" in main_image.attrs:
#                 image_links.add(main_image["href"])

#             # Thumbnail images in MagicScroll
#             magic_scroll = left_column.find("div", class_="MagicScroll")
#             if magic_scroll:
#                 thumbnails = magic_scroll.find_all("a")
#                 for thumb in thumbnails:
#                     if "href" in thumb.attrs:
#                         image_links.add(f'{BASE_URL}{thumb["href"]}')

#             # Other thumbnail images
#             other_thumbs = left_column.find_all("a", class_="mz-thumb")
#             for thumb in other_thumbs:
#                 if "href" in thumb.attrs:
#                     image_links.add('{BASE_URL}{thumb["href"]}')

#             left_column_data["image_links"] = list(image_links)

#         # =============================================
#         # Extract from middle column
#         # =============================================
#         middle_column = product_info.find("div", class_="col-sm-4 col-xs-12")

#         if middle_column:
#             # Extract brand
#             # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
#             # –ò—â–µ–º span, –∫–æ—Ç–æ—Ä—ã–π –°–û–î–ï–†–ñ–ò–¢ —Ç–µ–∫—Å—Ç "–ë—Ä–µ–Ω–¥", –∞ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –µ–º—É
#             brand_span = middle_column.find(
#                 "span", string=lambda text: text and "–ë—Ä–µ–Ω–¥:" in text if text else False
#             )

#             if brand_span:
#                 # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫—É <a> –≤–Ω—É—Ç—Ä–∏ span
#                 brand_link = brand_span.find("a")
#                 if brand_link:
#                     brand = brand_link.find("b")
#                     if brand:
#                         middle_column_data["brand"] = brand.text.strip()
#                         logger.info(f"Extracted brand from link: {brand.text.strip()}")
#                 else:
#                     # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–≥ <b>
#                     brand = brand_span.find("b")
#                     if brand:
#                         middle_column_data["brand"] = brand.text.strip()
#                         logger.info(f"Extracted brand from span: {brand.text.strip()}")
#             else:
#                 # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—â–µ–º –ª—é–±–æ–π span —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –ø—Ä–æ –±—Ä–µ–Ω–¥
#                 for span in middle_column.find_all("span"):
#                     if span.text and "–ë—Ä–µ–Ω–¥:" in span.text:
#                         # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–≥ <b> –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ span
#                         brand = span.find("b")
#                         if brand:
#                             middle_column_data["brand"] = brand.text.strip()
#                             break

#             name_product = middle_column.find("span", class_="product_cat_name prodttl")
#             if name_product:
#                 middle_column_data["name_product"] = name_product.text.strip()
#             # Extract characteristics
#             chars_div = middle_column.find("div", class_="attrs table")
#             if chars_div:
#                 characteristics = {}
#                 char_rows = chars_div.find_all("div", class_="detail-chars")

#                 for row in char_rows:
#                     title_div = row.find("div", class_="detail-chars-title")
#                     field_div = row.find("div", class_="detail-chars-field")

#                     if title_div and field_div:
#                         title = title_div.find("span", class_="detail-chars-title-name")
#                         if title:
#                             char_name = title.text.strip()
#                             char_value = field_div.text.strip()
#                             characteristics[char_name] = char_value

#                 middle_column_data["characteristics"] = characteristics

#             # Extract "–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö" data
#             aggregates_div = middle_column.find("div", class_="product-applicability")
#             if aggregates_div:
#                 aggregates_parts = []

#                 for div in aggregates_div.find_all("div", recursive=False):
#                     brand_elem = div.find("span", class_="s-s")
#                     if brand_elem and brand_elem.find("b"):
#                         brand_name = brand_elem.find("b").text.strip().rstrip(":")

#                         # Get all links (part numbers) for this brand
#                         links = div.find("div", class_="more").find_all("a")
#                         part_numbers = [link.text.strip() for link in links]

#                         # Format as requested: Brand:part1!part2!part3
#                         if part_numbers:
#                             aggregates_parts.append(
#                                 f"{brand_name}:{'!'.join(part_numbers)}"
#                             )

#                 # Join all brands with /
#                 middle_column_data["–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö"] = "/".join(
#                     aggregates_parts
#                 )

#         # =============================================
#         # Extract from right column
#         # =============================================
#         right_column = product_info.find("div", class_="col-sm-5 col-xs-12")

#         if right_column:
#             # Extract "–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"
#             analogs_div = right_column.find("div", class_="analogs")
#             if analogs_div:
#                 analogs_parts = []

#                 # Find all brand sections
#                 for div in analogs_div.find_all("div", recursive=False):
#                     if div.find("span") and div.find("span").find("b"):
#                         brand_name = div.find("span").find("b").text.strip().rstrip(":")

#                         # Extract part numbers from links
#                         part_numbers = []
#                         for a_tag in div.find_all("a"):
#                             part_numbers.append(a_tag.text.strip())

#                         # Format as requested: Brand:part1!part2!part3
#                         if part_numbers:
#                             analogs_parts.append(
#                                 f"{brand_name}:{'!'.join(part_numbers)}"
#                             )

#                 # Join all brands with /
#                 right_column_data["–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = "/".join(analogs_parts)

#             # Extract "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"
#             applications_div = right_column.find("div", class_="by_car")
#             if applications_div:
#                 applications_span = applications_div.find("span")
#                 if applications_span:
#                     applications_text = applications_span.text.strip()

#                     # Process the applications text to merge by manufacturer
#                     lines = [
#                         line.strip()
#                         for line in applications_text.split("\n")
#                         if line.strip()
#                     ]

#                     # Process the applications text to merge by manufacturer
#                     # Get raw text from the span
#                     raw_text = applications_span.get_text()

#                     # Process the raw text to identify manufacturer patterns
#                     import re

#                     # Split the raw text into lines for processing
#                     lines = [
#                         line.strip() for line in raw_text.split("\n") if line.strip()
#                     ]

#                     # Dictionary to store manufacturer -> models mapping
#                     manufacturer_models = {}

#                     # Process each line
#                     for line in lines:
#                         # Identify manufacturer - first all uppercase word in the line
#                         match = re.match(r"^([A-Z]+)\s", line)
#                         if match:
#                             manufacturer = match.group(1)
#                             model_info = line[len(manufacturer) :].strip()

#                             if manufacturer not in manufacturer_models:
#                                 manufacturer_models[manufacturer] = []

#                             manufacturer_models[manufacturer].append(model_info)

#                     # Format as requested
#                     applications = []
#                     for manufacturer, models in manufacturer_models.items():
#                         models_text = "!".join(models)
#                         applications.append(f"{manufacturer} {models_text}")

#                     right_column_data["–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"] = applications


#         # Combine all data
#         result = {
#             "left_column": left_column_data,
#             "middle_column": middle_column_data,
#             "right_column": right_column_data,
#         }
#     with open(output_json_file, "w", encoding="utf-8") as f:
#         json.dump(result, f, ensure_ascii=False, indent=4)
#     return result
# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –∏–ª–∏ —Ñ–∞–π–ª, –≤–æ—Ç –ø—Ä–∏–º–µ—Ä:
def scrap_html():
    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
    all_products = []

    # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    processed_files = 0
    total_files = 0

    # –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ HTML —Ñ–∞–π–ª–æ–≤ –≤–æ –≤—Å–µ—Ö –ø–∞–ø–∫–∞—Ö
    for html_path in html_directory.glob("**/*.html"):
        total_files += 1

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –≤—Å–µ–≥–æ HTML —Ñ–∞–π–ª–æ–≤: {total_files}")

    # –û–±—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞–ø–∫–∏ –≤–Ω—É—Ç—Ä–∏ html_directory –∏ –∏—â–µ–º HTML —Ñ–∞–π–ª—ã
    for html_path in html_directory.glob("**/*.html"):
        try:
            logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ ({processed_files + 1}/{total_files}): {html_path}"
            )

            with html_path.open(encoding="utf-8") as file:
                content = file.read()

            # Initialize dictionaries for each section
            left_column_data = {}
            middle_column_data = {}
            right_column_data = {}

            # Create BeautifulSoup object
            soup = BeautifulSoup(content, "lxml")

            # Find the main product-info div
            product_info = soup.find("div", class_="product-info")

            if not product_info:
                logger.warning(
                    f"–ù–µ –Ω–∞–π–¥–µ–Ω div —Å –∫–ª–∞—Å—Å–æ–º product-info –≤ —Ñ–∞–π–ª–µ {html_path}"
                )
                processed_files += 1
                continue

            # =============================================
            # Extract from first column (left)
            # =============================================
            left_column = product_info.find("div", class_="col-sm-3 col-xs-12")

            if left_column:
                # Extract product name from ribbon
                ribbon_name = left_column.find(
                    "h2", class_="ribbon_name single_product"
                )
                if ribbon_name:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —É–¥–∞–ª—è–µ–º –∫—Ä–∞–π–Ω–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                    product_name = ribbon_name.text.strip()

                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ
                    product_name = product_name.replace("\xa0", "_").replace(" ", "_")

                    left_column_data["product_name"] = product_name

                # Extract price
                price_elem = left_column.find("span", id="price-old")
                if price_elem:
                    left_column_data["price"] = price_elem.text.strip()

                # Extract all image links using a set to avoid duplicates
                image_links = set()

                # Main product image
                main_image = left_column.find("a", class_="MagicZoom")
                if main_image and "href" in main_image.attrs:
                    image_links.add(main_image["href"])

                # Thumbnail images in MagicScroll
                magic_scroll = left_column.find("div", class_="MagicScroll")
                if magic_scroll:
                    thumbnails = magic_scroll.find_all("a")
                    for thumb in thumbnails:
                        if "href" in thumb.attrs:
                            image_links.add(f'{BASE_URL}{thumb["href"]}')

                # Other thumbnail images
                other_thumbs = left_column.find_all("a", class_="mz-thumb")
                for thumb in other_thumbs:
                    if "href" in thumb.attrs:
                        image_links.add(f'{BASE_URL}{thumb["href"]}')

                left_column_data["image_links"] = list(image_links)

            # =============================================
            # Extract from middle column
            # =============================================
            middle_column = product_info.find("div", class_="col-sm-4 col-xs-12")

            if middle_column:
                # Extract brand
                brand_span = middle_column.find(
                    "span",
                    string=lambda text: text and "–ë—Ä–µ–Ω–¥:" in text if text else False,
                )

                if brand_span:
                    # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫—É <a> –≤–Ω—É—Ç—Ä–∏ span
                    brand_link = brand_span.find("a")
                    if brand_link:
                        brand = brand_link.find("b")
                        if brand:
                            middle_column_data["brand"] = brand.text.strip()
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–≥ <b>
                        brand = brand_span.find("b")
                        if brand:
                            middle_column_data["brand"] = brand.text.strip()
                else:
                    # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—â–µ–º –ª—é–±–æ–π span —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –ø—Ä–æ –±—Ä–µ–Ω–¥
                    for span in middle_column.find_all("span"):
                        if span.text and "–ë—Ä–µ–Ω–¥:" in span.text:
                            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–≥ <b> –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ span
                            brand = span.find("b")
                            if brand:
                                middle_column_data["brand"] = brand.text.strip()
                                break

                name_product = middle_column.find(
                    "span", class_="product_cat_name prodttl"
                )
                if name_product:
                    middle_column_data["name_product"] = name_product.text.strip()

                # Extract characteristics
                chars_div = middle_column.find("div", class_="attrs table")
                if chars_div:
                    characteristics = {}
                    char_rows = chars_div.find_all("div", class_="detail-chars")

                    for row in char_rows:
                        title_div = row.find("div", class_="detail-chars-title")
                        field_div = row.find("div", class_="detail-chars-field")

                        if title_div and field_div:
                            title = title_div.find(
                                "span", class_="detail-chars-title-name"
                            )
                            if title:
                                char_name = title.text.strip()
                                char_value = field_div.text.strip()
                                characteristics[char_name] = char_value

                    middle_column_data["characteristics"] = characteristics

                # Extract "–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö" data
                aggregates_div = middle_column.find(
                    "div", class_="product-applicability"
                )
                if aggregates_div:
                    aggregates_parts = []

                    for div in aggregates_div.find_all("div", recursive=False):
                        brand_elem = div.find("span", class_="s-s")
                        if brand_elem and brand_elem.find("b"):
                            brand_name = brand_elem.find("b").text.strip().rstrip(":")

                            # Get all links (part numbers) for this brand
                            links = div.find("div", class_="more").find_all("a")
                            part_numbers = [link.text.strip() for link in links]

                            # Format as requested: Brand:part1!part2!part3
                            if part_numbers:
                                aggregates_parts.append(
                                    f"{brand_name}:{'!'.join(part_numbers)}"
                                )

                    # Join all brands with /
                    middle_column_data["aggregates"] = "/".join(aggregates_parts)

            # =============================================
            # Extract from right column
            # =============================================
            right_column = product_info.find("div", class_="col-sm-5 col-xs-12")

            if right_column:
                # Extract "–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤"
                analogs_div = right_column.find("div", class_="analogs")
                if analogs_div:
                    analogs_parts = []

                    # Find all brand sections
                    for div in analogs_div.find_all("div", recursive=False):
                        if div.find("span") and div.find("span").find("b"):
                            brand_name = (
                                div.find("span").find("b").text.strip().rstrip(":")
                            )

                            # Extract part numbers from links
                            part_numbers = []
                            for a_tag in div.find_all("a"):
                                part_numbers.append(a_tag.text.strip())

                            # Format as requested: Brand:part1!part2!part3
                            if part_numbers:
                                analogs_parts.append(
                                    f"{brand_name}:{'!'.join(part_numbers)}"
                                )

                    # Join all brands with /
                    right_column_data["analogs"] = "/".join(analogs_parts)

                # Extract "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é"
                applications_div = right_column.find("div", class_="by_car")
                if applications_div:
                    applications_span = applications_div.find("span")
                    if applications_span:
                        # Get raw text from the span
                        raw_text = applications_span.get_text()

                        # Split the raw text into lines for processing
                        lines = [
                            line.strip()
                            for line in raw_text.split("\n")
                            if line.strip()
                        ]

                        # Dictionary to store manufacturer -> models mapping
                        manufacturer_models = {}

                        # Process each line
                        for line in lines:
                            # Identify manufacturer - first all uppercase word in the line
                            match = re.match(r"^([A-Z]+)\s", line)
                            if match:
                                manufacturer = match.group(1)
                                model_info = line[len(manufacturer) :].strip()

                                if manufacturer not in manufacturer_models:
                                    manufacturer_models[manufacturer] = []

                                manufacturer_models[manufacturer].append(model_info)

                        # Format as requested
                        applications = []
                        for manufacturer, models in manufacturer_models.items():
                            models_text = "!".join(models)
                            applications.append(f"{manufacturer} {models_text}")

                        right_column_data["applications"] = applications

            # Combine all data
            product_data = {
                "left_column": left_column_data,
                "middle_column": middle_column_data,
                "right_column": right_column_data,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            all_products.append(product_data)
            processed_files += 1

            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Å–±–æ–µ
            if processed_files % 100 == 0:
                temp_output = output_json_file.with_name(
                    f"temp_output_{processed_files}.json"
                )
                with open(temp_output, "w", encoding="utf-8") as f:
                    json.dump(all_products, f, ensure_ascii=False, indent=4)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {temp_output}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {html_path}: {str(e)}")
            processed_files += 1
            continue

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(output_json_file, "w", encoding="utf-8") as f:
        json.dump(all_products, f, ensure_ascii=False, indent=4)

    logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}/{total_files}")
    logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_json_file}")

    return all_products


def sanitize_filename(filename: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –Ω–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ."""
    # –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ Windows
    forbidden_chars = r'\/:*?"<>|'
    # –°–æ–∑–¥–∞—ë–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∑–∞–º–µ–Ω—ã –ª—é–±–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    # –ó–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª –Ω–∞ '_'
    sanitized = re.sub(f"[{re.escape(forbidden_chars)}]", "_", filename)
    return sanitized


def get_img(img_url, product_name):
    headers = {
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "accept-language": "ru,en;q=0.9,uk;q=0.8",
        "cache-control": "no-cache",
        "dnt": "1",
        "pragma": "no-cache",
        "priority": "u=0, i",
        "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "none",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        # 'cookie': 'PHPSESSID=4u34v9cjpr22r449jev8j5ho40; PHPSESSID=4u34v9cjpr22r449jev8j5ho40; language=uk-ua; currency=UAH',
    }
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Å–ø–∏—Å–∫—É URL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤.
    –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    """
    all_data = []

    for i, url in enumerate(img_url):
        product_result = sanitize_filename(product_name)
        output_img_file = img_directory / f"{product_result}_{i}.jpg"
        name_img = f"{product_result}_{i}.jpg"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
        if output_img_file.exists():
            logger.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {output_img_file}")
            all_data.append(name_img)
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É URL

        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–∫–∞—á–∏–≤–∞–µ–º –µ–≥–æ
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                with open(output_img_file, "wb") as file:
                    file.write(response.content)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–∏–ª: {output_img_file}")
                all_data.append(name_img)
            else:
                logger.error(
                    f"Failed to get image {i}. Status code: {response.status_code}"
                )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}: {str(e)}")

    return all_data


# def convert_json_to_csv():
#     """
#     Convert JSON product data to CSV format

#     Args:
#         json_file_path (str): Path to the JSON file
#         csv_file_path (str): Path to save the CSV file
#     """
#     # Load JSON data
#     with open(output_json_file, "r", encoding="utf-8") as f:
#         data = json.load(f)

#     # Extract data from each section
#     left_column = data.get("left_column", {})
#     middle_column = data.get("middle_column", {})
#     right_column = data.get("right_column", {})

#     # Create a dictionary for the CSV row
#     row_data = {}

#     # Add product name and price
#     product_name = left_column.get("product_name", "")
#     row_data["–ê—Ä—Ç–∏–∫—É–ª"] = product_name
#     img_url = left_column.get("image_links", "")
#     img_list = get_img(img_url, product_name)
#     # For analogs, join all entries
#     row_data["–ù–æ–º–µ—Ä–∞ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = right_column.get("–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤", "")

#     # Add aggregates, analogs, and applications
#     row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å"] = middle_column.get("–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö", "")

#     # For applications, join all entries with a separator
#     applications = right_column.get("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é", [])
#     if applications:
#         row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = "||".join(applications)
#     else:
#         row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = ""
#     name_product = middle_column.get("name_product", "")
#     row_data["–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É"] = name_product
#     brand = middle_column.get("brand", None)
#     row_data["–í–∏—Ä–æ–±–Ω–∏–∫"] = brand
#     row_data["–¶—ñ–Ω–∞"] = left_column.get("price", "")
#     row_data["–§–æ—Ç–æ"] = ",".join(img_list)
#     row_data["–ö—ñ–ª—å–∫—ñ—Å—Ç—å"] = "100"

#     # Add all characteristics as separate columns
#     characteristics = middle_column.get("characteristics", {})
#     for key, value in characteristics.items():
#         # Clean column name for CSV
#         clean_key = key.replace(",", "").strip()
#         row_data[clean_key] = value

#     # Create DataFrame with a single row
#     df = pd.DataFrame([row_data])
#     output_csv_file = data_directory / "output.csv"
#     # Save to CSV
#     df.to_csv(output_csv_file, index=False, encoding="windows-1251", sep=";")

#     logger.info(f"CSV file created: {output_csv_file}")


def convert_json_to_csv():
    """
    Convert JSON product data (array of dictionaries) to CSV format and split into multiple files if exceeds 4000 rows

    Args:
        json_file_path (str): Path to the JSON file
        csv_file_path (str): Path to save the CSV file
    """
    # Load JSON data
    with open(output_json_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # List to store all rows
    rows = []

    # Iterate over each item in the JSON array
    for item in data:
        # Extract data from each section of the current item
        left_column = item.get("left_column", {})
        middle_column = item.get("middle_column", {})
        right_column = item.get("right_column", {})

        # Create a dictionary for the CSV row
        row_data = {}

        # Add product name and price
        product_name = left_column.get("product_name", "")
        row_data["–ê—Ä—Ç–∏–∫—É–ª"] = product_name
        img_url = left_column.get("image_links", "")
        img_list = get_img(img_url, product_name)
        # For analogs, join all entries
        row_data["–ù–æ–º–µ—Ä–∞ –∞–Ω–∞–ª–æ–≥—ñ–≤"] = right_column.get("–ù–æ–º–µ—Ä–∏ –∞–Ω–∞–ª–æ–≥—ñ–≤", "")

        # Add aggregates, analogs, and applications
        row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å"] = middle_column.get("–ó–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –∞–≥—Ä–µ–≥–∞—Ç–∞—Ö", "")

        # For applications, join all entries with a separator
        applications = right_column.get("–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ –∞–≤—Ç–æ–º–æ–±—ñ–ª—é", [])
        if applications:
            row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = "||".join(applications)
        else:
            row_data["–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ"] = ""
        name_product = middle_column.get("name_product", "")
        row_data["–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É"] = name_product
        brand = middle_column.get("brand", None)
        row_data["–í–∏—Ä–æ–±–Ω–∏–∫"] = brand
        row_data["–¶—ñ–Ω–∞"] = left_column.get("price", "")
        row_data["–§–æ—Ç–æ"] = ",".join(img_list)
        row_data["–ö—ñ–ª—å–∫—ñ—Å—Ç—å"] = "100"

        # Add all characteristics as separate columns
        characteristics = middle_column.get("characteristics", {})
        for key, value in characteristics.items():
            # Clean column name for CSV
            clean_key = key.replace(",", "").strip()
            row_data[clean_key] = value

        # Append the row to the list
        rows.append(row_data)

    # Create DataFrame from all rows
    df = pd.DataFrame(rows)

    # Define base output path
    base_output_path = data_directory / "output"

    # Check if DataFrame exceeds 4000 rows
    rows_per_file = 4000
    if len(df) > rows_per_file:
        # Split DataFrame into chunks
        total_files = (len(df) + rows_per_file - 1) // rows_per_file
        for i in range(total_files):
            start_idx = i * rows_per_file
            end_idx = min((i + 1) * rows_per_file, len(df))
            df_chunk = df.iloc[start_idx:end_idx]
            output_csv_file = base_output_path.with_name(f"output_part_{i+1}.csv")
            df_chunk.to_csv(
                output_csv_file, index=False, encoding="windows-1251", sep=";"
            )
            logger.info(f"CSV file created: {output_csv_file}")
    else:
        # Single file if less than 4000 rows
        output_csv_file = base_output_path.with_suffix(".csv")
        df.to_csv(output_csv_file, index=False, encoding="windows-1251", sep=";")
        logger.info(f"CSV file created: {output_csv_file}")


if __name__ == "__main__":
    main_scraper()
    scrap_html()
    convert_json_to_csv()
