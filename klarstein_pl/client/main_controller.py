import asyncio
import shutil
import signal
import time
import pandas as pd
from create_xml import export_products_to_xml
from downloader import downloader
from get_xml import parse_start_xml
from scrap import scrap_html_file, scrap_html_file_multithreaded

from config import Config, logger, paths

def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    sys.exit(0)


async def main():
    output_csv_file = paths.data / "output.csv"
    df = pd.read_csv(output_csv_file, encoding="utf-8")
    urls = df["url"].tolist()
    # –°–∫–∞—á–∏–≤–∞–µ–º
    results = await downloader.download_urls(urls)
    logger.info("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for url, success in results.items():
        status = "‚úÖ –£—Å–ø–µ—à–Ω–æ" if success else "‚ùå –û—à–∏–±–∫–∞"
        logger.info(f"{status}: {url}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {downloader.get_stats()}")


if __name__ == "__main__":
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
    # signal.signal(signal.SIGTERM, signal_handler)
    # signal.signal(signal.SIGINT, signal_handler)
    #  # –Ø–≤–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    if paths.temp.exists():
        shutil.rmtree(paths.temp)
        paths.create_directories()
    parse_start_xml()
    asyncio.run(main())
    
    scrap_html_file_multithreaded()
    
    export_products_to_xml()

