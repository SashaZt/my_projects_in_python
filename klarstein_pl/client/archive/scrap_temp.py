import asyncio
import aiohttp
import ssl
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

# –í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ ScraperAPI
SCRAPERAPI_PROXY = "http://scraperapi:b7141d2b54426945a9f0bf6ab4c7bc54@proxy-server.scraperapi.com:8001"
TEST_URL = "https://httpbin.org/ip"  # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ IP
TARGET_URL = "https://www.klarstein.pl/Male-AGD/Raclette/Szechuan-hot-pot-patelnia-z-plyta-grillowa-2-w-1-5-l-1350-600-W-Czerwony.html"

async def test_aiohttp_with_proxy():
    """–¢–µ—Å—Ç 1: –û–±—ã—á–Ω—ã–π aiohttp —Å –ø—Ä–æ–∫—Å–∏"""
    logger.info("üß™ –¢–µ—Å—Ç 1: aiohttp —Å ScraperAPI –ø—Ä–æ–∫—Å–∏")
    
    # –û—Ç–∫–ª—é—á–∞–µ–º SSL –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    connector = aiohttp.TCPConnector(ssl=ssl_context)
    
    try:
        async with aiohttp.ClientSession(connector=connector) as session:
            proxy_url = SCRAPERAPI_PROXY
            
            # –¢–µ—Å—Ç 1.1: –ü—Ä–æ–≤–µ—Ä–∫–∞ IP
            logger.info(f"üì° –¢–µ—Å—Ç–∏—Ä—É–µ–º IP —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏: {proxy_url}")
            async with session.get(TEST_URL, proxy=proxy_url, timeout=30) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"‚úÖ IP —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏: {data.get('origin', 'Unknown')}")
                else:
                    logger.error(f"‚ùå HTTP {response.status}")
            
            # –¢–µ—Å—Ç 1.2: –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç
            logger.info(f"üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç")
            async with session.get(TARGET_URL, proxy=proxy_url, timeout=30) as response:
                if response.status == 200:
                    content = await response.text()
                    logger.info(f"‚úÖ –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç: –ø–æ–ª—É—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return True
                else:
                    logger.error(f"‚ùå –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç: HTTP {response.status}")
                    return False
                    
    except Exception as e:
        logger.error(f"‚ùå aiohttp –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_rnet_with_proxy():
    """–¢–µ—Å—Ç 2: rnet —Å –ø—Ä–æ–∫—Å–∏"""
    logger.info("üß™ –¢–µ—Å—Ç 2: rnet —Å ScraperAPI –ø—Ä–æ–∫—Å–∏")
    
    try:
        from rnet import Impersonate, Client, Proxy
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç —Å –ø—Ä–æ–∫—Å–∏
        proxies = [Proxy.all(SCRAPERAPI_PROXY)]
        
        client = Client(
            impersonate=Impersonate.Chrome134,
            proxies=proxies,
            timeout=30
        )
        
        try:
            # –¢–µ—Å—Ç IP
            logger.info(f"üì° rnet: –¢–µ—Å—Ç–∏—Ä—É–µ–º IP")
            response = await client.get(TEST_URL)
            if response.status_code == 200:
                data = await response.json()
                logger.info(f"‚úÖ rnet IP: {data.get('origin', 'Unknown')}")
            
            # –¢–µ—Å—Ç —Ü–µ–ª–µ–≤–æ–≥–æ —Å–∞–π—Ç–∞
            logger.info(f"üéØ rnet: –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç")
            response = await client.get(TARGET_URL)
            if response.status_code == 200:
                content = await response.text()
                logger.info(f"‚úÖ rnet —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç: –ø–æ–ª—É—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return True
            else:
                logger.error(f"‚ùå rnet HTTP {response.status_code}")
                return False
                
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç
            if hasattr(client, 'close'):
                await client.close()
                
    except ImportError:
        logger.warning("‚ö†Ô∏è rnet –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç")
        return False
    except Exception as e:
        logger.error(f"‚ùå rnet –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_curl_cffi_with_proxy():
    """–¢–µ—Å—Ç 3: curl_cffi —Å –ø—Ä–æ–∫—Å–∏"""
    logger.info("üß™ –¢–µ—Å—Ç 3: curl_cffi —Å ScraperAPI –ø—Ä–æ–∫—Å–∏")
    
    try:
        from curl_cffi.requests import AsyncSession
        
        async with AsyncSession() as session:
            proxy_config = {
                "http": SCRAPERAPI_PROXY,
                "https": SCRAPERAPI_PROXY
            }
            
            # –¢–µ—Å—Ç IP
            logger.info(f"üì° curl_cffi: –¢–µ—Å—Ç–∏—Ä—É–µ–º IP")
            response = await session.get(
                TEST_URL,
                proxies=proxy_config,
                impersonate="chrome120",
                verify=False,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"‚úÖ curl_cffi IP: {data.get('origin', 'Unknown')}")
            
            # –¢–µ—Å—Ç —Ü–µ–ª–µ–≤–æ–≥–æ —Å–∞–π—Ç–∞
            logger.info(f"üéØ curl_cffi: –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç")
            response = await session.get(
                TARGET_URL,
                proxies=proxy_config,
                impersonate="chrome120",
                verify=False,
                timeout=30
            )
            
            if response.status_code == 200:
                content = response.text
                logger.info(f"‚úÖ curl_cffi —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç: –ø–æ–ª—É—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return True
            else:
                logger.error(f"‚ùå curl_cffi HTTP {response.status_code}")
                return False
                
    except ImportError:
        logger.warning("‚ö†Ô∏è curl_cffi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç")
        return False
    except Exception as e:
        logger.error(f"‚ùå curl_cffi –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_without_proxy():
    """–¢–µ—Å—Ç 4: –ó–∞–ø—Ä–æ—Å –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    logger.info("üß™ –¢–µ—Å—Ç 4: –ó–∞–ø—Ä–æ—Å –±–µ–∑ –ø—Ä–æ–∫—Å–∏ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)")
    
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    connector = aiohttp.TCPConnector(ssl=ssl_context)
    
    try:
        async with aiohttp.ClientSession(connector=connector) as session:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ IP –±–µ–∑ –ø—Ä–æ–∫—Å–∏
            async with session.get(TEST_URL, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"‚úÖ IP –±–µ–∑ –ø—Ä–æ–∫—Å–∏: {data.get('origin', 'Unknown')}")
                    return True
                    
    except Exception as e:
        logger.error(f"‚ùå –ó–∞–ø—Ä–æ—Å –±–µ–∑ –ø—Ä–æ–∫—Å–∏: {e}")
        return False


async def test_scraperapi_direct():
    """–¢–µ—Å—Ç 5: –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ ScraperAPI API"""
    logger.info("üß™ –¢–µ—Å—Ç 5: –ü—Ä—è–º–æ–π ScraperAPI API")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ScraperAPI –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ –∏—Ö API
        api_key = "b7141d2b54426945a9f0bf6ab4c7bc54"
        api_url = f"http://api.scraperapi.com/?api_key={api_key}&url={TARGET_URL}"
        
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            async with session.get(api_url, timeout=30) as response:
                if response.status == 200:
                    content = await response.text()
                    logger.info(f"‚úÖ ScraperAPI –ø—Ä—è–º–æ–π API: –ø–æ–ª—É—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return True
                else:
                    logger.error(f"‚ùå ScraperAPI API: HTTP {response.status}")
                    return False
                    
    except Exception as e:
        logger.error(f"‚ùå ScraperAPI API –æ—à–∏–±–∫–∞: {e}")
        return False


async def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ScraperAPI")
    logger.info("=" * 60)
    
    results = {}
    
    # –¢–µ—Å—Ç 1: aiohttp
    results['aiohttp'] = await test_aiohttp_with_proxy()
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç 2: rnet
    results['rnet'] = await test_rnet_with_proxy()
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç 3: curl_cffi
    results['curl_cffi'] = await test_curl_cffi_with_proxy()
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç 4: –±–µ–∑ –ø—Ä–æ–∫—Å–∏
    results['no_proxy'] = await test_without_proxy()
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç 5: ScraperAPI –ø—Ä—è–º–æ–π API
    results['scraperapi_direct'] = await test_scraperapi_direct()
    logger.info("=" * 60)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    for method, success in results.items():
        status = "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç" if success else "‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        logger.info(f"  {method:15} : {status}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    logger.info("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    working_methods = [method for method, success in results.items() if success]
    
    if working_methods:
        logger.info(f"‚úÖ –†–∞–±–æ—Ç–∞—é—â–∏–µ –º–µ—Ç–æ–¥—ã: {', '.join(working_methods)}")
        if 'curl_cffi' in working_methods:
            logger.info("üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å curl_cffi")
        elif 'aiohttp' in working_methods:
            logger.info("üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å aiohttp")
        elif 'scraperapi_direct' in working_methods:
            logger.info("üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä—è–º–æ–π API ScraperAPI")
    else:
        logger.info("‚ùå –ù–∏ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ ScraperAPI –∫–ª—é—á")


if __name__ == "__main__":
    asyncio.run(main())