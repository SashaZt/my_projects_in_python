# ===========================
# main.py - –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –ø–∞—Ä—Å–µ—Ä–∞
# ===========================

import asyncio
import pandas as pd
from pathlib import Path
from config import Config, logger, paths
from downloader import Downloader, download_urls_simple, download_from_csv_simple

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    
    # –ß–∏—Ç–∞–µ–º URLs –∏–∑ CSV
    output_csv_file = paths.data / "output.csv"
    df = pd.read_csv(output_csv_file, encoding="utf-8")
    urls = df["url"].tolist()
    
    logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL'–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è")
    
    # –°–ø–æ—Å–æ–± 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
    config = Config.load()
    downloader = Downloader(config)
    results = await downloader.download_urls(urls)
    
    # –°–ø–æ—Å–æ–± 2: –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    # results = await download_urls_simple(urls, max_workers=10)
    
    # –°–ø–æ—Å–æ–± 3: –ü—Ä—è–º–æ –∏–∑ CSV
    # results = await download_from_csv_simple(str(output_csv_file))
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    successful = sum(1 for success in results.values() if success)
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {successful}/{len(urls)}")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())


# ===========================
# single_url_downloader.py - –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ URL
# ===========================

import asyncio
from downloader import Downloader
from config import Config, logger

async def download_single_url(url: str, filename: str = None) -> bool:
    """
    –°–∫–∞—á–∞—Ç—å –æ–¥–∏–Ω URL
    
    Args:
        url: URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        filename: –ò–º—è —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    config = Config.load()
    downloader = Downloader(config)
    
    result = await downloader.download_url(url, filename)
    
    if result:
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω: {url}")
    else:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {url}")
    
    return result

async def example_single_download():
    """–ü—Ä–∏–º–µ—Ä —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ URL"""
    url = "https://www.klarstein.pl/some-product.html"
    success = await download_single_url(url, "custom_filename.html")
    return success

if __name__ == "__main__":
    asyncio.run(example_single_download())


# ===========================
# batch_downloader.py - –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
# ===========================

import asyncio
from typing import List
from downloader import download_urls_simple
from config import logger

async def download_batch(urls: List[str], batch_size: int = 50) -> dict:
    """
    –°–∫–∞—á–∞—Ç—å URLs –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤
        batch_size: –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    all_results = {}
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞–∫–µ—Ç—ã
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(urls) + batch_size - 1) // batch_size
        
        logger.info(f"üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç {batch_num}/{total_batches} ({len(batch)} URL'–æ–≤)")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç
        batch_results = await download_urls_simple(batch, max_workers=10)
        all_results.update(batch_results)
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
        if i + batch_size < len(urls):
            logger.info("‚è∏Ô∏è –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏...")
            await asyncio.sleep(2)
    
    return all_results

async def example_batch_download():
    """–ü—Ä–∏–º–µ—Ä –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
    # –ë–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫ URL'–æ–≤
    urls = [
        "https://www.klarstein.pl/product1.html",
        "https://www.klarstein.pl/product2.html",
        # ... –º–Ω–æ–≥–æ URL'–æ–≤
    ]
    
    results = await download_batch(urls, batch_size=20)
    successful = sum(1 for success in results.values() if success)
    logger.info(f"‚úÖ –ò—Ç–æ–≥–æ —Å–∫–∞—á–∞–Ω–æ: {successful}/{len(urls)}")

if __name__ == "__main__":
    asyncio.run(example_batch_download())


# ===========================
# csv_processor.py - –†–∞–±–æ—Ç–∞ —Å CSV —Ñ–∞–π–ª–∞–º–∏
# ===========================

import asyncio
import pandas as pd
from pathlib import Path
from downloader import download_from_csv_simple
from config import logger, paths

async def process_csv_file(csv_file: str, url_column: str = "url") -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å CSV —Ñ–∞–π–ª —Å URL'–∞–º–∏
    
    Args:
        csv_file: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        url_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å URL'–∞–º–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    logger.info(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º CSV —Ñ–∞–π–ª: {csv_file}")
    
    # –ß–∏—Ç–∞–µ–º CSV
    df = pd.read_csv(csv_file, encoding="utf-8")
    urls = df[url_column].tolist()
    
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL'–æ–≤")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º
    results = await download_from_csv_simple(csv_file, max_workers=15)
    
    return results

async def process_multiple_csv_files(csv_files: List[str]) -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ CSV —Ñ–∞–π–ª–æ–≤
    
    Args:
        csv_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ CSV —Ñ–∞–π–ª–∞–º
        
    Returns:
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    all_results = {}
    
    for csv_file in csv_files:
        logger.info(f"üóÇÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {csv_file}")
        results = await process_csv_file(csv_file)
        all_results.update(results)
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
        await asyncio.sleep(1)
    
    return all_results

async def example_csv_processing():
    """–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV —Ñ–∞–π–ª–æ–≤"""
    # –û–¥–∏–Ω —Ñ–∞–π–ª
    output_csv = paths.data / "output.csv"
    results = await process_csv_file(str(output_csv))
    
    # –ù–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
    csv_files = [
        str(paths.data / "products_1.csv"),
        str(paths.data / "products_2.csv"),
    ]
    # all_results = await process_multiple_csv_files(csv_files)

if __name__ == "__main__":
    asyncio.run(example_csv_processing())


# ===========================
# custom_downloader.py - –ö–∞—Å—Ç–æ–º–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# ===========================

import asyncio
from typing import Optional, List
from downloader import Downloader
from config import Config, logger

class CustomDownloader:
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    
    def __init__(self, max_workers: int = 10, proxy: Optional[str] = None):
        self.config = Config.load()
        self.config.client.max_workers = max_workers
        self.downloader = Downloader(self.config, proxy)
    
    async def download_with_retries(self, urls: List[str], max_retries: int = 3) -> dict:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏"""
        results = {}
        failed_urls = urls.copy()
        
        for retry in range(max_retries):
            if not failed_urls:
                break
                
            logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {retry + 1}/{max_retries}, URL'–æ–≤: {len(failed_urls)}")
            
            batch_results = await self.downloader.download_urls(failed_urls)
            results.update(batch_results)
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—É–¥–∞—á–Ω—ã–µ URL'—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞
            failed_urls = [url for url, success in batch_results.items() if not success]
            
            if failed_urls and retry < max_retries - 1:
                logger.info(f"‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º... –û—Å—Ç–∞–ª–æ—Å—å: {len(failed_urls)}")
                await asyncio.sleep(5)
        
        return results
    
    async def download_high_priority(self, urls: List[str]) -> dict:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö URL'–æ–≤"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –∑–∞–¥–∞—á
        old_workers = self.config.client.max_workers
        self.config.client.max_workers = min(20, len(urls))
        
        try:
            results = await self.downloader.download_urls(urls)
            return results
        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.config.client.max_workers = old_workers

async def example_custom_download():
    """–ü—Ä–∏–º–µ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
    custom = CustomDownloader(max_workers=15)
    
    urls = [
        "https://www.klarstein.pl/product1.html",
        "https://www.klarstein.pl/product2.html",
    ]
    
    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏
    results = await custom.download_with_retries(urls, max_retries=5)
    
    # –í—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    # priority_results = await custom.download_high_priority(priority_urls)

if __name__ == "__main__":
    asyncio.run(example_custom_download())


# ===========================
# utils.py - –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ===========================

import asyncio
from typing import List, Dict
from pathlib import Path
import pandas as pd
from downloader import Downloader
from config import Config, logger

async def quick_download(urls: List[str], max_workers: int = 10) -> Dict[str, bool]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL'–æ–≤
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤
        max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    """
    config = Config.load()
    config.client.max_workers = max_workers
    downloader = Downloader(config)
    
    return await downloader.download_urls(urls)

async def download_and_save_stats(urls: List[str], stats_file: str = "download_stats.csv"):
    """
    –°–∫–∞—á–∞—Ç—å URL'—ã –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤
        stats_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    config = Config.load()
    downloader = Downloader(config)
    
    results = await downloader.download_urls(urls)
    stats = downloader.get_stats()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats_df = pd.DataFrame([stats])
    stats_df.to_csv(stats_file, index=False)
    
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {stats_file}")
    
    return results

def create_url_batches(urls: List[str], batch_size: int = 100) -> List[List[str]]:
    """
    –†–∞–∑–±–∏—Ç—å —Å–ø–∏—Å–æ–∫ URL'–æ–≤ –Ω–∞ –ø–∞–∫–µ—Ç—ã
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤
        batch_size: –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø–∞–∫–µ—Ç–æ–≤
    """
    batches = []
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i + batch_size]
        batches.append(batch)
    
    return batches

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è utils
async def example_utils():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    urls = ["https://example1.com", "https://example2.com"]
    
    # –ë—ã—Å—Ç—Ä–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    results = await quick_download(urls, max_workers=5)
    
    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    # results = await download_and_save_stats(urls, "my_stats.csv")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤
    all_urls = ["url1", "url2", "url3"] * 50  # 150 URL'–æ–≤
    batches = create_url_batches(all_urls, batch_size=25)
    logger.info(f"üì¶ –°–æ–∑–¥–∞–Ω–æ {len(batches)} –ø–∞–∫–µ—Ç–æ–≤")

if __name__ == "__main__":
    asyncio.run(example_utils())