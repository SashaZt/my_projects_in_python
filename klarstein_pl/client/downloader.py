import asyncio
import aiofiles
import hashlib
import random
from typing import List, Optional, Dict, Any
from curl_cffi.requests import AsyncSession
import pandas as pd
from config import Config, logger, paths


class Downloader:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π HTTP –∫–ª–∏–µ–Ω—Ç —Å TLS fingerprinting –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –≠–º—É–ª—è—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±—Ä–∞—É–∑–µ—Ä–æ–≤ (Chrome, Firefox, Safari, Edge)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–∫—Å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä—ã –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    - –°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    - –†–æ—Ç–∞—Ü–∏—è User-Agent'–æ–≤
    """
    
    def __init__(self, config: Config, proxy: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
        
        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Config
            proxy: –ü—Ä–æ–∫—Å–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "http://user:pass@host:port" –∏–ª–∏ None (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä–µ—Ç—Å—è –∏–∑ config)
        """
        self.config = config.client
        
        # –ü—Ä–æ–∫—Å–∏: –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –∏–Ω–∞—á–µ None
        if proxy is not None:
            self.proxy = proxy
        elif self.config.proxy:
            self.proxy = self.config.proxy
        else:
            self.proxy = None
            
        self.semaphore = asyncio.Semaphore(self.config.max_workers)
        self.session_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'retry_attempts': 0
        }
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        self.output_path = paths.html
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
        self.logger = logger
        
    
    def _get_random_user_agent(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        return random.choice(self.config.user_agents)
    
    def _get_filename_from_url(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return f"{url_hash}.html"
    
    async def _make_request(self, url: str) -> Optional[str]:
        for attempt in range(self.config.retry_attempts):
            try:
                # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                if attempt > 0:
                    await asyncio.sleep(self.config.retry_delay * attempt)
                else:
                    await asyncio.sleep(random.uniform(self.config.delay_min, self.config.delay_max))

                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è curl_cffi
                proxy_config = None
                if self.proxy:
                    proxy_config = {
                        "http": self.proxy,
                        "https": self.proxy
                    }
                async with AsyncSession() as session:
                    headers = {
                        'User-Agent': self._get_random_user_agent(),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Sec-Fetch-User': '?1',
                        'Cache-Control': 'max-age=0'
                    }

                    response = await session.get(
                        url,
                        headers=headers,
                        proxies=proxy_config,
                        verify=False,
                        timeout=self.config.timeout
                    )

                    self.session_stats['total_requests'] += 1

                    if response.status_code == 200:
                        content = response.text
                        self.session_stats['successful_requests'] += 1
                        self.logger.debug(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {url}")
                        return content
                    else:
                        self.logger.warning(f"‚ùå HTTP {response.status_code} –¥–ª—è {url}")
                        if response.status_code in [403, 429]:
                            await asyncio.sleep(random.uniform(5, 10))
                        raise Exception(f"HTTP {response.status_code}")

            except Exception as e:
                self.session_stats['retry_attempts'] += 1
                self.logger.error(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.config.retry_attempts} –¥–ª—è {url}: {e}")

                if attempt == self.config.retry_attempts - 1:
                    self.session_stats['failed_requests'] += 1
                    self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} –ø–æ—Å–ª–µ {self.config.retry_attempts} –ø–æ–ø—ã—Ç–æ–∫")

        return None
    
    
    async def download_url(self, url: str, filename: Optional[str] = None) -> bool:
        """
        –°–∫–∞—á–∞—Ç—å –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        Args:
            url: URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        async with self.semaphore:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                if filename is None:
                    filename = self._get_filename_from_url(url)
                
                file_path = self.output_path / filename
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
                if file_path.exists():
                    self.logger.info(f"‚è≠Ô∏è –§–∞–π–ª {file_path} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return True
                
                # –°–∫–∞—á–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = await self._make_request(url)
                
                if content:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
                    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                        await f.write(content)
                    
                    self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
                    return True
                else:
                    return False
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}")
                return False
    
    async def download_urls(self, urls: List[str], custom_filenames: Optional[Dict[str, str]] = None) -> Dict[str, bool]:
        """
        –°–∫–∞—á–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ URL'–æ–≤
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            custom_filenames: –°–ª–æ–≤–∞—Ä—å {url: filename} –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {url: success_status}
        """
        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(urls)} URL'–æ–≤")
        self.logger.info(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏: {self.config.max_workers} –ø–æ—Ç–æ–∫–æ–≤, –ø—Ä–æ–∫—Å–∏: {'–î–∞' if self.proxy else '–ù–µ—Ç'}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = []
        for url in urls:
            filename = custom_filenames.get(url) if custom_filenames else None
            task = self.download_url(url, filename)
            tasks.append((url, task))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
        results = {}
        completed_tasks = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
        
        for (url, _), result in zip(tasks, completed_tasks):
            if isinstance(result, Exception):
                self.logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è {url}: {result}")
                results[url] = False
            else:
                results[url] = result
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        successful = sum(1 for success in results.values() if success)
        self.logger.info(f"üìä –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {successful}/{len(urls)} —É—Å–ø–µ—à–Ω–æ")
        self.logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏: {self.session_stats}")
        
        return results
    
    async def download_from_csv(self, csv_file: str, url_column: str = 'url') -> Dict[str, bool]:
        """
        –°–∫–∞—á–∞—Ç—å URL'—ã –∏–∑ CSV —Ñ–∞–π–ª–∞
        
        Args:
            csv_file: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            url_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å URL'–∞–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {url: success_status}
        """
        try:
            df = pd.read_csv(csv_file)
            urls = df[url_column].tolist()
            
            self.logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls)} URL'–æ–≤ –∏–∑ {csv_file}")
            return await self.download_urls(urls)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV —Ñ–∞–π–ª–∞: {e}")
            return {}
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏"""
        return self.session_stats.copy()
    
    def reset_stats(self):
        """–°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏"""
        self.session_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'retry_attempts': 0
        }

    @classmethod
    def create_from_config(cls, config_path: Optional[str] = None, proxy_override: Optional[str] = None):
        """
        –£–¥–æ–±–Ω—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)
            proxy_override: –ü—Ä–æ–∫—Å–∏ –¥–ª—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä Downloader
        """
        config = Config.load()
        return cls(config, proxy_override)

config = Config.load()
downloader = Downloader(config)


# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def download_urls_simple(urls: List[str], max_workers: int = 10, proxy: Optional[str] = None) -> Dict[str, bool]:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è URL'–æ–≤ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL'–æ–≤
        max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        proxy: –ü—Ä–æ–∫—Å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    """
    config = Config.load()
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º max_workers –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
    config.client.max_workers = max_workers
    
    downloader = Downloader(config, proxy)
    return await downloader.download_urls(urls)


async def download_from_csv_simple(csv_file: str, max_workers: int = 10, proxy: Optional[str] = None) -> Dict[str, bool]:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        csv_file: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        proxy: –ü—Ä–æ–∫—Å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    """
    config = Config.load()
    config.client.max_workers = max_workers
    
    downloader = Downloader(config, proxy)
    return await downloader.download_from_csv(csv_file)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ Downloader —Å –≤–∞—à–∏–º –∫–æ–Ω—Ñ–∏–≥–æ–º"""
    
    # –°–ø–∏—Å–æ–∫ URL'–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    start_xml_path = paths.data / "sitemap.xml"
    output_csv_file = paths.data / "output.csv"
    df = pd.read_csv(output_csv_file, encoding="utf-8")
    urls = df["url"].tolist()
    # –°–∫–∞—á–∏–≤–∞–µ–º
    results = await downloader.download_urls(urls)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for url, success in results.items():
        status = "‚úÖ –£—Å–ø–µ—à–Ω–æ" if success else "‚ùå –û—à–∏–±–∫–∞"
        logger.info(f"{status}: {url}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {downloader.get_stats()}")


if __name__ == "__main__":
    asyncio.run(main())