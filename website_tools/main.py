#!/usr/bin/env python3
"""
Website Monitor - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–∞–π—Ç–æ–≤.
–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç:
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–∞ (—Ä–∞–∑–º–µ—Ä–∞) –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- –ò–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∞–π—Ç—ã –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite
–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
"""
import sys
import asyncio
import aiohttp
import sqlite3
import json
import datetime
from loguru import logger
import pandas as pd
from typing import List, Dict, Tuple, Optional, Any
from aiohttp import ClientSession, ClientTimeout, TCPConnector
import csv
import time
from pathlib import Path

current_directory = Path.cwd()
log_directory = current_directory / "log"
data_directory = current_directory / "data"
RESULTS_DIR = current_directory / "results"
db_directory = current_directory / "db"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
db_directory.mkdir(parents=True, exist_ok=True)

DB_PATH  = db_directory / "website_monitor.db"
DOMAINS_FILE  = data_directory / "domains.txt"
log_file_path = log_directory / "log_message.log"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MAX_CONCURRENT_REQUESTS = 100  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_TIMEOUT = 30  # –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
RESULTS_DIR.mkdir(exist_ok=True)

class WebsiteMonitor:
    def __init__(self, db_path: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ —Å–∞–π—Ç–æ–≤"""
        self.db_path = db_path
        self.setup_database()
        
    def setup_database(self):
        """–°–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ–º–µ–Ω–∞—Ö
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS domains (
            id INTEGER PRIMARY KEY,
            domain TEXT UNIQUE NOT NULL
        )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫–∞–Ω–æ–≤
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS scans (
            id INTEGER PRIMARY KEY,
            scan_date TIMESTAMP NOT NULL,
            description TEXT
        )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS scan_results (
            id INTEGER PRIMARY KEY,
            domain_id INTEGER NOT NULL,
            scan_id INTEGER NOT NULL,
            status_code INTEGER,
            page_size INTEGER,
            response_time REAL,
            last_modified TEXT,
            FOREIGN KEY (domain_id) REFERENCES domains (id),
            FOREIGN KEY (scan_id) REFERENCES scans (id)
        )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞ –∏–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    def load_domains(self, filepath: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'r') as f:
                domains = [line.strip() for line in f if line.strip()]
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(domains)} –¥–æ–º–µ–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞")
            return domains
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–º–µ–Ω–æ–≤: {e}")
            return []
    
    def save_domains_to_db(self, domains: List[str]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for domain in domains:
            cursor.execute("INSERT OR IGNORE INTO domains (domain) VALUES (?)", (domain,))
        
        conn.commit()
        conn.close()
        logger.info(f"–î–æ–º–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
    
    def create_new_scan(self, description: str = "") -> int:
        """–°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ—ë id"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        now = datetime.datetime.now()
        cursor.execute("INSERT INTO scans (scan_date, description) VALUES (?, ?)", (now, description))
        scan_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å ID {scan_id} –∏ –¥–∞—Ç–æ–π {now}")
        return scan_id
    
    async def scan_website(self, session: ClientSession, domain: str) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Å–∞–π—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω—ë–º"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –¥–æ–º–µ–Ω —É–∂–µ –ø—Ä–æ—Ç–æ–∫–æ–ª
        if domain.startswith(('http://', 'https://')):
            url = domain
        else:
            url = f"https://{domain}"  # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º HTTPS
        
        start_time = time.time()
        
        try:
            async with session.get(url, allow_redirects=True) as response:
                status_code = response.status
                content = await response.read()
                page_size = len(content)
                response_time = time.time() - start_time
                last_modified = response.headers.get('Last-Modified', '')
                
                return {
                    "domain": domain,
                    "status_code": status_code,
                    "page_size": page_size,
                    "response_time": response_time,
                    "last_modified": last_modified,
                    "success": True
                }
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {domain}: {e}")
            return {
                "domain": domain,
                "status_code": None,
                "page_size": None,
                "response_time": time.time() - start_time,
                "last_modified": "",
                "success": False,
                "error": str(e)
            }
    
    async def scan_all_websites(self, domains: List[str], scan_id: int) -> List[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Å–∞–π—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞"""
        results = []
        connector = TCPConnector(limit=MAX_CONCURRENT_REQUESTS, ssl=False)
        timeout = ClientTimeout(total=REQUEST_TIMEOUT)
        
        headers = {
            "User-Agent": USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        async with ClientSession(connector=connector, timeout=timeout, headers=headers) as session:
            tasks = []
            for domain in domains:
                tasks.append(self.scan_website(session, domain))
            
            # –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤
            logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {len(domains)} –¥–æ–º–µ–Ω–æ–≤...")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏ –ø–∞–∫–µ—Ç–∞–º–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
            batch_size = 100
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i+batch_size]
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
                
                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {result}")
                    else:
                        results.append(result)
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i+batch_size, len(tasks))} –∏–∑ {len(tasks)} –¥–æ–º–µ–Ω–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        self.save_scan_results(results, scan_id)
        
        return results
    
    def save_scan_results(self, results: List[Dict[str, Any]], scan_id: int):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for result in results:
            domain = result["domain"]
            
            # –ü–æ–ª—É—á–∞–µ–º id –¥–æ–º–µ–Ω–∞
            cursor.execute("SELECT id FROM domains WHERE domain = ?", (domain,))
            domain_row = cursor.fetchone()
            
            if domain_row:
                domain_id = domain_row[0]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                cursor.execute("""
                INSERT INTO scan_results 
                (domain_id, scan_id, status_code, page_size, response_time, last_modified)
                VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    domain_id, 
                    scan_id,
                    result.get("status_code"),
                    result.get("page_size"),
                    result.get("response_time"),
                    result.get("last_modified")
                ))
        
        conn.commit()
        conn.close()
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
    
    def compare_with_previous_scan(self, current_scan_id: int) -> Tuple[List[Dict], List[Dict]]:
        """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        1. –°–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –∏–∑–º–µ–Ω–∏–ª—Å—è –≤–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        2. –°–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –∏–∑–º–µ–Ω–∏–ª—Å—è —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        """
        conn = sqlite3.connect(self.db_path)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pandas –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π scan_id
        previous_scan_df = pd.read_sql_query(
            "SELECT id FROM scans WHERE id < ? ORDER BY id DESC LIMIT 1",
            conn,
            params=(current_scan_id,)
        )
        
        if previous_scan_df.empty:
            logger.info("–≠—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            conn.close()
            return [], []
        
        previous_scan_id = previous_scan_df.iloc[0]['id']
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        query = """
        SELECT 
            d.domain,
            sr.status_code,
            sr.page_size,
            s.scan_date,
            s.id as scan_id
        FROM 
            scan_results sr
        JOIN 
            domains d ON sr.domain_id = d.id
        JOIN 
            scans s ON sr.scan_id = s.id
        WHERE 
            sr.scan_id IN (?, ?)
        """
        
        results_df = pd.read_sql_query(
            query,
            conn,
            params=(previous_scan_id, current_scan_id)
        )
        
        conn.close()
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ–∫—É—â–∏–µ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        current_results = results_df[results_df['scan_id'] == current_scan_id].copy()
        previous_results = results_df[results_df['scan_id'] == previous_scan_id].copy()
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        current_dict = {row['domain']: row for _, row in current_results.iterrows()}
        previous_dict = {row['domain']: row for _, row in previous_results.iterrows()}
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        size_changes = []
        status_changes = []
        
        for domain, current in current_dict.items():
            if domain in previous_dict:
                previous = previous_dict[domain]
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
                if current['page_size'] != previous['page_size'] and current['page_size'] is not None and previous['page_size'] is not None:
                    size_diff = current['page_size'] - previous['page_size']
                    size_changes.append({
                        'domain': domain,
                        'previous_size': previous['page_size'],
                        'current_size': current['page_size'],
                        'diff': size_diff,
                        'diff_percentage': (size_diff / previous['page_size']) * 100 if previous['page_size'] > 0 else 0,
                        'scan_date': current['scan_date']
                    })
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
                if current['status_code'] != previous['status_code']:
                    status_changes.append({
                        'domain': domain,
                        'previous_status': previous['status_code'],
                        'current_status': current['status_code'],
                        'scan_date': current['scan_date']
                    })
            
            # –ù–æ–≤—ã–µ –¥–æ–º–µ–Ω—ã (–≤ —Ç–µ–∫—É—â–µ–º —Å–∫–∞–Ω–µ, –Ω–æ –Ω–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º)
            elif domain not in previous_dict and current['status_code'] is not None:
                status_changes.append({
                    'domain': domain,
                    'previous_status': None,
                    'current_status': current['status_code'],
                    'scan_date': current['scan_date'],
                    'note': 'New domain'
                })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—á–µ–∑–ª–∏
        for domain, previous in previous_dict.items():
            if domain not in current_dict and previous['status_code'] is not None:
                status_changes.append({
                    'domain': domain,
                    'previous_status': previous['status_code'],
                    'current_status': None,
                    'scan_date': previous['scan_date'],
                    'note': 'Domain disappeared'
                })
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(size_changes)} –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ –∏ {len(status_changes)} –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–∞")
        return size_changes, status_changes
    
    def export_changes_to_csv(self, size_changes: List[Dict], status_changes: List[Dict]):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ CSV —Ñ–∞–π–ª—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞
        if size_changes:
            size_changes_file = RESULTS_DIR / f"size_changes_{timestamp}.csv"
            with open(size_changes_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=size_changes[0].keys())
                writer.writeheader()
                writer.writerows(size_changes)
            logger.info(f"–ò–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {size_changes_file}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–∞
        if status_changes:
            status_changes_file = RESULTS_DIR / f"status_changes_{timestamp}.csv"
            with open(status_changes_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=status_changes[0].keys())
                writer.writeheader()
                writer.writerows(status_changes)
            logger.info(f"–ò–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {status_changes_file}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞
        all_changes = []
        
        for change in size_changes:
            all_changes.append({
                'domain': change['domain'],
                'change_type': 'size',
                'previous_value': change['previous_size'],
                'current_value': change['current_size'],
                'diff': change['diff'],
                'diff_percentage': change['diff_percentage'],
                'scan_date': change['scan_date']
            })
        
        for change in status_changes:
            all_changes.append({
                'domain': change['domain'],
                'change_type': 'status',
                'previous_value': change['previous_status'],
                'current_value': change['current_status'],
                'diff': None,
                'diff_percentage': None,
                'scan_date': change['scan_date'],
                'note': change.get('note', '')
            })
        
        if all_changes:
            dashboard_file = RESULTS_DIR / f"dashboard_data_{timestamp}.csv"
            with open(dashboard_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=all_changes[0].keys())
                writer.writeheader()
                writer.writerows(all_changes)
            logger.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {dashboard_file}")
            
            # –°–æ–∑–¥–∞–µ–º JSON –¥–ª—è –ª–µ–≥–∫–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥–∞—à–±–æ—Ä–¥–æ–º
            dashboard_json = RESULTS_DIR / f"dashboard_data_{timestamp}.json"
            with open(dashboard_json, 'w') as f:
                json.dump(all_changes, f)
            logger.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞ (JSON) —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {dashboard_json}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    monitor = WebsiteMonitor(DB_PATH)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–º–µ–Ω–æ–≤
    domains = monitor.load_domains(DOMAINS_FILE)
    if not domains:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–º–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞ {DOMAINS_FILE}")
        return
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    monitor.save_domains_to_db(domains)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    scan_id = monitor.create_new_scan(description="–ï–∂–µ–º–µ—Å—è—á–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∞–π—Ç–æ–≤")
    
    # –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(monitor.scan_all_websites(domains, scan_id))
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    size_changes, status_changes = monitor.compare_with_previous_scan(scan_id)
    
    # –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ CSV
    if size_changes or status_changes:
        monitor.export_changes_to_csv(size_changes, status_changes)
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        logger.info(f"–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å–∞–π—Ç–æ–≤: {len(domains)}")
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(size_changes)}")
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞: {len(status_changes)}")
    else:
        logger.info("–ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

if __name__ == "__main__":
    main()