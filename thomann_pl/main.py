import csv
import hashlib
import json
import os
import re
import shutil
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
config_directory = current_directory / "config"
data_directory = current_directory / "data"
log_directory = current_directory / "log"
html_directory = current_directory / "html"
html_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
output_xlsx_file = data_directory / "thomann.xlsx"
output_csv_file = data_directory / "output.csv"
output_json_file = data_directory / "output.json"
log_file_path = log_directory / "log_message.log"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
cookies = {
    "sid": "8b38b72f9859d012d7a3c31725a200d7",
    "thomann_settings": "1b949fd4-46c6-44e8-842a-538fdac94f1f",
    "uslk_umm_27718_s": "ewAiAHYAZQByAHMAaQBvAG4AIgA6ACIAMQAiACwAIgBkAGEAdABhACIAOgB7AH0AfQA=",
    "__cf_bm": "szK1gdsrSId8_3cFefMErvPbQSAUMG2d8oJRzHeKpxs-1743707252-1.0.1.1-oVPzr8eK6S3IJQtUIwHwOOF8xeiNYq9ffesRpbWbilQ0vaVXE158wx8vgkBg3zIgwCYT.vbis9ljXAfrq18A8Byqd_T_FmmOq0MX6pc8nDo",
}

headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "cache-control": "no-cache",
    "dnt": "1",
    "pragma": "no-cache",
    "priority": "u=0, i",
    "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "same-origin",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
}


def read_xlsx():

    # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª, —É–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - –∑–∞–≥–æ–ª–æ–≤–æ–∫
    df = pd.read_excel(output_xlsx_file, header=0)

    # –ë–µ—Ä–µ–º –∫–æ–ª–æ–Ω–∫—É –ø–æ –∏–º–µ–Ω–∏ 'url' (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ)
    urls = df["url"]

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π DataFrame
    result_df = pd.DataFrame(urls, columns=["url"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    result_df.to_csv(output_csv_file, index=False)


def main_th():

    urls = []
    with open(output_csv_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            urls.append(row["url"])

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for url in urls:
            output_html_file = (
                html_directory / f"{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html, url, output_html_file))
                # time.sleep(random.uniform(5, 10))
                time.sleep(5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            else:
                logger.info(f"–§–∞–π–ª –¥–ª—è {url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def fetch(url):
    try:
        response = requests.get(
            url, cookies=cookies, headers=headers, timeout=30, stream=True
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞
        if response.status_code != 200:
            logger.warning(
                f"–°—Ç–∞—Ç—É—Å –Ω–µ 200 –¥–ª—è {url}. –ü–æ–ª—É—á–µ–Ω —Å—Ç–∞—Ç—É—Å: {response.status_code}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º."
            )
            return None
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8
        response.encoding = "utf-8"
        return response.text

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")
        return None


def get_html(url, html_file):
    src = fetch(url)

    if src is None:
        return url, html_file, False

    with open(html_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_file}")
    return url, html_file, True


def pars_htmls():
    logger.info("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü html")
    all_data = []

    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(content, "lxml")

        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
        result = {}

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä—Ç–∏–∫—É–ª (Numer artyku≈Çu)
        labels = soup.find_all("span", class_="keyfeature__label")
        for label in labels:

            if label.text.strip() == "Numer artyku≈Çu":
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π span —Å –∫–ª–∞—Å—Å–æ–º fx-text--bold
                article_number = label.find_next("span", class_="fx-text--bold")
                if article_number:
                    result["article_number"] = article_number.text.strip()
                    break  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª, –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω—É–∂–Ω—ã–π –∞—Ä—Ç–∏–∫—É–ª

        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (title)
        product_title = soup.find("div", class_="fx-content-product__main")
        if product_title:
            h1 = product_title.find("h1", itemprop="name")
            if h1:
                result["title"] = re.sub(r"\s+", " ", h1.text).strip()

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
        price_wrapper = soup.find("div", class_="price-wrapper")
        if price_wrapper:
            price_div = price_wrapper.find("div", class_="price")
            if price_div:
                price = price_div.text.strip().replace(
                    " z≈Ç", ""
                )  # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª –≤–∞–ª—é—Ç—ã
                result["price"] = price

        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        availability = soup.find("span", class_=lambda x: x and "fx-availability" in x)

        if availability:
            result["availability"] = availability.text.strip()
        else:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–ª—è {html_file.name}.")
            exit()

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –Ω–µ –ø—É—Å—Ç–æ–π
        if result:
            all_data.append(result)

    logger.info(f"–°–æ–±—Ä–∞–Ω–æ {len(all_data)} –∑–∞–ø–∏—Å–µ–π")
    # logger.info(all_data)
    with open(output_json_file, "w", encoding="utf-8") as json_file:
        json.dump(all_data, json_file, ensure_ascii=False, indent=4)

    update_excel_with_array(all_data)


def update_excel_with_array(data_array):
    # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª, –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - –∑–∞–≥–æ–ª–æ–≤–æ–∫
    df = pd.read_excel(output_xlsx_file, header=0)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Å—Å–∏–≤ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ title
    array_dict = {item["title"]: item for item in data_array}

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ DataFrame
    for index, row in df.iterrows():
        excel_title = row["title"]
        # –ï—Å–ª–∏ title –∏–∑ Excel –µ—Å—Ç—å –≤ –º–∞—Å—Å–∏–≤–µ
        if excel_title in array_dict:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
            df.at[index, "article_number"] = float(
                array_dict[excel_title]["article_number"]
            )
            df.at[index, "price"] = array_dict[excel_title]["price"]
            df.at[index, "availability"] = array_dict[excel_title]["availability"]
        else:
            logger.warning(
                f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª—è {excel_title} –≤ –º–∞—Å—Å–∏–≤–µ –¥–∞–Ω–Ω—ã—Ö."
            )
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–æ—Ç –∂–µ Excel —Ñ–∞–π–ª
    df.to_excel(output_xlsx_file, index=False)
    logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ {output_xlsx_file}")


if __name__ == "__main__":
    read_xlsx()
    while True:
        print(
            "\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:\n"
            "1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ\n"
            "2. –ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ï–∫—Å–µ–ª—å\n"
            "3. –û—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã\n"
            "0. –í—ã—Ö–æ–¥"
        )
        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è: ")

        if choice == "1":
            main_th()

        elif choice == "2":
            pars_htmls()
        elif choice == "3":
            shutil.rmtree(html_directory)
            if not os.path.exists(html_directory):
                html_directory.mkdir(parents=True, exist_ok=True)
        elif choice == "0":
            exit()
        else:
            logger.info("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
