import json
import queue
import random
import sys
import threading
import time
from pathlib import Path

import pandas as pd
import requests
from loguru import logger

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
MAX_RETRIES = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
RETRY_DELAY = 5  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

current_directory = Path.cwd()
json_directory = current_directory / "json"
data_directory = current_directory / "data"
log_directory = current_directory / "log"
data_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
json_directory.mkdir(parents=True, exist_ok=True)

input_csv_file = data_directory / "output.csv"
log_file_path = log_directory / "log_message.log"
proxy_file = data_directory / "proxy.txt"  # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –ø—Ä–æ–∫—Å–∏

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ headers –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≥–¥–µ-—Ç–æ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ
headers = {
    "accept": "*/*",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "origin": "https://ba.prg.kz",
    "priority": "u=1, i",
    "referer": "https://ba.prg.kz/",
    "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "cross-site",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ (–±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤—Å–µ–º –ø–æ—Ç–æ–∫–∞–º)
ALL_PROXIES = []


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞
def load_proxies():
    if not proxy_file.exists():
        logger.warning(
            f"–§–∞–π–ª —Å –ø—Ä–æ–∫—Å–∏ {proxy_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"
        )
        return []

    try:
        with open(proxy_file, "r") as f:
            proxies = [line.strip() for line in f if line.strip()]

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞")
        return proxies
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–∫—Å–∏: {str(e)}")
        return []


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
def get_random_proxy():
    if not ALL_PROXIES:
        return None
    return random.choice(ALL_PROXIES)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è ID –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞
def read_companies_from_csv(input_csv_file):
    df = pd.read_csv(input_csv_file)
    return df["bin"].tolist()


# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏
def get_json(id_company, q):
    file_name = json_directory / f"{id_company}.json"

    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if file_name.exists():
        logger.warning(f"–§–∞–π–ª –¥–ª—è ID {id_company} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        q.task_done()
        return

    retry_count = 0

    while retry_count < MAX_RETRIES:
        # –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏
        proxy = get_random_proxy()
        proxies = None

        if proxy:
            proxies = {"http": proxy, "https": proxy}
            logger.debug(
                f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏ {proxy} –¥–ª—è ID {id_company} (–ø–æ–ø—ã—Ç–∫–∞ {retry_count+1})"
            )

        try:
            params = {
                "id": id_company,
                "lang": "ru",
            }

            response = requests.get(
                "https://apiba.prgapp.kz/CompanyFullInfo",
                params=params,
                headers=headers,
                proxies=proxies,
                timeout=30,
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if response.status_code == 200:
                try:
                    data = response.json()
                    with open(file_name, "w", encoding="utf-8") as f:
                        json.dump(data, f, ensure_ascii=False, indent=4)
                    if proxy:
                        logger.info(
                            f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª –¥–ª—è ID {id_company} (–ø—Ä–æ–∫—Å–∏: {proxy})"
                        )
                    else:
                        logger.info(
                            f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª –¥–ª—è ID {id_company} (–±–µ–∑ –ø—Ä–æ–∫—Å–∏)"
                        )
                    q.task_done()
                    return
                except ValueError:
                    logger.error(f"–û—à–∏–±–∫–∞: –æ—Ç–≤–µ—Ç –¥–ª—è ID {id_company} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç JSON")
            else:
                if proxy:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –¥–ª—è ID {id_company}: —Å—Ç–∞—Ç—É—Å {response.status_code} (–ø—Ä–æ–∫—Å–∏: {proxy})"
                    )
                else:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ –¥–ª—è ID {id_company}: —Å—Ç–∞—Ç—É—Å {response.status_code} (–±–µ–∑ –ø—Ä–æ–∫—Å–∏)"
                    )

            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ 200 –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
            retry_count += 1
            logger.warning(
                f"–ü–æ–ø—ã—Ç–∫–∞ {retry_count}/{MAX_RETRIES} –¥–ª—è ID {id_company}, –ø–∞—É–∑–∞ {RETRY_DELAY} —Å–µ–∫."
            )
            time.sleep(RETRY_DELAY)

        except requests.exceptions.RequestException as e:
            if proxy:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è ID {id_company} (–ø—Ä–æ–∫—Å–∏: {proxy}): {str(e)}"
                )
            else:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è ID {id_company} (–±–µ–∑ –ø—Ä–æ–∫—Å–∏): {str(e)}"
                )

            retry_count += 1
            logger.error(
                f"–ü–æ–ø—ã—Ç–∫–∞ {retry_count}/{MAX_RETRIES} –¥–ª—è ID {id_company}, –ø–∞—É–∑–∞ {RETRY_DELAY} —Å–µ–∫."
            )
            time.sleep(RETRY_DELAY)

    logger.warning(f"–ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è ID {id_company}")
    q.task_done()


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø–æ—Ç–æ–∫–∞
def worker(q, thread_id):
    logger.info(f"–ó–∞–ø—É—â–µ–Ω –ø–æ—Ç–æ–∫ {thread_id}")

    while True:
        id_company = q.get()
        if id_company is None:  # –°–∏–≥–Ω–∞–ª –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞
            q.task_done()
            break
        get_json(id_company, q)


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def process_companies(num_threads=5):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
    global ALL_PROXIES
    ALL_PROXIES = load_proxies()

    if ALL_PROXIES:
        logger.info(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {len(ALL_PROXIES)} —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤")
    else:
        logger.info("–ü—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ ID –∫–æ–º–ø–∞–Ω–∏–π
    company_ids = read_companies_from_csv(input_csv_file)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(company_ids)} ID –∫–æ–º–ø–∞–Ω–∏–π")

    # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥—å –∏ –ø–æ—Ç–æ–∫–∏
    q = queue.Queue()
    threads = []

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–±–æ—á–∏–µ –ø–æ—Ç–æ–∫–∏
    for i in range(num_threads):
        t = threading.Thread(target=worker, args=(q, i))
        t.daemon = True
        t.start()
        threads.append(t)

    # –î–æ–±–∞–≤–ª—è–µ–º ID –∫–æ–º–ø–∞–Ω–∏–π –≤ –æ—á–µ—Ä–µ–¥—å
    for id_company in company_ids:
        q.put(id_company)

    # –î–æ–∂–∏–¥–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏
    q.join()

    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏
    for i in range(num_threads):
        q.put(None)

    # –î–æ–∂–∏–¥–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤
    for t in threads:
        t.join()

    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    num_threads = 50  # –£–∫–∞–∂–∏—Ç–µ –∂–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤

    process_companies(num_threads)
