import json
import sys
from pathlib import Path
import pandas as pd
import requests
from loguru import logger
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
from pathlib import Path
import time
import requests
from requests.exceptions import RequestException
import json
import hashlib
import json
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import re
from sqlalchemy.orm import Session
import pandas as pd
from sqlalchemy import select, join
import pandas as pd
import requests
from loguru import logger
from sqlalchemy import create_engine, Column, String, Integer, Float, DateTime
from sqlalchemy.orm import sessionmaker
from datetime import datetime
from sqlalchemy.orm import DeclarativeBase
import time
import json
from pathlib import Path
from datetime import datetime
import logging
from collections import defaultdict
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import create_engine, String, Integer
import json

current_directory = Path.cwd()
json_category_directory = current_directory / "json_category"
json_comment_directory = current_directory / "json_comment"
log_directory = current_directory / "log"
data_directory = current_directory / "data"
html_directory = current_directory / "html"
html_id_directory = current_directory / "html_id"

html_id_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
json_category_directory.mkdir(parents=True, exist_ok=True)
json_comment_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
output_csv_file = data_directory / "output.csv"
output_id_csv_file = data_directory / "output_id.csv"
output_xlsx_file = data_directory / "output.xlsx"

BASE_URL = "https://kaspi.kz"
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "ru,en;q=0.9,uk;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    # 'Cookie': 'layout=d; dt-i=env=production|ssrVersion=v1.18.39|pageMode=catalog; ks.tg=47; k_stat=a6864b24-87cc-4ce5-94ea-db68e661c075; current-action-name=Index; kaspi.storefront.cookie.city=750000000',
    "DNT": "1",
    "Pragma": "no-cache",
    "Referer": "https://kaspi.kz/shop/c/categories/",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-User": "?1",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
    "sec-ch-ua": '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
}

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
class Base(DeclarativeBase):
    pass

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
class Merchant(Base):
    __tablename__ = 'merchants'
    __table_args__ = {'extend_existing': True}  # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä
    id = Column(Integer, primary_key=True, autoincrement=True)
    uid = Column(String(50), unique=True, nullable=False)
    name = Column(String(255), nullable=False)
    phone = Column(String(20))
    create_date = Column(DateTime)
    sales_count = Column(Integer)
    rating = Column(Float)
    
    def __repr__(self):
        return f"<Merchant(uid={self.uid}, name={self.name})>"
class Review(Base):
    __tablename__ = 'reviews'
    __table_args__ = {'extend_existing': True}  # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä
    id: Mapped[int] = mapped_column(primary_key=True)
    uid: Mapped[str] = mapped_column(String(100), nullable=False)
    year_2025: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2024: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2023: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2022: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2021: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2020: Mapped[int] = mapped_column(Integer, nullable=True)
    year_2019: Mapped[int] = mapped_column(Integer, nullable=True)
    
    def __repr__(self):
        return f"<Review(uid={self.uid})>"
def create_db_connection(
    user="python_mysql", password="python_mysql", host="localhost", db="kaspi_kz"
):
    """Create database connection and return engine"""
    engine = create_engine(f"mysql+pymysql://{user}:{password}@{host}/{db}", echo=True)
    return engine

def init_db(engine):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    Base.metadata.create_all(engine)

def save_merchant_data(engine, merchant_data):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–≤—Ü–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
    Session = sessionmaker(bind=engine)
    session = Session()
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã –≤ –æ–±—ä–µ–∫—Ç datetime
        create_date = datetime.strptime(merchant_data['create'], '%Y-%m-%dT%H:%M:%S.%f')
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç Merchant
        merchant = Merchant(
            uid=merchant_data['uid'],
            name=merchant_data['name'],
            phone=merchant_data['phone'],
            create_date=create_date,
            sales_count=merchant_data['salesCount'],
            rating=merchant_data['rating']
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –ø—Ä–æ–¥–∞–≤–µ—Ü —Å —Ç–∞–∫–∏–º uid
        existing_merchant = session.query(Merchant).filter_by(uid=merchant_data['uid']).first()
        
        if existing_merchant:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
            existing_merchant.name = merchant.name
            existing_merchant.phone = merchant.phone
            existing_merchant.create_date = merchant.create_date
            existing_merchant.sales_count = merchant.sales_count
            existing_merchant.rating = merchant.rating
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            session.add(merchant)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        session.commit()
        print(f"Successfully saved/updated merchant with UID: {merchant_data['uid']}")
        
    except Exception as e:
        print(f"Error saving merchant data: {e}")
        session.rollback()
        raise
    finally:
        session.close()


def get_json_category():

    cookies = {
        "ks.tg": "47",
        "k_stat": "a6864b24-87cc-4ce5-94ea-db68e661c075",
        "current-action-name": "Index",
        "kaspi.storefront.cookie.city": "750000000",
    }

    headers = {
        "Accept": "application/json, text/*",
        "Accept-Language": "ru,en;q=0.9,uk;q=0.8",
        "Connection": "keep-alive",
        # 'Cookie': 'ks.tg=47; k_stat=a6864b24-87cc-4ce5-94ea-db68e661c075; current-action-name=Index; kaspi.storefront.cookie.city=750000000',
        "DNT": "1",
        "Referer": "https://kaspi.kz/shop/c/categories/",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
        "X-KS-City": "750000000",
        "sec-ch-ua": '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }
    codes = [
        "Smartphones and gadgets",
        "Home equipment",
        "TV_Audio",
        "Computers",
        "Furniture",
        "Beauty care",
        "Child goods",
        "Pharmacy",
        "Construction and repair",
        "Sports and outdoors",
        "Leisure",
        "Car goods",
        "Jewelry and Bijouterie",
        "Fashion accessories",
        "Fashion",
        "Shoes",
        "Pet goods",
        "Home",
        "Gifts and party supplies",
        "Office and school supplies",
    ]
    for code in codes:
        logger.info(code)
        params = {
            "depth": "2",
            "city": "750000000",
            "code": code,
            "rootType": "desktop",
        }
        json_category_file = json_category_directory / f"{code}.json"
        if json_category_file.exists():
            continue
        response = requests.get(
            "https://kaspi.kz/yml/main-navigation/n/n/desktop-menu",
            params=params,
            cookies=cookies,
            headers=headers,
            timeout=30,
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            json_data = response.json()
            with open(json_category_file, "w", encoding="utf-8") as f:
                json.dump(
                    json_data, f, ensure_ascii=False, indent=4
                )  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
            logger.info(json_category_file)
        else:
            logger.error(response.status_code)


def scrap_json_category():
    all_data = []
    for json_file in json_category_directory.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as file:
            data = json.load(file)
        subnodes_level_01 = data.get("subNodes", None)
        if subnodes_level_01:
            for level_02 in subnodes_level_01:
                subnodes_level_02 = level_02.get("subNodes", None)
                if subnodes_level_02:
                    for level_03 in subnodes_level_02:
                        subnodes_level_03 = level_03.get("link", None)
                        url = f"{BASE_URL}{subnodes_level_03}"
                        all_data.append(url)

    # –°–æ–∑–¥–∞—Ç—å DataFrame –∏–∑ —Å–ø–∏—Å–∫–∞ URL
    df = pd.DataFrame(all_data, columns=["url"])

    # –ó–∞–ø–∏—Å–∞—Ç—å DataFrame –≤ CSV —Ñ–∞–π–ª
    df.to_csv(output_csv_file, index=False)


def main_th():
    urls = []
    with open(output_csv_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            urls.append(row["url"])

    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = []
        for url in urls:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html, url, output_html_file))
            else:
                print(f"–§–∞–π–ª –¥–ª—è {url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def fetch(url):
    cookies = {
        "layout": "d",
        "dt-i": "env=production|ssrVersion=v1.18.39|pageMode=catalog",
        "ks.tg": "47",
        "k_stat": "a6864b24-87cc-4ce5-94ea-db68e661c075",
        "current-action-name": "Index",
        "kaspi.storefront.cookie.city": "750000000",
    }

    try:
        response = requests.get(url, cookies=cookies, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.exceptions.HTTPError as err:
        logger.error(f"HTTP error occurred: {err}")
        return None
    except Exception as err:
        logger.error(f"An error occurred: {err}")
        return None
    return response.text


def get_html(url, html_file):
    src = fetch(url)
    if src:
        with open(html_file, "w", encoding="utf-8") as file:
            file.write(src)
        logger.info(f"HTML saved to {html_file}")


def main_th_id():
    ids = []
    with open(output_id_csv_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            ids.append(row["url"])

    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = []
        for id_site in ids:
            output_html_file = (
                html_id_directory
                / f"html_{hashlib.md5(id_site.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html_id, id_site, output_html_file))
            else:
                logger.warning(f"–§–∞–π–ª –¥–ª—è {id} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def fetch_id(merchantId):
    cookies = {
        "layout": "d",
        "dt-i": "env=production|ssrVersion=v1.18.39|pageMode=merchant",
        "ks.tg": "47",
        "k_stat": "a6864b24-87cc-4ce5-94ea-db68e661c075",
        "current-action-name": "Index",
        "kaspi.storefront.cookie.city": "750000000",
    }

    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "ru,en;q=0.9,uk;q=0.8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        # 'Cookie': 'layout=d; dt-i=env=production|ssrVersion=v1.18.39|pageMode=merchant; ks.tg=47; k_stat=a6864b24-87cc-4ce5-94ea-db68e661c075; current-action-name=Index; kaspi.storefront.cookie.city=750000000',
        "DNT": "1",
        "Pragma": "no-cache",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
        "sec-ch-ua": '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }

    try:
        url = f"https://kaspi.kz/shop/info/merchant/{merchantId}/address-tab/"
        logger.info(url)
        response = requests.get(url, cookies=cookies, headers=headers, timeout=30)
        response.raise_for_status()
        logger.info(response.status_code)
    except requests.exceptions.HTTPError as err:
        logger.error(f"HTTP error occurred: {err}")
        return None
    except Exception as err:
        logger.error(f"An error occurred: {err}")
        return None
    return response.text


def get_html_id(url, html_file):
    src = fetch_id(url)
    if src:
        with open(html_file, "w", encoding="utf-8") as file:
            file.write(src)
        logger.info(f"HTML saved to {html_file}")


def scrap_html():
    all_data = set()
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö allMerchants
        pattern = re.compile(r'\{"id":"(:allMerchants:[^"]+)",.*?\}', re.DOTALL)
        matches = pattern.findall(content)

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        all_merchants = []
        for match in matches:
            merchant_pattern = re.compile(
                r'\{"id":"' + re.escape(match) + r'",.*?\}', re.DOTALL
            )
            merchant_data = merchant_pattern.search(content)
            if merchant_data:
                all_merchants.append(json.loads(merchant_data.group(0)))

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for merchant in all_merchants:
            id_site = merchant["id"].replace(":allMerchants:", "")
            all_data.add(id_site)
        # –°–æ–∑–¥–∞—Ç—å DataFrame –∏–∑ —Å–ø–∏—Å–∫–∞ URL
    df = pd.DataFrame(all_data, columns=["url"])

    # –ó–∞–ø–∏—Å–∞—Ç—å DataFrame –≤ CSV —Ñ–∞–π–ª
    df.to_csv(output_id_csv_file, index=False)
def scrap_html_id():
    engine = create_db_connection()
    init_db(engine)
    
    all_data = []
    for html_file in html_id_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        pattern = r'BACKEND\.components\.merchant = ({.*?});?\s*</script>'
        match = re.search(pattern, content, re.DOTALL)
        
        if match:
            json_str = match.group(1).strip()
            try:
                merchant_data = json.loads(json_str)
                data = {
                    "uid": merchant_data.get("uid"),
                    "name": merchant_data.get("name"),
                    "phone": merchant_data.get("phone"),
                    "create": merchant_data.get("create"),
                    "salesCount": merchant_data.get("salesCount"),
                    "rating": merchant_data.get("rating")
                }
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ë–î
                save_merchant_data(engine, data)
                
                all_data.append(data)
                logger.info(data)
            except Exception as e:
                logger.error(f"Error processing merchant data: {e}")
                
    return all_data

def get_json_comment(merchant):

    cookies = {
        'ks.tg': '47',
        'k_stat': 'a6864b24-87cc-4ce5-94ea-db68e661c075',
        'current-action-name': 'Index',
        'kaspi.storefront.cookie.city': '750000000',
    }

    headers = {
        'Accept': 'application/json, text/*',
        'Accept-Language': 'ru,en;q=0.9,uk;q=0.8',
        'Connection': 'keep-alive',
        # 'Cookie': 'ks.tg=47; k_stat=a6864b24-87cc-4ce5-94ea-db68e661c075; current-action-name=Index; kaspi.storefront.cookie.city=750000000',
        'DNT': '1',
        'Referer': 'https://kaspi.kz/shop/info/merchant/1274021/address-tab/',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
        'X-KS-City': '750000000',
        'sec-ch-ua': '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
    }
    try:
        for page in range(0, 3):
            json_file_path = json_comment_directory / f"0{page}_{merchant}.json"
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
            if json_file_path.exists():
                logger.info(f"File already exists: {json_file_path}")
                continue
                
            params = {
                'limit': '1000',
                'page': page,
                'filter': 'COMMENT',
                'sort': 'DATE',
                'withAgg': 'false',
                'days': '2000',
            }
            
            try:
                response = requests.get(
                    f'https://kaspi.kz/yml/review-view/api/v1/reviews/merchant/{merchant}',
                    params=params,
                    cookies=cookies,
                    headers=headers,
                    timeout=30
                )
                response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
                with open(json_file_path, "w", encoding="utf-8") as json_file:
                    json.dump(response.json(), json_file, ensure_ascii=False, indent=4)
                
                logger.info(f"Successfully saved {json_file_path}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(1)
                
            except RequestException as e:
                logger.error(f"Error fetching data for merchant {merchant}, page {page}: {e}")
                continue
                
        return f"Completed processing merchant {merchant}"
        
    except Exception as e:
        logger.error(f"Unexpected error processing merchant {merchant}: {e}")
        return f"Failed processing merchant {merchant}: {str(e)}"
        
def main_comment():
    # –ß–∏—Ç–∞–µ–º IDs –∏–∑ CSV
    ids = []
    try:
        with open(output_id_csv_file, newline="", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            ids = [row["url"] for row in reader]
    except Exception as e:
        logger.error(f"Error reading CSV file: {e}")
        return

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
    with ThreadPoolExecutor(max_workers=50) as executor:
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–¥–∞—á
        future_to_merchant = {executor.submit(get_json_comment, merchant): merchant for merchant in ids}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        for future in as_completed(future_to_merchant):
            merchant = future_to_merchant[future]
            try:
                result = future.result()
                logger.info(result)
            except Exception as e:
                logger.error(f"Error processing merchant {merchant}: {e}")
def process_reviews_by_year():
    all_stats = []
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ merchant_uid
    merchant_files = {}
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã
        json_files = list(Path(json_comment_directory).glob("[0-9]*_*.json"))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ merchant_uid
        for json_file in json_files:
            merchant_uid = json_file.stem.split('_')[1]
            if merchant_uid not in merchant_files:
                merchant_files[merchant_uid] = []
            merchant_files[merchant_uid].append(json_file)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –∫–∞–∂–¥–æ–≥–æ –º–µ—Ä—á–∞–Ω—Ç–∞
        for merchant_uid, files in merchant_files.items():
            stats = defaultdict(int)
            
            for json_file in files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤
                    for review in data.get('data', []):
                        try:
                            date_str = review.get('date')
                            if date_str:
                                date_obj = datetime.strptime(date_str, '%d.%m.%Y')
                                year = date_obj.year
                                stats[str(year)] += 1
                        except ValueError as e:
                            logger.error(f"Error parsing date in review: {date_str}, Error: {e}")
                            continue
                            
                except json.JSONDecodeError as e:
                    logger.error(f"Error decoding JSON file {json_file}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"Error processing file {json_file}: {e}")
                    continue
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Ä—á–∞–Ω—Ç–∞
            result = {
                "uid": merchant_uid
            }
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥–æ–¥–∞–º –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è
            for year in sorted(stats.keys(), reverse=True):
                result[year] = str(stats[year])
            
            all_stats.append(result)
            logger.info(f"Processed merchant: {merchant_uid}")
        
        return all_stats
        
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return None

def save_stats_to_json(stats, output_file):
    if stats:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            logger.info(f"Statistics saved to {output_file}")
        except Exception as e:
            logger.error(f"Error saving statistics to file: {e}")
def import_reviews_from_json(json_file):
    engine = create_db_connection()
    init_db(engine)
    """–ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    from sqlalchemy.orm import Session
    
    # –ß–∏—Ç–∞–µ–º JSON —Ñ–∞–π–ª
    with open(json_file, 'r', encoding='utf-8') as f:
        reviews_data = json.load(f)
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
    session = Session(engine)
    
    try:
        for review_data in reviews_data:
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Review
            review = Review(
                uid=review_data['uid'],
                year_2025=int(review_data.get('2025', 0)),
                year_2024=int(review_data.get('2024', 0)),
                year_2023=int(review_data.get('2023', 0)),
                year_2022=int(review_data.get('2022', 0)),
                year_2021=int(review_data.get('2021', 0)),
                year_2020=int(review_data.get('2020', 0)),
                year_2019=int(review_data.get('2019', 0))
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏
            existing_review = session.query(Review).filter_by(uid=review.uid).first()
            
            if existing_review:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                existing_review.year_2025 = review.year_2025
                existing_review.year_2024 = review.year_2024
                existing_review.year_2023 = review.year_2023
                existing_review.year_2022 = review.year_2022
                existing_review.year_2021 = review.year_2021
                existing_review.year_2020 = review.year_2020
                existing_review.year_2019 = review.year_2019
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                session.add(review)
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        session.commit()
        print("Data successfully imported")
        
    except Exception as e:
        print(f"Error importing data: {e}")
        session.rollback()
    finally:
        session.close()
def export_to_excel(engine, output_file='merchants_reviews.xlsx'):
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü
        query = select(
            Merchant.uid,
            Merchant.name,
            Merchant.phone,
            Merchant.create_date,
            Merchant.sales_count,
            Merchant.rating,
            Review.year_2025,
            Review.year_2024,
            Review.year_2023,
            Review.year_2022,
            Review.year_2021,
            Review.year_2020,
            Review.year_2019
        ).join(Review, Merchant.uid == Review.uid)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        with Session(engine) as session:
            results = session.execute(query).all()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
        df = pd.DataFrame(results, columns=[
            'UID', '–ù–∞–∑–≤–∞–Ω–∏–µ', '–¢–µ–ª–µ—Ñ–æ–Ω', '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è', 
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥–∞–∂', '–†–µ–π—Ç–∏–Ω–≥',
            '–û—Ç–∑—ã–≤—ã 2025', '–û—Ç–∑—ã–≤—ã 2024', '–û—Ç–∑—ã–≤—ã 2023',
            '–û—Ç–∑—ã–≤—ã 2022', '–û—Ç–∑—ã–≤—ã 2021', '–û—Ç–∑—ã–≤—ã 2020', '–û—Ç–∑—ã–≤—ã 2019'
        ])

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
        df['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'] = pd.to_datetime(df['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è']).dt.strftime('%d.%m.%Y')

        # –°–æ–∑–¥–∞–µ–º writer –¥–ª—è Excel —Å engine='xlsxwriter'
        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º DataFrame –≤ Excel
            df.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)

            # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç workbook –∏ worksheet
            workbook = writer.book
            worksheet = writer.sheets['–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞']

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            header_format = workbook.add_format({
                'bold': True,
                'text_wrap': True,
                'valign': 'top',
                'align': 'center',
                'border': 1,
                'bg_color': '#D9E1F2'
            })

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —à–∏—Ä–∏–Ω—É —Å—Ç–æ–ª–±—Ü–æ–≤
            worksheet.set_column('A:A', 15)  # UID
            worksheet.set_column('B:B', 30)  # –ù–∞–∑–≤–∞–Ω–∏–µ
            worksheet.set_column('C:C', 15)  # –¢–µ–ª–µ—Ñ–æ–Ω
            worksheet.set_column('D:D', 15)  # –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è
            worksheet.set_column('E:E', 15)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥–∞–∂
            worksheet.set_column('F:F', 10)  # –†–µ–π—Ç–∏–Ω–≥
            worksheet.set_column('G:M', 12)  # –û—Ç–∑—ã–≤—ã –ø–æ –≥–æ–¥–∞–º

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            for col_num, value in enumerate(df.columns.values):
                worksheet.write(0, col_num, value, header_format)

        logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
        print(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {output_file}")
        
        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return False
    
if __name__ == "__main__":
    # –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    # get_json_category()
    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 3 —É—Ä–æ–≤–Ω—è
    # scrap_json_category()
    # main_th()
    # scrap_html()
    # main_th_id()
    # scrap_html_id()
    # main_comment()
    # stats = process_reviews_by_year()
    # if stats:
    #     output_file = "merchants_reviews_stats.json"
    #     save_stats_to_json(stats, output_file)
    #     print(f"Processed {len(stats)} merchants")
    #     # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π
    #     print("\nExample of first few records:")
    #     print(json.dumps(stats[:3], indent=2))
    # json_file = "merchants_reviews_stats.json"
    # import_reviews_from_json(json_file)
    # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    engine = create_db_connection()
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    export_to_excel(engine, 'merchants_reviews_export.xlsx')