import os
import re
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import urlparse

import requests
from loguru import logger

current_directory = Path.cwd()
sitemaps_directory = current_directory / "sitemaps"
urls_directory = current_directory / "urls"
log_directory = current_directory / "log"
log_directory.mkdir(parents=True, exist_ok=True)
urls_directory.mkdir(parents=True, exist_ok=True)
sitemaps_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
all_urls_file = urls_directory / "all_urls.txt"
legal_file = current_directory / "legal.csv"
fop_file = current_directory / "fop.csv"


logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
cookies = {
    "_csrf": "c1e6328d4d1a00430f580954cd699bfcb582e349d7cdb35b0fc25fc69f79504fa%3A2%3A%7Bi%3A0%3Bs%3A5%3A%22_csrf%22%3Bi%3A1%3Bs%3A32%3A%22sPIghgsE62pvjuIdspysobQGcw1EBt3j%22%3B%7D",
    "device-referrer": "https://edrpou.ubki.ua/ua/FO12726884",
    "LNG": "UA",
    "device-source": "https://edrpou.ubki.ua/ua/01056190",
}

headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "cache-control": "no-cache",
    "dnt": "1",
    "pragma": "no-cache",
    "priority": "u=0, i",
    "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "cross-site",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–∞
def download_xml(url, save_path=None, force_download=False):
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–π sitemap –∏ force_download=False
        if save_path and Path(save_path).exists() and not force_download:
            logger.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ: {save_path}")
            with open(save_path, "rb") as f:
                return f.read()

        logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {url}")
        response = requests.get(url, cookies=cookies, headers=headers, timeout=30)
        response.raise_for_status()

        if save_path:
            with open(save_path, "wb") as f:
                f.write(response.content)
            logger.info(f"XML —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")

        return response.content
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}")
        return None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ sitemap.xml –∏ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ—á–µ—Ä–Ω–∏–µ sitemaps
def parse_sitemap_index(xml_content):
    sitemap_urls = []
    try:
        root = ET.fromstring(xml_content)
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω
        namespace = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <sitemap>
        for sitemap in root.findall(".//sm:sitemap", namespace):
            loc = sitemap.find("sm:loc", namespace)
            if loc is not None and loc.text:
                sitemap_urls.append(loc.text)

        return sitemap_urls
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ sitemap index: {e}")
        return []


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ sitemap —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL —Å—Ç—Ä–∞–Ω–∏—Ü
def parse_sitemap(xml_content):
    page_urls = []
    try:
        root = ET.fromstring(xml_content)
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω
        namespace = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <url>
        for url in root.findall(".//sm:url", namespace):
            loc = url.find("sm:loc", namespace)
            if loc is not None and loc.text:
                page_urls.append(loc.text)

        return page_urls
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ sitemap: {e}")
        return []


def split_urls_by_type():
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è URL-–∞–¥—Ä–µ—Å–æ–≤ –∏–∑ all_urls.txt –Ω–∞ –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞:
    - legal.csv –¥–ª—è URL –≤–∏–¥–∞ https://edrpou.ubki.ua/ua/24391608
    - fop.csv –¥–ª—è URL –≤–∏–¥–∞ https://edrpou.ubki.ua/ua/FO1171076

    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ URL —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º /ua/, –∏–≥–Ω–æ—Ä–∏—Ä—É—è URL —Å /en/
    """
    all_urls_file = urls_directory / "all_urls.txt"
    legal_file = current_directory / "legal.csv"
    fop_file = current_directory / "fop.csv"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
    if not all_urls_file.exists():
        logger.error(f"–§–∞–π–ª {all_urls_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    legal_count = 0
    fop_count = 0
    other_count = 0
    filtered_out_count = 0

    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–æ–≤ URL
    # –¢–æ–ª—å–∫–æ —É–∫—Ä–∞–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è (ua) –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü
    legal_pattern = re.compile(r"https://edrpou\.ubki\.ua/ua/\d+$")
    # –¢–æ–ª—å–∫–æ —É–∫—Ä–∞–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è (ua) –¥–ª—è –§–û–ü
    fop_pattern = re.compile(r"https://edrpou\.ubki\.ua/ua/FO\d+$")

    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –≤–µ—Ä—Å–∏–π (–¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
    en_pattern = re.compile(r"https://edrpou\.ubki\.ua/en/")

    # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–ø–∏—Å–∏
    with open(legal_file, "w", encoding="utf-8") as legal_out, open(
        fop_file, "w", encoding="utf-8"
    ) as fop_out, open(all_urls_file, "r", encoding="utf-8") as urls_in:

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ CSV
        legal_out.write("url\n")
        fop_out.write("url\n")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π URL
        for line in urls_in:
            url = line.strip()
            if not url:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≤–µ—Ä—Å–∏–µ–π
            if en_pattern.match(url):
                filtered_out_count += 1
                continue

            if legal_pattern.match(url):
                # –≠—Ç–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ (—É–∫—Ä–∞–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è)
                legal_out.write(f"{url}\n")
                legal_count += 1

            elif fop_pattern.match(url):
                # –≠—Ç–æ –§–û–ü (—É–∫—Ä–∞–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è)
                fop_out.write(f"{url}\n")
                fop_count += 1

            else:
                # –î—Ä—É–≥–æ–π —Ç–∏–ø URL
                other_count += 1

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    logger.info(f"–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ª–∏—Ü–∞ (legal.csv): {legal_count}")
    logger.info(f"–§–û–ü (fop.csv): {fop_count}")
    logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –≤–µ—Ä—Å–∏–π: {filtered_out_count}")
    logger.info(f"–î—Ä—É–≥–∏–µ URL: {other_count}")
    logger.info(
        f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {legal_count + fop_count + other_count + filtered_out_count}"
    )

    return {
        "legal_count": legal_count,
        "fop_count": fop_count,
        "filtered_en_count": filtered_out_count,
        "other_count": other_count,
        "total": legal_count + fop_count + other_count + filtered_out_count,
    }


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    sitemap_url = "https://edrpou.ubki.ua/sitemap.xml"

    # –°–∫–∞—á–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π sitemap.xml (–≤—Å–µ–≥–¥–∞ –æ–±–Ω–æ–≤–ª—è–µ–º)
    logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–≥–æ sitemap: {sitemap_url}")
    main_sitemap = sitemaps_directory / "main_sitemap.xml"

    # –î–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ sitemap –≤—Å–µ–≥–¥–∞ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å (force_download=True)
    main_sitemap_content = download_xml(sitemap_url, main_sitemap, force_download=True)

    if not main_sitemap_content:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π sitemap.xml. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
        return

    # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π sitemap.xml –∏ –ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ—á–µ—Ä–Ω–∏–µ sitemaps
    sitemap_urls = parse_sitemap_index(main_sitemap_content)
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(sitemap_urls)} –¥–æ—á–µ—Ä–Ω–∏—Ö sitemap-—Ñ–∞–π–ª–æ–≤")

    child_sitemaps = sitemaps_directory / "child_sitemaps.txt"
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ—á–µ—Ä–Ω–∏–µ sitemaps
    with open(child_sitemaps, "w", encoding="utf-8") as f:
        for url in sitemap_urls:
            f.write(f"{url}\n")

    # –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ –¥–æ—á–µ—Ä–Ω–∏–µ sitemaps –∏ –ø–∞—Ä—Å–∏–º URL —Å—Ç—Ä–∞–Ω–∏—Ü
    all_page_urls = []

    for i, sitemap_url in enumerate(sitemap_urls):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        filename = os.path.basename(urlparse(sitemap_url).path)
        save_path = sitemaps_directory / filename
        output_file = urls_directory / f"urls_from_{filename}.txt"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª —Å URL-–∞–º–∏
        if output_file.exists():
            logger.info(
                f"–§–∞–π–ª —Å URL-–∞–º–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É: {output_file}"
            )

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ URL –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            with open(output_file, "r", encoding="utf-8") as f:
                existing_urls = [line.strip() for line in f if line.strip()]
                all_page_urls.extend(existing_urls)
                logger.info(
                    f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_urls)} URL —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞"
                )

            continue

        logger.info(
            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ—á–µ—Ä–Ω–µ–≥–æ sitemap [{i+1}/{len(sitemap_urls)}]: {sitemap_url}"
        )

        # –°–∫–∞—á–∏–≤–∞–µ–º sitemap —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç (force_download=False)
        sitemap_content = download_xml(sitemap_url, save_path, force_download=False)

        if sitemap_content:
            # –ü–∞—Ä—Å–∏–º –¥–æ—á–µ—Ä–Ω–∏–π sitemap –∏ –ø–æ–ª—É—á–∞–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü
            page_urls = parse_sitemap(sitemap_content)
            logger.info(f"  - –ù–∞–π–¥–µ–Ω–æ {len(page_urls)} URL —Å—Ç—Ä–∞–Ω–∏—Ü")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
            with open(output_file, "w", encoding="utf-8") as f:
                for url in page_urls:
                    f.write(f"{url}\n")

            all_page_urls.extend(page_urls)

    all_url = urls_directory / "all_urls.txt"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–æ—Å—å –ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL
    should_update_all_urls = True
    if all_url.exists():
        with open(all_url, "r", encoding="utf-8") as f:
            existing_count = sum(1 for _ in f)

        if existing_count == len(all_page_urls):
            logger.info(
                f"–û–±—â–∏–π —Ñ–∞–π–ª —Å URL –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è ({existing_count} URLs), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ"
            )
            should_update_all_urls = False

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ–±—â–∏–π —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if should_update_all_urls:
        with open(all_url, "w", encoding="utf-8") as f:
            for url in all_page_urls:
                f.write(f"{url}\n")
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω –æ–±—â–∏–π —Ñ–∞–π–ª —Å URL: {all_url}")

    logger.info(f"\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_page_urls)} URL —Å—Ç—Ä–∞–Ω–∏—Ü")
    logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö 'sitemaps' –∏ 'urls'")
    split_urls_by_type()


if __name__ == "__main__":
    main()
