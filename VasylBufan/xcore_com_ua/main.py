import json
import subprocess
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import urlparse

import pandas as pd
import requests
from config.logger import logger

current_directory = Path.cwd()
data_directory = current_directory / "data"
xml_directory = current_directory / "xml"
log_directory = current_directory / "log"
confi_directory = current_directory / "config"

data_directory.mkdir(parents=True, exist_ok=True)
confi_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
urls_xml_file_path = data_directory / "urls_xml.csv"
urls_product_file_path = data_directory / "urls.csv"
config_file_path = confi_directory / "config.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def load_json_data(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}: {e}")
        return None


def save_json_data(data, file_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª {file_path}: {e}")
        return False


config = load_json_data(config_file_path)
URL_XML = config.get("url_xml")
COOKIES = config.get("cookies", {})
HEADERS = config.get("headers", {})
FILENAME_XML = urlparse(URL_XML).path.split("/")[-1]
XML_FILE_PATH = xml_directory / f"{FILENAME_XML}"


def download_with_curl(url, xml_file_path):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ URL –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏—Å—Ç–µ–º–Ω—É—é –∫–æ–º–∞–Ω–¥—É curl

    Returns:
        str or None: –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å {url}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É curl
        command = [
            "curl",
            "-o",
            xml_file_path,  # –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            "-L",  # –°–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
            "-A",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",  # User-Agent
            "--max-redirs",
            "10",  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
            "--connect-timeout",
            "30",  # –¢–∞–π–º-–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            url,
        ]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É
        result = subprocess.run(command, capture_output=True, text=True, check=False)

        if result.returncode == 0:
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {xml_file_path}")
            return xml_file_path
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {result.stderr}")
            return None
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        return None


def download_all_xml():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª(—ã) –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL –∏–ª–∏ —Å–ø–∏—Å–∫—É URL.

    Args:
        urls (str –∏–ª–∏ list): URL –∏–ª–∏ —Å–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–æ–≤
        cookies (dict): Cookies –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞
        headers (dict): –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        list –∏–ª–∏ Path –∏–ª–∏ None: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º, –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    # –ß—Ç–µ–Ω–∏–µ CSV-—Ñ–∞–π–ª–∞ —Å URL
    urls_df = pd.read_csv(urls_xml_file_path, encoding="utf-8")
    urls = urls_df["url"].tolist()
    for url in urls:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
            file_name = urlparse(url).path.split("/")[-1]
            xml_file_path = xml_directory / file_name
            logger.info(f"–°–∫–∞—á–∏–≤–∞–µ–º XML —Ñ–∞–π–ª: {url}")

            download_with_curl(url, xml_file_path)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url}: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º URL


def parse_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("jetsitemap-products-page-1*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <url> –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º—ë–Ω
            for url_elem in root.findall(
                ".//{http://www.sitemaps.org/schemas/sitemap/0.9}url"
            ):
                loc = url_elem.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
                if loc is not None:
                    urls.append(loc.text)

        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML {xml_file}: {e}")
            return []
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    if urls:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å URL
        url_data = pd.DataFrame(urls, columns=["url"])
        url_data.to_csv(urls_product_file_path, index=False)
        logger.info(f"URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {urls_product_file_path}")


def parsin_xml(file_name):
    with open(file_name, "r", encoding="utf-8") as file:
        xml_content = file.read()

    # –ü–∞—Ä—Å–∏–º XML
    root = ET.fromstring(xml_content)

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <loc>
    product_sitemaps = []
    for sitemap in root.findall(
        ".//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap"
    ):
        loc = sitemap.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
        if loc is not None and "jetsitemap-products-page-" in loc.text:
            product_sitemaps.append(loc.text)
    url_data = pd.DataFrame(product_sitemaps, columns=["url"])
    url_data.to_csv(urls_xml_file_path, index=False)


def xml_temp():

    # –ó–∞–≥—Ä—É–∑–∫–∞ XML-—Ñ–∞–π–ª–∞
    xml_file = "index.xml"  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É XML-—Ñ–∞–π–ª—É

    tree = ET.parse(xml_file)
    root = tree.getroot()

    # –ù–∞–π—Ç–∏ —Å–µ–∫—Ü–∏—é offers
    offers_section = root.find(".//offers")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ offers_section –Ω–∞–π–¥–µ–Ω
    if offers_section is not None:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL-–∞–¥—Ä–µ—Å–∞
        urls = [
            offer.find("url").text
            for offer in offers_section.findall("offer")
            if offer.find("url") is not None
        ]

        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(urls, columns=["url"])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV-—Ñ–∞–π–ª
        csv_filename = "urls.csv"
        df.to_csv(csv_filename, index=False)

        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {csv_filename}")
    else:
        print("–û—à–∏–±–∫–∞: –°–µ–∫—Ü–∏—è <offers> –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ XML.")


if __name__ == "__main__":
    # download_xml(URL_XML, FILENAME_XML, COOKIES, HEADERS)
    # parsin_xml(XML_FILE_PATH)
    # download_all_xml()
    parse_sitemap_urls()

    # # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤
    # logger.info("=== –ò—Ç–æ–≥–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ===")
    # for url, file_path in results.items():
    #     status = "–£–°–ü–ï–®–ù–û" if file_path else "–û–®–ò–ë–ö–ê"
    # logger.info(f"{status}: {url}")
    # parse_si??temap_urls()
    # parsin_xml()
    # xml_temp()
