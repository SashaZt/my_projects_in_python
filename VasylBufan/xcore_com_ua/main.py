import csv
import hashlib
import json
import os
import re
import shutil
import subprocess
import sys
import time
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urlparse

import demjson3
import gspread
import pandas as pd
import requests
from bs4 import BeautifulSoup
from config.logger import logger
from google.oauth2.service_account import Credentials

current_directory = Path.cwd()
data_directory = current_directory / "data"
xml_directory = current_directory / "xml"
log_directory = current_directory / "log"
config_directory = current_directory / "config"
html_directory = current_directory / "html"
html_directory.mkdir(parents=True, exist_ok=True)

data_directory.mkdir(parents=True, exist_ok=True)
config_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
urls_xml_file_path = data_directory / "urls_xml.csv"
urls_product_file_path = data_directory / "urls.csv"
output_json_file = data_directory / "output.json"
config_file_path = config_directory / "config.json"
service_account_file = config_directory / "credentials.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def load_json_data(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}: {e}")
        return None


def save_json_data(data, file_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª {file_path}: {e}")
        return False


config = load_json_data(config_file_path)
URL_XML = config.get("url_xml")
COOKIES = config.get("cookies", {})
HEADERS = config.get("headers", {})
FILENAME_XML = urlparse(URL_XML).path.split("/")[-1]
XML_FILE_PATH = xml_directory / f"{FILENAME_XML}"
SPREADSHEET = config["google"]["spreadsheet"]
SHEET_ALL = config["google"]["sheet_all"]
SHEET_01 = config["google"]["sheet_01"]


def get_google_sheet(sheet_one):
    """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Google Sheets –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏—Å—Ç."""
    try:
        # –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å google-auth
        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]

        credentials = Credentials.from_service_account_file(
            service_account_file, scopes=scopes
        )

        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ gspread —Å –Ω–æ–≤—ã–º–∏ —É—á–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        client = gspread.authorize(credentials)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ –∫–ª—é—á—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–∏—Å—Ç
        spreadsheet = client.open_by_key(SPREADSHEET)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Spreadsheet.")
        return spreadsheet.worksheet(sheet_one)
    except FileNotFoundError:
        logger.error("–§–∞–π–ª —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
        raise FileNotFoundError("–§–∞–π–ª —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
    except gspread.exceptions.APIError as e:
        logger.error(f"–û—à–∏–±–∫–∞ API Google Sheets: {e}")
        raise
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        raise


def update_sheet_with_data(sheet, data, total_rows=10000):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ª–∏—Å—Ç–∞ Google Sheets —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    if not data:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–ª—é—á–µ–π —Å–ª–æ–≤–∞—Ä—è
    headers = list(data[0].keys())

    # –ó–∞–ø–∏—Å—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
    sheet.update(values=[headers], range_name="A1", value_input_option="RAW")

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
    rows = [[entry.get(header, "") for header in headers] for entry in data]

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –¥–æ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ total_rows
    if len(rows) < total_rows:
        empty_row = [""] * len(headers)
        rows.extend([empty_row] * (total_rows - len(rows)))

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö
    end_col = chr(65 + len(headers) - 1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –±—É–∫–≤—É (A, B, C...)
    range_name = f"A2:{end_col}{total_rows + 1}"

    # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç
    sheet.update(values=rows, range_name=range_name, value_input_option="USER_ENTERED")
    logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –≤ Google Sheets")


def download_with_curl(url, xml_file_path):
    if os.path.exists(html_directory):
        shutil.rmtree(html_directory)
    html_directory.mkdir(parents=True, exist_ok=True)
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ URL –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏—Å—Ç–µ–º–Ω—É—é –∫–æ–º–∞–Ω–¥—É curl

    Returns:
        str or None: –ü—É—Ç—å –∫ —Å–∫–∞—á–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å {url}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É curl
        command = [
            "curl",
            "-o",
            xml_file_path,  # –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            "-L",  # –°–ª–µ–¥–æ–≤–∞—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
            "-A",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",  # User-Agent
            "--max-redirs",
            "10",  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
            "--connect-timeout",
            "30",  # –¢–∞–π–º-–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            url,
        ]

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É
        result = subprocess.run(command, capture_output=True, text=True, check=False)

        if result.returncode == 0:
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {xml_file_path}")
            return xml_file_path
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {result.stderr}")
            return None
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        return None


def download_all_xml():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª(—ã) –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL –∏–ª–∏ —Å–ø–∏—Å–∫—É URL.

    Args:
        urls (str –∏–ª–∏ list): URL –∏–ª–∏ —Å–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–æ–≤
        cookies (dict): Cookies –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞
        headers (dict): –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞

    Returns:
        list –∏–ª–∏ Path –∏–ª–∏ None: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º, –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    # –ß—Ç–µ–Ω–∏–µ CSV-—Ñ–∞–π–ª–∞ —Å URL
    urls_df = pd.read_csv(urls_xml_file_path, encoding="utf-8")
    urls = urls_df["url"].tolist()
    for url in urls:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
            file_name = urlparse(url).path.split("/")[-1]
            xml_file_path = xml_directory / file_name

            download_with_curl(url, xml_file_path)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url}: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–∏–º URL


def parse_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("jetsitemap-products-page-1*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <url> –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º—ë–Ω
            for url_elem in root.findall(
                ".//{http://www.sitemaps.org/schemas/sitemap/0.9}url"
            ):
                loc = url_elem.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
                if loc is not None:
                    urls.append(loc.text)

        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML {xml_file}: {e}")
            return []
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    if urls:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å URL
        url_data = pd.DataFrame(urls, columns=["url"])
        url_data.to_csv(urls_product_file_path, index=False)
        logger.info(f"URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {urls_product_file_path}")


def parsin_xml(file_name):
    with open(file_name, "r", encoding="utf-8") as file:
        xml_content = file.read()

    # –ü–∞—Ä—Å–∏–º XML
    root = ET.fromstring(xml_content)

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã <loc>
    product_sitemaps = []
    for sitemap in root.findall(
        ".//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap"
    ):
        loc = sitemap.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
        if loc is not None and "jetsitemap-products-page-" in loc.text:
            product_sitemaps.append(loc.text)
    url_data = pd.DataFrame(product_sitemaps, columns=["url"])
    url_data.to_csv(urls_xml_file_path, index=False)
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–∏–ª {urls_xml_file_path}")


def fetch(url):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ URL —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.

    Args:
        url (str): URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

    Returns:
        str or None: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –Ω–µ—É–¥–∞—á–∏
    """
    max_attempts = 10
    delay_seconds = 5

    for attempt in range(max_attempts):
        try:
            response = requests.get(
                url, cookies=COOKIES, headers=HEADERS, timeout=100, stream=True
            )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞
            if response.status_code == 200:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8
                response.encoding = "utf-8"
                return response.text
            else:
                logger.warning(
                    f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}: –°—Ç–∞—Ç—É—Å {response.status_code} –¥–ª—è {url}. –ñ–¥—ë–º {delay_seconds} —Å–µ–∫—É–Ω–¥."
                )
                if attempt < max_attempts - 1:  # –ù–µ –∂–¥—ë–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–∏
                    time.sleep(delay_seconds)

        except requests.exceptions.RequestException as e:
            logger.error(
                f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}"
            )
            if attempt < max_attempts - 1:
                time.sleep(delay_seconds)
            else:
                logger.error(f"–í—Å–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫ –Ω–µ—É–¥–∞—á–Ω—ã –¥–ª—è {url}")
                return None

    logger.error(f"–í—Å–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫ –Ω–µ—É–¥–∞—á–Ω—ã –¥–ª—è {url} (—Å—Ç–∞—Ç—É—Å –Ω–µ 200)")
    return None


def get_html(url, html_file):
    src = fetch(url)

    if src is None:
        return url, html_file, False

    with open(html_file, "w", encoding="utf-8") as file:
        file.write(src)

    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_file}")
    return url, html_file, True


def main_th():
    if not os.path.exists(html_directory):
        html_directory.mkdir(parents=True, exist_ok=True)
    urls = []
    with open(urls_product_file_path, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            urls.append(row["url"])

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for url in urls:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html, url, output_html_file))
            else:
                logger.info(f"–§–∞–π–ª –¥–ª—è {url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def extract_product_data(product_json):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

    Args:
        product_json (dict): JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–¥—É–∫—Ç–∞

    Returns:
        dict: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç–∞
    """
    try:
        data_json_all = {}
        data_json_01 = {}
        product_name = product_json.get("name")
        model = product_json.get("model")
        sku = product_json.get("sku")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ offers
        offers = product_json.get("offers", {})
        offer_price = None
        if "price" in offers:
            offer_price = offers.get("price")
        elif "lowPrice" in offers:
            offer_price = offers.get("lowPrice")
        offer_price = str(offer_price).replace(".", ",")
        availability = offers.get("availability")
        schema_terms = (
            r"(InStock|PreOrder|OutOfStock|Discontinued)"  # –®–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
        )
        all_availability = {
            "PreOrder": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è",
            "InStock": "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ",
            "OutOfStock": "–ù–µ–º–∞—î –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ",
            "Discontinued": "–ü—Ä–∏–ø–∏–Ω–µ–Ω–æ",
        }

        matches = re.findall(schema_terms, availability or "")  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ None
        result_availability = None
        if matches:
            last_term = matches[-1]
            result_availability = all_availability[last_term]
        if model.endswith("~01"):
            data_json_01 = {
                "–ù–∞–∑–≤–∞": product_name,
                "–ö–æ–¥ —Ç–æ–≤–∞—Ä—É": model,
                "–ê—Ä—Ç–∏–∫—É": sku,
                "–¶—ñ–Ω–∞": offer_price,
                "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": result_availability,
            }
        else:
            data_json_all = {
                "–ù–∞–∑–≤–∞": product_name,
                "–ö–æ–¥ —Ç–æ–≤–∞—Ä—É": model,
                "–ê—Ä—Ç–∏–∫—É": sku,
                "–¶—ñ–Ω–∞": offer_price,
                "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": result_availability,
            }

        return data_json_all, data_json_01
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞: {e}")
        return None


def pars_htmls():
    logger.info("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü html")
    list_all = []
    list_01 = []
    all_product = []
    html_count = len(list(html_directory.glob("*.html")))
    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        soup = BeautifulSoup(content, "lxml")
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–∫—Ä–∏–ø—Ç—ã —Å —Ç–∏–ø–æ–º application/ld+json
        scripts = soup.find_all("script", type="application/ld+json")

        if not scripts:
            logger.warning(
                f"–í —Ñ–∞–π–ª–µ {html_file.name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–∫—Ä–∏–ø—Ç–æ–≤ —Å —Ç–∏–ø–æ–º application/ld+json"
            )
            continue

        # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∫—Ä–∏–ø—Ç—ã JSON-LD
        for script in scripts:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ–≥–æ –Ω–∞–ª–∏—á–∏–µ
                script_text = script.string
                if not script_text or not script_text.strip():
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —ç—Ç–æ —Å–∫—Ä–∏–ø—Ç JSON-LD
                if "application/ld+json" not in script.get("type", ""):
                    continue

                # –£–¥–∞–ª—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                cleaned_text = script_text.strip()

                # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                cleaned_text = re.sub(r"[\x00-\x1F\x7F]", "", cleaned_text)

                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ JSON –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω
                opening_braces = cleaned_text.count("{")
                closing_braces = cleaned_text.count("}")

                if opening_braces > closing_braces:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏
                    cleaned_text += "}" * (opening_braces - closing_braces)
                    logger.info(
                        f"–î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏: {opening_braces - closing_braces}"
                    )

                try:
                    json_data = json.loads(cleaned_text)
                except json.JSONDecodeError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                    # 1. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –æ—Ç—Å—Ç—É–ø–æ–≤ –¥–ª—è –∫–ª—é—á–µ–π gtin –∏ sku
                    cleaned_text = re.sub(r'(\s+)"(gtin|sku)"', r'"$2"', cleaned_text)

                    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –≤ –∫–æ–Ω—Ü–µ
                    if not cleaned_text.rstrip().endswith("}"):
                        cleaned_text = cleaned_text.rstrip() + "}"

                    try:
                        json_data = json.loads(cleaned_text)
                        logger.info("JSON —É—Å–ø–µ—à–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω")
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞

                    except json.JSONDecodeError as e2:
                        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä
                        try:

                            json_data = demjson3.decode(cleaned_text)
                            logger.info("JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω —Å –ø–æ–º–æ—â—å—é demjson3")
                        except Exception:
                            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å JSON: {e2}")
                            continue

                if json_data.get("@type") == "BreadcrumbList":
                    continue
                if json_data.get("@type") == "Product":
                    sklad_all, sklad_01 = extract_product_data(json_data)
                    if sklad_all:
                        list_all.append(sklad_all)
                        all_product.append(sklad_all)
                        html_count -= 1
                        print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {html_count} —Ñ–∞–π–ª–æ–≤", end="\r")
                    if sklad_01:
                        list_01.append(sklad_01)
                        all_product.append(sklad_01)
                        html_count -= 1
                        print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {html_count} —Ñ–∞–π–ª–æ–≤", end="\r")

            except Exception as e:
                logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∫—Ä–∏–ø—Ç–∞: {str(e)}")

    with open(output_json_file, "w", encoding="utf-8") as json_file:
        json.dump(all_product, json_file, ensure_ascii=False, indent=4)
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–∏—Å—Ç–∞ Google Sheets
    if list_all:
        sheet = get_google_sheet(SHEET_ALL)
        update_sheet_with_data(sheet, list_all)
    if list_01:
        sheet = get_google_sheet(SHEET_01)
        update_sheet_with_data(sheet, list_01)


## –ö–æ–¥ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
# def ensure_row_limit(sheet, required_rows=10000):
#     """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –ª–∏—Å—Ç–µ Google Sheets, –µ—Å–ª–∏ –∏—Ö –º–µ–Ω—å—à–µ —Ç—Ä–µ–±—É–µ–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞."""
#     current_rows = len(sheet.get_all_values())
#     if current_rows < required_rows:
#         sheet.add_rows(required_rows - current_rows)


# ensure_row_limit(sheet, 1000)

if __name__ == "__main__":
    download_with_curl(URL_XML, XML_FILE_PATH)
    parsin_xml(XML_FILE_PATH)
    download_all_xml()
    parse_sitemap_urls()
    main_th()
    pars_htmls()
