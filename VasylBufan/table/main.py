import json
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import urlparse

import gspread
import pandas as pd
import requests
from google.oauth2.service_account import Credentials
from loguru import logger
from lxml import etree

current_directory = Path.cwd()
xml_directory = current_directory / "xml"
log_directory = current_directory / "log"
config_directory = current_directory / "config"

config_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
config_file_path = config_directory / "config.json"
service_account_file = config_directory / "credentials.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def load_json_data(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}: {e}")
        return None


def save_json_data(data, file_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª {file_path}: {e}")
        return False


config = load_json_data(config_file_path)
URLS = config.get("competitor_www", [])
MY_URL = config.get("my_www")
HEADERS = config.get("headers", {})
SPREADSHEET = config["google"]["spreadsheet"]
SHEET = config["google"]["sheet"]


def get_google_sheet(sheet_one):
    """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Google Sheets –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏—Å—Ç."""
    try:
        # –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å google-auth
        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive",
        ]

        credentials = Credentials.from_service_account_file(
            service_account_file, scopes=scopes
        )

        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ gspread —Å –Ω–æ–≤—ã–º–∏ —É—á–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        client = gspread.authorize(credentials)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ –∫–ª—é—á—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–∏—Å—Ç
        spreadsheet = client.open_by_key(SPREADSHEET)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Spreadsheet.")
        return spreadsheet.worksheet(sheet_one)
    except FileNotFoundError:
        logger.error("–§–∞–π–ª —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
        raise FileNotFoundError("–§–∞–π–ª —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
    except gspread.exceptions.APIError as e:
        logger.error(f"–û—à–∏–±–∫–∞ API Google Sheets: {e}")
        raise
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        raise


def update_sheet_with_data(sheet, data, total_rows=20000):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ª–∏—Å—Ç–∞ Google Sheets —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    if not data:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–ª—é—á–µ–π —Å–ª–æ–≤–∞—Ä—è
    headers = list(data[0].keys())

    # –ó–∞–ø–∏—Å—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
    sheet.update(values=[headers], range_name="A1", value_input_option="RAW")

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
    rows = [[entry.get(header, "") for header in headers] for entry in data]

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –¥–æ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ total_rows
    if len(rows) < total_rows:
        empty_row = [""] * len(headers)
        rows.extend([empty_row] * (total_rows - len(rows)))

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö
    end_col = chr(65 + len(headers) - 1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –±—É–∫–≤—É (A, B, C...)
    range_name = f"A2:{end_col}{total_rows + 1}"

    # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç
    sheet.update(values=rows, range_name=range_name, value_input_option="USER_ENTERED")
    logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –≤ Google Sheets")


def download_xml(url, headers):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL.

    Args:
        url (str): URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–∞
        headers (dict): –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞
        xml_dir (Path, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é xml_directory.

    Returns:
        Path or None: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL
        filename = urlparse(url).path.split("/")[-1]

        # –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–º–µ–Ω
        if not filename:
            filename = urlparse(url).netloc.replace(".", "_")

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .xml –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if not filename.endswith(".xml"):
            xml_file_path = xml_directory / f"{filename}.xml"
        else:
            xml_file_path = xml_directory / filename

        logger.info(f"–°–∫–∞—á–∏–≤–∞–µ–º XML —Ñ–∞–π–ª: {url}")

        response = requests.get(
            url,
            headers=headers,
            timeout=30,
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
            with open(xml_file_path, "wb") as file:
                file.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {xml_file_path}")
            return xml_file_path
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")
            return None
    except Exception as e:
        logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ {url}: {e}")
        return None


def download_all_xml_files():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ XML —Ñ–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Args:
        config_file_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url: –ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É_–∏–ª–∏_None}
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    results = {}

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    for url in URLS:
        results[url] = download_xml(url, HEADERS)

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if MY_URL:
        results[MY_URL] = download_xml(MY_URL, HEADERS)

    return results


def parse_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
            for url in root.findall(".//sitemap:url", namespace):
                loc = url.find("sitemap:loc", namespace)

                if loc is not None and loc.text:
                    urls.append(loc.text)

            # return urls

        except ET.ParseError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
            return []
        except FileNotFoundError:
            print(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")


def parsin_xml():
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ sku
    data_dict = {}
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ ean –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    ean_dict = {}
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è SKU –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ INS
    normalized_sku_dict = {}

    # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
    matched_data = []  # –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
    unmatched_data = []  # –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã

    # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª "all", —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å sku –∏ ean
    for xml_file in xml_directory.glob("*.xml"):
        name_file = xml_file.stem

        if name_file == "all":
            tree = etree.parse(xml_file)
            root = tree.getroot()
            offers = root.xpath("//offer")

            for offer in offers:
                price_my_site = extract_xml_value(offer, "price")
                sku = (
                    offer.xpath('param[@name="sku"]/text()')[0]
                    if offer.xpath('param[@name="sku"]')
                    else None
                )

                ean = (
                    offer.xpath('param[@name="ean"]/text()')[0]
                    if offer.xpath('param[@name="ean"]')
                    else None
                )

                if sku:  # –ò—Å–ø–æ–ª—å–∑—É–µ–º sku –∫–∞–∫ –∫–ª—é—á
                    data_dict[sku] = {
                        "–ú–æ–π —Å–∞–π—Ç sku": sku,
                        "–ú–æ–π —Å–∞–π—Ç ean": ean,
                        "–ú–æ–π —Å–∞–π—Ç —Ü–µ–Ω–∞": price_my_site,
                        "insportline": None,
                        "insportline —Ü–µ–Ω–∞": None,
                        "matched": False,  # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
                    }

                    # –ï—Å–ª–∏ –µ—Å—Ç—å EAN, —Å–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∑–∞–ø–∏—Å—å –≤ data_dict
                    if ean:
                        ean_dict[ean] = sku

                    # –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é SKU (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ INS)
                    normalized_sku = normalize_sku(sku)
                    normalized_sku_dict[normalized_sku] = sku

    # –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è insportline (vendorCode)
    for xml_file in xml_directory.glob("*.xml"):
        name_file = xml_file.stem

        if name_file in ["export_yandex_market", "yml_dualprice"]:
            tree = etree.parse(xml_file)
            root = tree.getroot()
            offers = root.xpath("//offer")

            for offer in offers:
                vendor_code = extract_xml_value(offer, "vendorCode")
                insportline_price = extract_xml_value(offer, "price")

                if not vendor_code:
                    continue

                match_found = False

                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –ø–æ sku
                if vendor_code in data_dict:
                    # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é SKU
                    data_dict[vendor_code]["insportline vendor_code"] = vendor_code
                    data_dict[vendor_code]["insportline —Ü–µ–Ω–∞"] = insportline_price
                    data_dict[vendor_code]["matched"] = True
                    match_found = True
                else:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º vendor_code –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    normalized_vendor = normalize_sku(vendor_code)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É SKU
                    if normalized_vendor in normalized_sku_dict:
                        original_sku = normalized_sku_dict[normalized_vendor]
                        data_dict[original_sku]["insportline vendor_code"] = vendor_code
                        data_dict[original_sku]["insportline —Ü–µ–Ω–∞"] = insportline_price
                        data_dict[original_sku]["matched"] = True
                        match_found = True
                    else:
                        # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ EAN
                        ean_value = extract_xml_value(
                            offer, "barcode"
                        ) or extract_xml_value(offer, "ean")

                        if ean_value and ean_value in ean_dict:
                            # –ù–∞—à–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ EAN
                            matching_sku = ean_dict[ean_value]
                            data_dict[matching_sku][
                                "insportline vendor_code"
                            ] = vendor_code
                            data_dict[matching_sku][
                                "insportline —Ü–µ–Ω–∞"
                            ] = insportline_price
                            data_dict[matching_sku]["matched"] = True
                            match_found = True

                if not match_found:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∏ –ø–æ SKU, –Ω–∏ –ø–æ EAN, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                    new_key = f"insportline_{vendor_code}"
                    data_dict[new_key] = {
                        "–ú–æ–π —Å–∞–π—Ç sku": None,
                        "–ú–æ–π —Å–∞–π—Ç ean": None,
                        "–ú–æ–π —Å–∞–π—Ç —Ü–µ–Ω–∞": None,
                        "insportline vendor_code": vendor_code,
                        "insportline —Ü–µ–Ω–∞": insportline_price,
                        "matched": False,  # –≠—Ç–æ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å
                    }

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∏ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ
    for key, value in data_dict.items():
        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ø–æ–ª–µ matched, –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ result
        matched = value.pop("matched", False)

        if matched:
            matched_data.append(value)
        else:
            unmatched_data.append(value)

    # –í—ã–≤–æ–¥–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∏ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    print(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(matched_data)}")
    print(f"–ù–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(unmatched_data)}")

    # –°–æ–µ–¥–∏–Ω—è–µ–º —Å–Ω–∞—á–∞–ª–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ, –∑–∞—Ç–µ–º –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
    result = matched_data + unmatched_data

    # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –≤—ã–≤–µ–¥–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    print("\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:")
    for i, item in enumerate(matched_data[:5]):  # –ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        print(
            f"{i+1}. SKU: {item['–ú–æ–π —Å–∞–π—Ç sku']}, EAN: {item['–ú–æ–π —Å–∞–π—Ç ean']}, "
            + f"–ú–æ–π —Å–∞–π—Ç —Ü–µ–Ω–∞: {item['–ú–æ–π —Å–∞–π—Ç —Ü–µ–Ω–∞']}, "
            + f"Insportline: {item['insportline']}, Insportline —Ü–µ–Ω–∞: {item['insportline —Ü–µ–Ω–∞']}"
        )

    # –ü–æ–ª—É—á–∞–µ–º –ª–∏—Å—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    sheet_name = "Data"
    sheet = get_google_sheet(sheet_name)
    update_sheet_with_data(sheet, result)

    return result  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    # # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
    # result = list(data_dict.values())

    # # –ü–æ–ª—É—á–∞–µ–º –ª–∏—Å—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    # sheet_name = "Data"
    # sheet = get_google_sheet(sheet_name)
    # update_sheet_with_data(sheet, result)


def normalize_sku(sku):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç SKU, —É–¥–∞–ª—è—è –ø—Ä–µ—Ñ–∏–∫—Å 'INS' –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å.
    –ù–∞–ø—Ä–∏–º–µ—Ä: 'INS9410-3' -> '9410-3'
    """
    if sku and isinstance(sku, str):
        if sku.startswith("INS"):
            return sku[3:]  # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–∏–º–≤–æ–ª–∞ (INS)
    return sku


def extract_xml_value(element, tag_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–≥–∞ –∏–∑ XML —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'N/A', –µ—Å–ª–∏ —Ç–µ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω."""
    node = element.find(tag_name)
    return node.text if node is not None else None


if __name__ == "__main__":
    # download_all_xml_files()
    parsin_xml()
