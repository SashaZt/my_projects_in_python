import json
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from urllib.parse import urlparse

import pandas as pd
import requests
from loguru import logger

current_directory = Path.cwd()
xml_directory = current_directory / "xml"
log_directory = current_directory / "log"
confi_directory = current_directory / "config"

confi_directory.mkdir(parents=True, exist_ok=True)
log_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
log_file_path = log_directory / "log_message.log"
config_file_path = confi_directory / "config.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)


def load_json_data(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}: {e}")
        return None


def save_json_data(data, file_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª {file_path}: {e}")
        return False


config = load_json_data(config_file_path)
URLS = config.get("competitor_www", [])
MY_URL = config.get("my_www")
HEADERS = config.get("headers", {})


def download_xml(url, headers, xml_dir=xml_directory):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL.

    Args:
        url (str): URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è XML —Ñ–∞–π–ª–∞
        headers (dict): –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞
        xml_dir (Path, optional): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é xml_directory.

    Returns:
        Path or None: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL
        filename = urlparse(url).path.split("/")[-1]

        # –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–º–µ–Ω
        if not filename:
            filename = urlparse(url).netloc.replace(".", "_")

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .xml –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if not filename.endswith(".xml"):
            xml_file_path = xml_dir / f"{filename}.xml"
        else:
            xml_file_path = xml_dir / filename

        logger.info(f"–°–∫–∞—á–∏–≤–∞–µ–º XML —Ñ–∞–π–ª: {url}")

        response = requests.get(
            url,
            headers=headers,
            timeout=30,
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
            with open(xml_file_path, "wb") as file:
                file.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {xml_file_path}")
            return xml_file_path
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")
            return None
    except Exception as e:
        logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ {url}: {e}")
        return None


def download_all_xml_files(config_file_path):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ XML —Ñ–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

    Args:
        config_file_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url: –ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É_–∏–ª–∏_None}
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    results = {}

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    for url in URLS:
        results[url] = download_xml(url, HEADERS)

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if MY_URL:
        results[MY_URL] = download_xml(MY_URL, HEADERS)

    return results


def parse_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
            for url in root.findall(".//sitemap:url", namespace):
                loc = url.find("sitemap:loc", namespace)

                if loc is not None and loc.text:
                    urls.append(loc.text)

            # return urls

        except ET.ParseError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
            return []
        except FileNotFoundError:
            print(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")


def parsin_xml():
    with open("sitemap_0.xml", "r", encoding="utf-8") as file:
        xml_content = file.read()

    root = ET.fromstring(xml_content)
    namespace = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}

    urls = [
        url.text.strip()
        for url in root.findall(".//ns:loc", namespace)
        if not url.text.strip().startswith("https://bsspart.com/ru/")
    ]

    url_data = pd.DataFrame(urls, columns=["url"])
    url_data.to_csv("urls.csv", index=False)


def xml_temp():

    # –ó–∞–≥—Ä—É–∑–∫–∞ XML-—Ñ–∞–π–ª–∞
    xml_file = "index.xml"  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É XML-—Ñ–∞–π–ª—É

    tree = ET.parse(xml_file)
    root = tree.getroot()

    # –ù–∞–π—Ç–∏ —Å–µ–∫—Ü–∏—é offers
    offers_section = root.find(".//offers")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ offers_section –Ω–∞–π–¥–µ–Ω
    if offers_section is not None:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL-–∞–¥—Ä–µ—Å–∞
        urls = [
            offer.find("url").text
            for offer in offers_section.findall("offer")
            if offer.find("url") is not None
        ]

        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(urls, columns=["url"])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV-—Ñ–∞–π–ª
        csv_filename = "urls.csv"
        df.to_csv(csv_filename, index=False)

        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {csv_filename}")
    else:
        print("–û—à–∏–±–∫–∞: –°–µ–∫—Ü–∏—è <offers> –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ XML.")


if __name__ == "__main__":
    results = download_all_xml_files(config_file_path)

    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤
    logger.info("=== –ò—Ç–æ–≥–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ===")
    for url, file_path in results.items():
        status = "–£–°–ü–ï–®–ù–û" if file_path else "–û–®–ò–ë–ö–ê"
        logger.info(f"{status}: {url}")
    # parse_si??temap_urls()
    # parsin_xml()
    # xml_temp()
