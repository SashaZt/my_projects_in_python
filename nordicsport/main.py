import csv
import hashlib
import json
import os
import shutil
import sys
import time
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import gspread
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from loguru import logger
from oauth2client.service_account import ServiceAccountCredentials

env_path = os.path.join(os.getcwd(), "configuration", ".env")
load_dotenv(env_path)

SPREADSHEET = os.getenv("SPREADSHEET")
SHEET = os.getenv("SHEET")
time_a = os.getenv("TIME_A")
time_b = os.getenv("TIME_B")
BATCH_SIZE = os.getenv("BATCH_SIZE")
PAUSE_DURATION = os.getenv("PAUSE_DURATION")

current_directory = Path.cwd()
data_directory = current_directory / "data"
html_directory = current_directory / "html"
log_directory = current_directory / "log"
configuration_directory = current_directory / "configuration"
service_account_file = configuration_directory / "credentials.json"
log_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)

output_json_file = data_directory / "output.json"
output_csv_file = data_directory / "output.csv"
output_xml_file = data_directory / "output.xml"
log_file_path = log_directory / "log_message.log"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
cookies = {
    "secure_customer_sig": "",
    "localization": "UA",
    "cart_currency": "EUR",
    "_shopify_y": "E717B01C-0604-4E6E-b4a4-a2cf8e0db410",
    "_tracking_consent": "%7B%22con%22%3A%7B%22CMP%22%3A%7B%22a%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A%22%22%2C%22s%22%3A%22%22%7D%7D%2C%22v%22%3A%222.1%22%2C%22region%22%3A%22UA18%22%2C%22reg%22%3A%22%22%2C%22purposes%22%3A%7B%22a%22%3Atrue%2C%22p%22%3Atrue%2C%22m%22%3Atrue%2C%22t%22%3Atrue%7D%2C%22display_banner%22%3Afalse%2C%22sale_of_data_region%22%3Afalse%2C%22consent_id%22%3A%222B8B1ACA-52ad-4A74-a4ca-f851092e7cd7%22%7D",
    "_orig_referrer": "",
    "_landing_page": "%2F",
    "locale_bar_accepted": "1",
}

headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "cache-control": "no-cache",
    # 'cookie': 'secure_customer_sig=; localization=UA; cart_currency=EUR; _shopify_y=E717B01C-0604-4E6E-b4a4-a2cf8e0db410; _tracking_consent=%7B%22con%22%3A%7B%22CMP%22%3A%7B%22a%22%3A%22%22%2C%22m%22%3A%22%22%2C%22p%22%3A%22%22%2C%22s%22%3A%22%22%7D%7D%2C%22v%22%3A%222.1%22%2C%22region%22%3A%22UA18%22%2C%22reg%22%3A%22%22%2C%22purposes%22%3A%7B%22a%22%3Atrue%2C%22p%22%3Atrue%2C%22m%22%3Atrue%2C%22t%22%3Atrue%7D%2C%22display_banner%22%3Afalse%2C%22sale_of_data_region%22%3Afalse%2C%22consent_id%22%3A%222B8B1ACA-52ad-4A74-a4ca-f851092e7cd7%22%7D; _orig_referrer=; _landing_page=%2F; locale_bar_accepted=1',
    "dnt": "1",
    "pragma": "no-cache",
    "priority": "u=0, i",
    "sec-ch-ua": '"Not A(Brand";v="8", "Chromium";v="132", "Google Chrome";v="132"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "cross-site",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
}


def get_google_sheet():
    """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Google Sheets –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏—Å—Ç."""
    try:
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive.file",
            "https://www.googleapis.com/auth/drive",
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_name(
            service_account_file, scope
        )
        client = gspread.authorize(creds)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ –∫–ª—é—á—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–∏—Å—Ç
        spreadsheet = client.open_by_key(SPREADSHEET)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Spreadsheet.")
        return spreadsheet.worksheet(SHEET)
    except FileNotFoundError:
        raise FileNotFoundError("–§–∞–π–ª credentials.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
    except gspread.exceptions.APIError as e:
        logger.error(f"–û—à–∏–±–∫–∞ API Google Sheets: {e}")
        raise
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        raise


# –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–∏—Å—Ç–∞ Google Sheets
sheet = get_google_sheet()


def download_xml():
    if os.path.exists(html_directory):
        shutil.rmtree(html_directory)
    response = requests.get(
        "https://nordicsport.com/sitemap_products_1.xml?from=1447756726390&to=14706192122188",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
        with open(output_xml_file, "wb") as file:
            file.write(response.content)
        logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_xml_file}")
    else:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")


def parsin_xml():
    download_xml()

    # –ß—Ç–µ–Ω–∏–µ XML —Ñ–∞–π–ª–∞
    with open(output_xml_file, "r", encoding="utf-8") as file:
        xml_content = file.read()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º–µ–Ω
    namespaces = {
        "ns": "http://www.sitemaps.org/schemas/sitemap/0.9",
        "image": "http://www.google.com/schemas/sitemap-image/1.1",
    }

    # –ü–∞—Ä—Å–∏–º XML –∏–∑ —Å—Ç—Ä–æ–∫–∏
    root = ET.fromstring(xml_content)

    product_urls = []

    # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <loc> –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã
    for url in root.findall("ns:url", namespaces):
        loc = url.find("ns:loc", namespaces)
        if loc is not None:
            url_text = loc.text
            if "/products/" in url_text:
                product_urls.append(url_text)

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_urls = list(set(product_urls))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    url_data = pd.DataFrame(unique_urls, columns=["url"])
    url_data.to_csv(output_csv_file, index=False)

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã.")


def fetch(url):
    response = requests.get(url, cookies=cookies, headers=headers, timeout=30)
    response.raise_for_status()
    return response.text


def get_html(url, html_file):
    src = fetch(url)
    with open(html_file, "w", encoding="utf-8") as file:
        file.write(src)
    logger.info(html_file)
    # time.sleep(5)


def main_th():
    if not os.path.exists(html_directory):
        html_directory.mkdir(parents=True, exist_ok=True)
    urls = []
    with open(output_csv_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            urls.append(row["url"])

    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = []
        for url in urls:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html, url, output_html_file))
            else:
                logger.info(f"–§–∞–π–ª –¥–ª—è {url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def pars_htmls():
    logger.info("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü html")
    all_data = []

    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(content, "lxml")
        # –ü–æ–∏—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ —Å —Ç–∏–ø–æ–º application/ld+json –∏ —Ç–∏–ø–æ–º Product
        product_script = soup.find(
            "script",
            type="application/ld+json",
            string=lambda text: text and '"@type": "Product"' in text,
        )

        if product_script:
            try:
                product_data = json.loads(product_script.string)

                name = product_data.get("name")
                sku = product_data.get("sku")

                offers = product_data.get("offers", [])
                price = None

                if isinstance(offers, list) and offers:
                    raw_price = offers[0].get("price")
                    if raw_price:
                        price = raw_price.replace(".", ",")
                elif isinstance(offers, dict):
                    raw_price = offers.get("price")
                    if raw_price:
                        price = raw_price.replace(".", ",")

                data_json = {"name": name, "sku": sku, "price": price}
                all_data.append(data_json)
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                return None
        else:
            logger.error("Product JSON –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return None

    logger.info(all_data)
    update_sheet_with_data(sheet, all_data)


def ensure_row_limit(sheet, required_rows=8000):
    """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –ª–∏—Å—Ç–µ Google Sheets, –µ—Å–ª–∏ –∏—Ö –º–µ–Ω—å—à–µ —Ç—Ä–µ–±—É–µ–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞."""
    current_rows = len(sheet.get_all_values())
    if current_rows < required_rows:
        sheet.add_rows(required_rows - current_rows)


ensure_row_limit(sheet, 1000)


def update_sheet_with_data(sheet, data, total_rows=8000):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ª–∏—Å—Ç–∞ Google Sheets —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    if not data:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–ª—é—á–µ–π —Å–ª–æ–≤–∞—Ä—è
    headers = list(data[0].keys())

    # –ó–∞–ø–∏—Å—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
    sheet.update(values=[headers], range_name="A1", value_input_option="RAW")

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
    rows = [[entry.get(header, "") for header in headers] for entry in data]

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –¥–æ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ total_rows
    if len(rows) < total_rows:
        empty_row = [""] * len(headers)
        rows.extend([empty_row] * (total_rows - len(rows)))

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö
    end_col = chr(65 + len(headers) - 1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –±—É–∫–≤—É (A, B, C...)
    range_name = f"A2:{end_col}{total_rows + 1}"

    # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç
    sheet.update(values=rows, range_name=range_name, value_input_option="USER_ENTERED")


if __name__ == "__main__":
    parsin_xml()
    main_th()
    pars_htmls()
