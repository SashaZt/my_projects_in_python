# –†–∞–±–æ—á–∏–π –∫–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞ IKEA
# –°–∫—Ä–∏–ø—Ç —Å–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª—ã, –ø–∞—Ä—Å–∏—Ç –∏—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç URL –≤ CSV —Ñ–∞–π–ª, —Å–∫–∞—á–∏–≤–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –ø–∞–ø–∫—É html
# –î–∞—Ç–∞ 21.03.2025
import asyncio
import csv
import hashlib
import json
import math
import os
import re
import shutil
import sys
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger
from playwright.async_api import async_playwright

current_directory = Path.cwd()
xml_directory = current_directory / "xml"
data_directory = current_directory / "data"
log_directory = current_directory / "log"
html_directory = current_directory / "html"

log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
start_xml_path = xml_directory / "sitemap.xml"
all_urls_csv_file = data_directory / "all_urls.csv"
find_urls_csv_file = data_directory / "find_urls.csv"
matches_file = data_directory / "ikea_matches.json"
output_json_file = data_directory / "output.json"
prom_file = current_directory / "–ü—Ä–æ–º.xlsx"
rozetka_file = current_directory / "–†–æ–∑–µ—Ç–∫–∞.xlsx"
exclusion_file = current_directory / "exclusion_products.txt"
not_found_file = data_directory / "not_found_links.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
NUM_WORKERS = 9  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤


headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
}


def download_start_xml():
    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQHAxAF1AcH6mVAQAArt3WuBtwUXBti3t8ORPsEa+dFMMxmYOPPuW6mV9bRo4YAU43ETM+agNUiAjLv2HPh3eGG3iRPib/T1s03gUdXWs2G+vFGjnz9s7jX4W5uNuONMHOK0lVziHXhMm0TVYAwrk5VtVzxTdtraP3H0iEdry7euHhC8TBTXFKmo7wJGGhL2Tmsrir2z5g9Jd9Va+NeR0JLZmrb6zkOZgzDcNEtB4MZFE176DknLRKOibPy1LGjrmkIdF7ZuBoME9RbywxgScsUC2wCsjiKw0oACz1RecoLBuv2HTQIk/XJAsBgqHvkrymGhbvp9lk2wptWfjPe0cKKrP0c2nANArfeKboPNYrtD4wnjHtbDCfjCaLLccWhCcLPzIo1iCQCp5VgEi4j7uNzEctDya8AqqzafUJQ2pG8iptFKEJH/H+lfEQNVClKVxbLvZ+A3QOGYbwvnEiutcq8srawJo0rPg2hRA=~4273222~4342841",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQJAxAF0YyzK+VAQAAT5nauBtZgz5TU1DiJq+Ghr3pgpTkZ417WU3s5T88ORowu8uNdkfZlP9rBE8X6X1dNsj5dX+xV/uzXJxRqIoVmCRpWxiQahl2GDAyRTls4ss4IWu88Evh0+kQXwIKVBg19U8PnwjPPwP1H/NfeTclL3GeO4RD8Nn+0tcUxNgIHzGuN26X+CnEUInsB2UkpbYzgRVHn/UgJCuofQkBsR2BnGhGOuQuZbm9BV1CZ1aGBVmi~1",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
    }

    response = requests.get(
        "https://www.ikea.com/sitemaps/sitemap.xml",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
        with open(start_xml_path, "wb") as file:
            file.write(response.content)
        logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {start_xml_path}")
    else:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")


def parse_start_xml():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    download_start_xml()
    target = "https://www.ikea.com/sitemaps/prod-pl-PL_"
    try:
        # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
        tree = ET.parse(start_xml_path)
        root = tree.getroot()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
        matching_urls = [
            url.text
            for url in root.findall(".//sitemap:loc", namespace)
            if url.text and target in url.text
        ]
        return matching_urls

    except FileNotFoundError:
        return []


def download_all_xml():
    urls = parse_start_xml()

    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQFwxAF/d/lqiVAQAAxIPduBt4QfzIbqtpFjIX5nJXPN1vx4bSUsqKdhHw34Bn556sW/wDVkhfY0p3pm7nDsmWgMZu5DAAoPiYqKftTfHsuxCKIrqz5oQ2lN04Y9OWLuAD1Kh4vAhN/g0jA8mvDLu+gaASszOqDZSZZgyWEcw1YFcrWiK3cz5zm97yk8tVRq/gD/yZ17zVvmFFZZctWrBHkJP0th75fgI6i5n87mDsaLQDgM1L6jahwALTXmLD~1",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQFwxAF/h/lqiVAQAAxIPduBscyVdbfLzmjiN6zjXpG/5VYksQLL3n8CrhyDp0CdoOsVGkUePS671Ge54HHfjq401ZbIwBpPb4EOUQ8lwMuZNBMqJC0Z+nINR1AAo5rmKnkkCx0Z1uNg1RX6sMYI8PzXzBo+v3wzplsOEAfyrRvuW9u/um96ogFa6VR7GhreNOovjW/DF03CaxmS3tusAQzQ4bdpo5tognIeMMDsZ+XlnuXe2LQk2brAEqs7TxtXYhqQ3Rqi7rxiD36NsJ8JVdU7QXTjCO/AwMyZQVK9hVoCVgX+qqVByN3H5Rlup8SPvCKV9MiHeq8EatVT8Pw9wj1ZutDYyvIj/3gWzjOV8NdSTPX4EpDgP4l5oUPvIPBtXhpyLWr/Y8HFdLHv+TWbMYvUfs3Ka3wIHxzQ4AH/j0EKEYtcu57Lww/imSo4jVhHZiuec8SQzeSiCUBJUcOzBXC+ofofKINZ8gk8a0OFX/OCafh1xuoQG/lgRTYESBtw==~4273222~4342841",
    }

    for url in urls:

        response = requests.get(
            url,
            cookies=cookies,
            headers=headers,
            timeout=30,
        )
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL —Å –ø–æ–º–æ—â—å—é Path
        file_name = Path(urlparse(url).path).name  # –ò–∑–≤–ª–µ–∫–∞–µ—Ç 'prod-pl-PL_6.xml'
        file_path = xml_directory / file_name  # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å –ø–æ–º–æ—â—å—é /
        if file_path.exists():
            logger.info(f"–§–∞–π–ª {file_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            continue
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
            with open(file_path, "wb") as file:
                file.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {file_path}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")
    parse_all_sitemap_urls()


def parse_all_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("prod*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
            for url in root.findall(".//sitemap:url", namespace):
                loc = url.find("sitemap:loc", namespace)

                if loc is not None and loc.text:
                    urls.append(loc.text)

        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
            return []
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")
    url_data = pd.DataFrame(urls, columns=["url"])
    url_data.to_csv(all_urls_csv_file, index=False)


async def setup_browser_context():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    p = await async_playwright().start()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
    browser = await p.chromium.launch(
        headless=True
    )  # headless=True –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    context = await browser.new_context(
        bypass_csp=True,
        java_script_enabled=True,
        permissions=["geolocation"],
        device_scale_factor=1.0,
        has_touch=True,
        ignore_https_errors=True,
    )

    # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —à—Ä–∏—Ñ—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤
    await context.route(
        "**/*",
        lambda route, request: (
            route.abort()
            if request.resource_type in ["image", "media", "font", "stylesheet"]
            else route.continue_()
        ),
    )

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    page = await context.new_page()

    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É IKEA
    await page.goto("https://www.ikea.com/pl/pl/")

    # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    await asyncio.sleep(2)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–Ω–æ–ø–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è cookies –∏ –Ω–∞–∂–∏–º–∞–µ–º, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    try:
        cookie_button = await page.wait_for_selector(
            "#onetrust-accept-btn-handler", timeout=5000
        )
        if cookie_button:
            await cookie_button.click()
            # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∫–Ω–æ–ø–∫—É cookies
            await asyncio.sleep(2)
    except Exception as e:
        logger.warning(f"Cookie button not found or error clicking it: {e}")

    # –ñ–¥–µ–º, –ø–æ–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è —ç–ª–µ–º–µ–Ω—Ç —Å –ø–æ—á—Ç–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–æ–º, –∏ –Ω–∞–∂–∏–º–∞–µ–º –Ω–∞ –Ω–µ–≥–æ
    try:
        postal_code_button = await page.wait_for_selector(
            'span:text("Wpisz kod pocztowy")', timeout=10000
        )
        await postal_code_button.click()

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∫–Ω–æ–ø–∫—É –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        await asyncio.sleep(2)

        # –ñ–¥–µ–º, –ø–æ–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è –ø–æ–ª–µ –≤–≤–æ–¥–∞
        postal_code_input = await page.wait_for_selector(
            'input[aria-describedby="hnf-postalcode-helper"]', timeout=10000
        )

        # –í–≤–æ–¥–∏–º –ø–æ—á—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        await postal_code_input.fill("22-100")

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª—è –≤–≤–æ–¥–∞
        await asyncio.sleep(2)

        # –ù–∞–∂–∏–º–∞–µ–º Enter
        await postal_code_input.press("Enter")

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è Enter
        await asyncio.sleep(2)
    except Exception as e:
        logger.warning(f"Error during postal code setup: {e}")

    return p, browser, context, page


async def process_batch_with_single_browser(urls_batch, worker_id):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ URL-–∞–¥—Ä–µ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
    logger.info(f"Worker {worker_id}: Starting to process {len(urls_batch)} URLs")

    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –≤–µ—Å—å –ø–∞–∫–µ—Ç
        p, browser, context, page = await setup_browser_context()
        logger.info(f"Worker {worker_id}: Browser setup completed")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ URL –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –±—Ä–∞—É–∑–µ—Ä–∞
        for url in urls_batch:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if output_html_file.exists():
                logger.debug(
                    f"Worker {worker_id}: File already exists for {url}, skipping"
                )
                continue

            try:
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π URL
                logger.info(f"Worker {worker_id}: Navigating to {url}")
                await page.goto(
                    url, timeout=60000
                )  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

                # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                # await asyncio.sleep(2)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML-–∫–æ–Ω—Ç–µ–Ω—Ç
                content = await page.content()
                with open(output_html_file, "w", encoding="utf-8") as f:
                    f.write(content)

                logger.info(f"Worker {worker_id}: Saved {url} to {output_html_file}")

            except Exception as e:
                logger.error(f"Worker {worker_id}: Error processing URL {url}: {e}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö URL –≤ –ø–∞–∫–µ—Ç–µ
        await browser.close()
        await p.stop()
        logger.info(f"Worker {worker_id}: Completed processing batch")

    except Exception as e:
        logger.error(f"Worker {worker_id}: Critical error in batch processing: {e}")


async def main_pl():
    # –ß—Ç–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –∏–∑ CSV
    urls = []
    try:
        with open(find_urls_csv_file, newline="", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                urls.append(row["url"])
        logger.info(f"Loaded {len(urls)} URLs from CSV")
    except FileNotFoundError:
        logger.error(f"CSV file {find_urls_csv_file} not found.")
        return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ URL –Ω–∞ –ø–∞–∫–µ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞
    batch_size = len(urls) // NUM_WORKERS + (1 if len(urls) % NUM_WORKERS > 0 else 0)
    url_batches = [urls[i : i + batch_size] for i in range(0, len(urls), batch_size)]

    logger.info(f"Processing {len(urls)} URLs with {NUM_WORKERS} workers")
    logger.info(f"Each worker will process approximately {batch_size} URLs")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞–∫–µ—Ç–∞ URL
    tasks = []
    for i, batch in enumerate(url_batches):
        worker_id = i + 1
        tasks.append(process_batch_with_single_browser(batch, worker_id))

    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
    await asyncio.gather(*tasks)

    logger.info("All URLs processed")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–¥–∞ —Ç–æ–≤–∞—Ä–∞ IKEA
def is_valid_ikea_code(code):
    if not isinstance(code, str):
        return False
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç: —Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã
    pattern = r"^\d+\.\d+\.\d+$"
    return bool(re.match(pattern, str(code)))


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ IKEA –≤ formated_code (–±–µ–∑ —Ç–æ—á–µ–∫)
def format_ikea_code(code):
    if not isinstance(code, str):
        return None
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ –∏–∑ –∫–æ–¥–∞
    return code.replace(".", "")


# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel-—Ñ–∞–π–ª–æ–≤
def process_excel_files():
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ü—Ä–æ–º.xlsx
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ {prom_file}...")
        prom_df = pd.read_excel(prom_file, sheet_name=0)  # –ü–µ—Ä–≤—ã–π –ª–∏—Å—Ç

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É'
        if "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É" not in prom_df.columns:
            logger.error(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–æ–Ω–∫–∞ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {prom_file}. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'A'..."
            )
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–∞–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
            prom_df = prom_df.rename(columns={prom_df.columns[0]: "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"})

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ {rozetka_file}...")
        rozetka_df = pd.read_excel(rozetka_file, sheet_name=0)  # –ü–µ—Ä–≤—ã–π –ª–∏—Å—Ç

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ '–ê—Ä—Ç–∏–∫—É–ª'
        if "–ê—Ä—Ç–∏–∫—É–ª" not in rozetka_df.columns:
            logger.error(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–æ–Ω–∫–∞ '–ê—Ä—Ç–∏–∫—É–ª' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {rozetka_file}. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'D'..."
            )
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É D
            try:
                rozetka_df = rozetka_df.rename(
                    columns={rozetka_df.columns[3]: "–ê—Ä—Ç–∏–∫—É–ª"}
                )
            except IndexError:
                logger.error(f"–û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ {rozetka_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ D.")
                return None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
        prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"].astype(str)
        rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"].astype(str)

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ü—Ä–æ–º.xlsx
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ü—Ä–æ–º.xlsx...")
        for idx, row in prom_df.iterrows():
            code = row["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"]
            is_valid = is_valid_ikea_code(code)
            formatted_code = format_ikea_code(code) if is_valid else None

            item = {
                "id_ikea": code,
                "find_ikea": formatted_code,
                "source": "prom",
                "valid": is_valid,
            }

            if not is_valid:
                logger.error(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥ –∏–∑ –ü—Ä–æ–º.xlsx: {code}")

            result.append(item)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx...")
        for idx, row in rozetka_df.iterrows():
            code = row["–ê—Ä—Ç–∏–∫—É–ª"]
            is_valid = is_valid_ikea_code(code)
            formatted_code = format_ikea_code(code) if is_valid else None

            item = {
                "id_ikea": code,
                "find_ikea": formatted_code,
                "source": "rozetka",
                "valid": is_valid,
            }

            if not is_valid:
                logger.error(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx: {code}")

            result.append(item)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_json_file}...")
        with open(output_json_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=4)

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(result)} –∑–∞–ø–∏—Å–µ–π.")
        return result

    except FileNotFoundError as e:
        logger.error(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - {e}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–æ–≤: {e}")

    return None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–æ–≤ IKEA —Å URL –∏–∑ CSV —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
# def match_codes_with_urls(codes_data):
#     if not codes_data:
#         logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–¥–∞—Ö –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è.")
#         return

#     try:
#         if not all_urls_csv_file.exists():
#             logger.error(f"–§–∞–π–ª {all_urls_csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
#             return

#         logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ URL –∏–∑ {all_urls_csv_file}...")
#         urls_df = pd.read_csv(all_urls_csv_file)

#         if "url" not in urls_df.columns:
#             logger.error(f"–ö–æ–ª–æ–Ω–∫–∞ 'url' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {all_urls_csv_file}.")
#             return

#         # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–æ–¥–æ–≤
#         matches = []
#         # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL-–∞–¥—Ä–µ—Å–æ–≤
#         find_urls = []

#         # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∫–æ–¥—ã
#         valid_codes = [item for item in codes_data if item["valid"]]

#         logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(valid_codes)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–¥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")

#         # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –∏—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π URL
#         for code_item in valid_codes:
#             formatted_code = code_item["find_ikea"]
#             if not formatted_code:
#                 continue

#             # –ò—â–µ–º URL, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–¥ —Ç–æ–≤–∞—Ä–∞
#             matching_urls = []
#             for url in urls_df["url"]:
#                 if formatted_code in url:
#                     matching_urls.append(url)
#                     find_urls.append({"url": url})

#             if matching_urls:
#                 # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ (–±–µ–∑ URL)
#                 matches.append(
#                     {
#                         "id_ikea": code_item["id_ikea"],
#                         "find_ikea": formatted_code,
#                         "source": code_item["source"],
#                     }
#                 )

#         logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.")

#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
#         logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {matches_file}...")
#         with open(matches_file, "w", encoding="utf-8") as f:
#             json.dump(matches, f, ensure_ascii=False, indent=4)

#         # –°–æ–∑–¥–∞–µ–º DataFrame —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ URL
#         find_urls_df = pd.DataFrame(find_urls)

#         # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã URL
#         original_count = len(find_urls_df)
#         find_urls_df = find_urls_df.drop_duplicates(subset=["url"])
#         unique_count = len(find_urls_df)

#         # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
#         duplicates_removed = original_count - unique_count
#         logger.info(
#             f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ URL: {duplicates_removed} ({duplicates_removed/original_count*100:.2f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)"
#         )

#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –≤ CSV
#         find_urls_df.to_csv(find_urls_csv_file, index=False)

#         logger.info(
#             f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {matches_file} –∏ {find_urls_csv_file}."
#         )
#         logger.info(
#             f"–í —Ñ–∞–π–ª {find_urls_csv_file} –∑–∞–ø–∏—Å–∞–Ω–æ {unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL."
#         )

#         return matches, find_urls_df.to_dict("records")


#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–¥–æ–≤ —Å URL: {e}")
#         return None, None
def match_codes_with_urls(codes_data):
    if not codes_data:
        logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–¥–∞—Ö –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è.")
        return

    try:
        if not all_urls_csv_file.exists():
            logger.error(f"–§–∞–π–ª {all_urls_csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ URL –∏–∑ {all_urls_csv_file}...")
        urls_df = pd.read_csv(all_urls_csv_file)

        if "url" not in urls_df.columns:
            logger.error(f"–ö–æ–ª–æ–Ω–∫–∞ 'url' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {all_urls_csv_file}.")
            return

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–æ–¥–æ–≤
        matches = []
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL-–∞–¥—Ä–µ—Å–æ–≤
        find_urls = []
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
        not_found_items = []

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∫–æ–¥—ã
        valid_codes = [item for item in codes_data if item["valid"]]

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(valid_codes)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–¥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –∏—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π URL
        for code_item in valid_codes:
            formatted_code = code_item["find_ikea"]
            if not formatted_code:
                continue

            # –ò—â–µ–º URL, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–¥ —Ç–æ–≤–∞—Ä–∞
            matching_urls = []
            for url in urls_df["url"]:
                if formatted_code in url:
                    matching_urls.append(url)
                    find_urls.append({"url": url})

            if matching_urls:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ (–±–µ–∑ URL)
                matches.append(
                    {
                        "id_ikea": code_item["id_ikea"],
                        "find_ikea": formatted_code,
                        "source": code_item["source"],
                    }
                )
            else:
                # –ï—Å–ª–∏ URL –Ω–µ –Ω–∞–π–¥–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
                not_found_items.append(
                    {
                        "id_ikea": code_item["id_ikea"],
                        "find_ikea": formatted_code,
                        "source": code_item["source"],
                    }
                )

        logger.info(
            f"–ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π. –ù–µ –Ω–∞–π–¥–µ–Ω–æ {len(not_found_items)} —Ç–æ–≤–∞—Ä–æ–≤."
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {matches_file}...")
        with open(matches_file, "w", encoding="utf-8") as f:
            json.dump(matches, f, ensure_ascii=False, indent=4)

        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ {not_found_file}...")
        with open(not_found_file, "w", encoding="utf-8") as f:
            json.dump(not_found_items, f, ensure_ascii=False, indent=4)

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ URL
        find_urls_df = pd.DataFrame(find_urls)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã URL
        original_count = len(find_urls_df)
        find_urls_df = find_urls_df.drop_duplicates(subset=["url"])
        unique_count = len(find_urls_df)

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        duplicates_removed = original_count - unique_count
        logger.info(
            f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ URL: {duplicates_removed} ({duplicates_removed/original_count*100:.2f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)"
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –≤ CSV
        find_urls_df.to_csv(find_urls_csv_file, index=False)

        logger.info(
            f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {matches_file} –∏ {find_urls_csv_file}."
        )
        logger.info(
            f"–í —Ñ–∞–π–ª {find_urls_csv_file} –∑–∞–ø–∏—Å–∞–Ω–æ {unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL."
        )
        logger.info(
            f"–í —Ñ–∞–π–ª {not_found_file} –∑–∞–ø–∏—Å–∞–Ω–æ {len(not_found_items)} –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤."
        )

        return matches, find_urls_df.to_dict("records"), not_found_items

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–¥–æ–≤ —Å URL: {e}")
        return None, None, None


def pars_htmls():
    logger.info("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü html")
    extracted_data = []

    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(content, "lxml")
        # 1. –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–∞
        product_title = soup.find("script", attrs={"id": "pip-range-json-ld"})
        json_string = product_title.string
        json_data = None
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ None
        if json_string:
            try:
                # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –∫–∞–∫ JSON
                json_data = json.loads(json_string)
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {e}")
        mpn = json_data.get("mpn") if json_data else None

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
        price = None
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å lowPrice
        price = json_data.get("offers", {}).get("lowPrice")
        # –ï—Å–ª–∏ lowPrice –Ω–µ—Ç (None), –±–µ—Ä–µ–º price
        if price is None:
            price = json_data.get("offers", {}).get("price")

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —ç–ª–µ–º–µ–Ω—Ç —Å "Sklep - Dostƒôpne w magazynie"
        sklep_element = soup.select_one(
            "div.pip-store-section__button.js-stockcheck-section"
        )

        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –≤ —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è "Sklep" –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π
        all_status_elements = soup.select(
            "span.pip-status__label div.pip-store-section__button"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ "Sklep"
        if sklep_element and "Sklep" in sklep_element.text:
            availability_text = sklep_element.text.strip()

        elif all_status_elements:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —è–≤–Ω–æ "Sklep", –Ω–æ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—É—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∏–∑ –Ω–∏—Ö
            availability_text = all_status_elements[0].text.strip()
            logger.info(
                f"–§–∞–π–ª {mpn}: –ù–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {availability_text}"
            )
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏–∫–∞–∫–æ–π —ç–ª–µ–º–µ–Ω—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ
            alt_availability_element = soup.select_one(".pip-status__label")
            if alt_availability_element:
                availability_text = alt_availability_element.text.strip()
                logger.info(
                    f"–§–∞–π–ª {mpn}: –ù–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {availability_text}"
                )
            else:
                availability_text = "–°—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"
                logger.warning(f"–§–∞–π–ª {mpn}: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ñ—Ä–∞–∑–∞ "Sklep - Dostƒôpne w magazynie"
        has_in_store = "Sklep - Dostƒôpne w magazynie" in availability_text
        all_data = {
            "mpn": mpn,
            "price": price,
            "product_in_stock": has_in_store,
        }
        logger.info(all_data)
        extracted_data.append(all_data)
    with open(output_json_file, "w", encoding="utf-8") as json_file:
        json.dump(extracted_data, json_file, ensure_ascii=False, indent=4)
    # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –≤—ã–∑–æ–≤
    update_ikea_matches_with_parsed_data(extracted_data)


def update_ikea_matches_with_parsed_data(extracted_data):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö –≤ ikea_matches.json, –¥–æ–±–∞–≤–ª—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–Ω–µ –∏ –Ω–∞–ª–∏—á–∏–∏.

    Args:
        extracted_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ pars_htmls()
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏
    if not matches_file.exists():
        logger.error(f"–§–∞–π–ª {matches_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
    try:
        with open(matches_file, "r", encoding="utf-8") as f:
            ikea_matches = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ikea_matches)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {matches_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ JSON-—Ñ–∞–π–ª–∞ {matches_file}: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ MPN
    mpn_to_data = {}
    for item in extracted_data:
        if item["mpn"]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ MPN –Ω–µ –ø—É—Å—Ç–æ–π
            mpn_to_data[item["mpn"]] = item

    logger.info(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å —Å {len(mpn_to_data)} —Ç–æ–≤–∞—Ä–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ MPN")

    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    updated_count = 0
    not_found_count = 0

    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
    for ikea_item in ikea_matches:
        id_ikea = ikea_item["id_ikea"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ MPN
        if id_ikea in mpn_to_data:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ –∏ —Ü–µ–Ω–µ
            data_item = mpn_to_data[id_ikea]

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ –∏ —Ü–µ–Ω–µ
            ikea_item["product_in_stock"] = data_item["product_in_stock"]
            ikea_item["price"] = data_item["price"]

            logger.info(
                f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {id_ikea}: —Ü–µ–Ω–∞={data_item['price']}, –Ω–∞–ª–∏—á–∏–µ={data_item['product_in_stock']}"
            )
            updated_count += 1
        else:
            # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ –æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            ikea_item["product_in_stock"] = False
            ikea_item["price"] = None
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {id_ikea}")
            not_found_count += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    try:
        with open(matches_file, "w", encoding="utf-8") as f:
            json.dump(ikea_matches, f, ensure_ascii=False, indent=4)
        logger.info(
            f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {matches_file}. –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count}, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {not_found_count}"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")


def update_excel_files_with_availability_info(
    exchange_rate_rozetka, exchange_rate_prom
):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª—ã Excel –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ikea_matches.json.

    –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –†–æ–∑–µ—Ç–∫–∏:
    - –ò—â–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ id_ikea –≤ –∫–æ–ª–æ–Ω–∫–µ D (–ê—Ä—Ç–∏–∫—É–ª)
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É M (–ù–∞—è–≤–Ω—ñ—Å—Ç—å): "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ" –∏–ª–∏ "–ù–µ –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É H (–¶—ñ–Ω–∞) —Å —Ü–µ–Ω–æ–π

    –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ Prom:
    - –ò—â–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ id_ikea –≤ –∫–æ–ª–æ–Ω–∫–µ A (–ö–æ–¥_—Ç–æ–≤–∞—Ä—É)
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É P (–ù–∞—è–≤–Ω—ñ—Å—Ç—å): "7" –µ—Å–ª–∏ –≤ –Ω–∞–ª–∏—á–∏–∏, "-" –µ—Å–ª–∏ –Ω–µ—Ç
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É I (–¶—ñ–Ω–∞) —Å —Ü–µ–Ω–æ–π
    """

    skipped_count = 0
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    if not matches_file.exists():
        logger.error(f"–§–∞–π–ª {matches_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    if not prom_file.exists():
        logger.error(f"–§–∞–π–ª {prom_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    if not rozetka_file.exists():
        logger.error(f"–§–∞–π–ª {rozetka_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return
    if not not_found_file.exists():
        logger.error(f"–§–∞–π–ª {not_found_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ikea_matches.json
    try:
        with open(matches_file, "r", encoding="utf-8") as f:
            ikea_matches = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ikea_matches)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {matches_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ JSON-—Ñ–∞–π–ª–∞ {matches_file}: {e}")
        return
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ not_found_file.json
    try:
        with open(not_found_file, "r", encoding="utf-8") as f:
            not_found_items = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(not_found_items)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {not_found_file}")

        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        not_found_ids = {item["id_ikea"] for item in not_found_items}

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ JSON-—Ñ–∞–π–ª–∞ {not_found_file}: {e}")
        return

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
    rozetka_items = [item for item in ikea_matches if item.get("source") == "rozetka"]
    prom_items = [item for item in ikea_matches if item.get("source") == "prom"]

    logger.info(
        f"–¢–æ–≤–∞—Ä–æ–≤ –¥–ª—è –†–æ–∑–µ—Ç–∫–∏: {len(rozetka_items)}, –¥–ª—è Prom: {len(prom_items)}"
    )
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    excluded_products = set()
    if exclusion_file.exists():
        try:
            with open(exclusion_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        excluded_products.add(line)
            logger.info(
                f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(excluded_products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π: {e}")
    else:
        logger.warning(
            f"–§–∞–π–ª –∏—Å–∫–ª—é—á–µ–Ω–∏–π {exclusion_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π."
        )
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª –†–æ–∑–µ—Ç–∫–∏
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª
        rozetka_df = pd.read_excel(rozetka_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –†–æ–∑–µ—Ç–∫–∞.xlsx, —Ä–∞–∑–º–µ—Ä: {rozetka_df.shape}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if "–ê—Ä—Ç–∏–∫—É–ª" not in rozetka_df.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É D
            try:
                column_D_name = rozetka_df.columns[3]  # 4-–π —Å—Ç–æ–ª–±–µ—Ü (–∏–Ω–¥–µ–∫—Å 3) - —ç—Ç–æ D
                rozetka_df = rozetka_df.rename(columns={column_D_name: "–ê—Ä—Ç–∏–∫—É–ª"})
                logger.info(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ D '{column_D_name}' –≤ '–ê—Ä—Ç–∏–∫—É–ª'")
            except IndexError:
                logger.error(f"–í —Ñ–∞–π–ª–µ {rozetka_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ D.")
                return

        # –ö–æ–ª–æ–Ω–∫–∞ M (–ù–∞—è–≤–Ω—ñ—Å—Ç—å) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 12
        availability_column_idx = 12
        availability_column_name = rozetka_df.columns[availability_column_idx]

        # –ö–æ–ª–æ–Ω–∫–∞ H (–¶—ñ–Ω–∞) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 7
        price_column_idx = 7
        price_column_name = rozetka_df.columns[price_column_idx]

        logger.info(
            f"–ö–æ–ª–æ–Ω–∫–∞ –Ω–∞–ª–∏—á–∏—è: {availability_column_name} (M), –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã: {price_column_name} (H)"
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—Ä—Ç–∏–∫—É–ª—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"].astype(str)
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –†–æ–∑–µ—Ç–∫–∏
        not_found_rozetka = [
            item for item in not_found_items if item["source"] == "rozetka"
        ]
        not_found_updated_count = 0

        for not_found_item in not_found_rozetka:
            mask = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] == not_found_item["id_ikea"]
            if mask.any():
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏ –Ω—É–ª–µ–≤—É—é —Ü–µ–Ω—É
                rozetka_df.loc[mask, availability_column_name] = "–ù–µ –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
                rozetka_df.loc[mask, price_column_name] = 0
                not_found_updated_count += sum(mask)

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        updated_count = 0
        for item in rozetka_items:
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –ê—Ä—Ç–∏–∫—É–ª —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å id_ikea
            mask = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] == item["id_ikea"]
            if mask.any():
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ
                avail_value = (
                    "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
                    if item.get("product_in_stock", False)
                    else "–ù–µ –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
                )
                rozetka_df.loc[mask, availability_column_name] = avail_value

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É, –µ—Å–ª–∏ —Ç–æ–≤–∞—Ä –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
                if (
                    item.get("price") is not None
                    and item["id_ikea"] not in excluded_products
                ):
                    price_value = abs(float(item["price"])) * exchange_rate_rozetka
                    price = math.ceil(float(price_value))
                    rozetka_df.loc[mask, price_column_name] = price

                updated_count += sum(mask)

        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ –†–æ–∑–µ—Ç–∫–∞.xlsx")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        output_rozetka_file = current_directory / "–†–æ–∑–µ—Ç–∫–∞_–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π.xlsx"
        rozetka_df.to_excel(output_rozetka_file, index=False)
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_rozetka_file}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –†–æ–∑–µ—Ç–∫–∞.xlsx: {e}")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª Prom
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª
        prom_df = pd.read_excel(prom_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –ü—Ä–æ–º.xlsx, —Ä–∞–∑–º–µ—Ä: {prom_df.shape}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É" not in prom_df.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É A
            try:
                column_A_name = prom_df.columns[0]  # 1-–π —Å—Ç–æ–ª–±–µ—Ü (–∏–Ω–¥–µ–∫—Å 0) - —ç—Ç–æ A
                prom_df = prom_df.rename(columns={column_A_name: "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"})
                logger.info(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ A '{column_A_name}' –≤ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É'")
            except IndexError:
                logger.error(f"–í —Ñ–∞–π–ª–µ {prom_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ A.")
                return

        # –ö–æ–ª–æ–Ω–∫–∞ P (–ù–∞—è–≤–Ω—ñ—Å—Ç—å) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 15
        availability_column_idx = 15
        availability_column_name = prom_df.columns[availability_column_idx]

        # –ö–æ–ª–æ–Ω–∫–∞ I (–¶—ñ–Ω–∞) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 8
        price_column_idx = 8
        price_column_name = prom_df.columns[price_column_idx]

        logger.info(
            f"–ö–æ–ª–æ–Ω–∫–∞ –Ω–∞–ª–∏—á–∏—è: {availability_column_name} (P), –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã: {price_column_name} (I)"
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–¥—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"].astype(str)
        not_found_prom = [item for item in not_found_items if item["source"] == "prom"]
        not_found_updated_count_prom = 0

        for not_found_item in not_found_prom:
            mask = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] == not_found_item["id_ikea"]
            if mask.any():
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏ –Ω—É–ª–µ–≤—É—é —Ü–µ–Ω—É
                prom_df.loc[mask, availability_column_name] = "-"
                prom_df.loc[mask, price_column_name] = 0
                not_found_updated_count_prom += sum(mask)
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        updated_count = 0
        for item in prom_items:
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –ö–æ–¥_—Ç–æ–≤–∞—Ä—É —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å id_ikea
            mask = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] == item["id_ikea"]
            if mask.any():
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ
                avail_value = "7" if item.get("product_in_stock", False) else "-"
                prom_df.loc[mask, availability_column_name] = avail_value

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                if (
                    item.get("price") is not None
                    and item["id_ikea"] not in excluded_products
                ):

                    price_value = abs(float(item["price"])) * exchange_rate_prom
                    price = math.ceil(float(price_value))
                    prom_df.loc[mask, price_column_name] = price

                updated_count += sum(mask)

        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ –ü—Ä–æ–º.xlsx")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        output_prom_file = current_directory / "–ü—Ä–æ–º_–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π.xlsx"
        prom_df.to_excel(output_prom_file, index=False)
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_prom_file}")
        logger.info(
            f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ –ü—Ä–æ–º.xlsx, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_count} –∏–∑ —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –ü—Ä–æ–º.xlsx: {e}")


def main_loop():
    while True:
        # –ó–∞–ø—Ä–æ—Å –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print(
            "–í–≤–µ–¥–∏—Ç–µ 1 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Å—ã–ª–æ–∫"
            "\n–í–≤–µ–¥–∏—Ç–µ 2 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"
            "\n–í–≤–µ–¥–∏—Ç–µ 3 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –≤ Excel"
            "\n–í–≤–µ–¥–∏—Ç–µ 4 –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"
            "\n–í–≤–µ–¥–∏—Ç–µ 0 –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"
        )
        user_input = int(input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: "))

        if user_input == 1:
            download_all_xml()
            codes_data = process_excel_files()
            match_codes_with_urls(codes_data)
        elif user_input == 2:
            asyncio.run(main_pl())
        elif user_input == 3:
            logger.info("–í–≤–µ–¥–∏—Ç–µ –∫—É—Ä—Å –¥–ª—è rozetka")
            exchange_rate_rozetka = float(input())
            logger.info("–í–≤–µ–¥–∏—Ç–µ –∫—É—Ä—Å –¥–ª—è prom")
            exchange_rate_prom = float(input())
            pars_htmls()
            update_excel_files_with_availability_info(
                exchange_rate_rozetka, exchange_rate_prom
            )
        elif user_input == 4:
            if os.path.exists(html_directory):
                shutil.rmtree(html_directory)
            html_directory.mkdir(parents=True, exist_ok=True)
        elif user_input == 0:
            print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            break  # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è.")


if __name__ == "__main__":
    main_loop()
