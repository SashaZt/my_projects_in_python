# –†–∞–±–æ—á–∏–π –∫–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞ IKEA
# –°–∫—Ä–∏–ø—Ç —Å–∫–∞—á–∏–≤–∞–µ—Ç XML —Ñ–∞–π–ª—ã, –ø–∞—Ä—Å–∏—Ç –∏—Ö, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç URL –≤ CSV —Ñ–∞–π–ª, —Å–∫–∞—á–∏–≤–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –ø–∞–ø–∫—É html
# –î–∞—Ç–∞ 21.03.2025
import asyncio
import csv
import hashlib
import json
import os
import re
import sys
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger
from playwright.async_api import async_playwright

current_directory = Path.cwd()
xml_directory = current_directory / "xml"
data_directory = current_directory / "data"
log_directory = current_directory / "log"
html_directory = current_directory / "html"

log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
start_xml_path = xml_directory / "sitemap.xml"
all_urls_csv_file = data_directory / "all_urls.csv"
find_urls_csv_file = data_directory / "find_urls.csv"
matches_file = data_directory / "ikea_matches.json"
output_json_file = data_directory / "output.json"
prom_file = current_directory / "–ü—Ä–æ–º.xlsx"
rozetka_file = current_directory / "–†–æ–∑–µ—Ç–∫–∞.xlsx"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
NUM_WORKERS = 9  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤


headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
}


def download_start_xml():
    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQHAxAF1AcH6mVAQAArt3WuBtwUXBti3t8ORPsEa+dFMMxmYOPPuW6mV9bRo4YAU43ETM+agNUiAjLv2HPh3eGG3iRPib/T1s03gUdXWs2G+vFGjnz9s7jX4W5uNuONMHOK0lVziHXhMm0TVYAwrk5VtVzxTdtraP3H0iEdry7euHhC8TBTXFKmo7wJGGhL2Tmsrir2z5g9Jd9Va+NeR0JLZmrb6zkOZgzDcNEtB4MZFE176DknLRKOibPy1LGjrmkIdF7ZuBoME9RbywxgScsUC2wCsjiKw0oACz1RecoLBuv2HTQIk/XJAsBgqHvkrymGhbvp9lk2wptWfjPe0cKKrP0c2nANArfeKboPNYrtD4wnjHtbDCfjCaLLccWhCcLPzIo1iCQCp5VgEi4j7uNzEctDya8AqqzafUJQ2pG8iptFKEJH/H+lfEQNVClKVxbLvZ+A3QOGYbwvnEiutcq8srawJo0rPg2hRA=~4273222~4342841",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQJAxAF0YyzK+VAQAAT5nauBtZgz5TU1DiJq+Ghr3pgpTkZ417WU3s5T88ORowu8uNdkfZlP9rBE8X6X1dNsj5dX+xV/uzXJxRqIoVmCRpWxiQahl2GDAyRTls4ss4IWu88Evh0+kQXwIKVBg19U8PnwjPPwP1H/NfeTclL3GeO4RD8Nn+0tcUxNgIHzGuN26X+CnEUInsB2UkpbYzgRVHn/UgJCuofQkBsR2BnGhGOuQuZbm9BV1CZ1aGBVmi~1",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
    }

    response = requests.get(
        "https://www.ikea.com/sitemaps/sitemap.xml",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
        with open(start_xml_path, "wb") as file:
            file.write(response.content)
        logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {start_xml_path}")
    else:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")


def parse_start_xml():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    download_start_xml()
    target = "https://www.ikea.com/sitemaps/prod-pl-PL_"
    try:
        # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
        tree = ET.parse(start_xml_path)
        root = tree.getroot()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
        matching_urls = [
            url.text
            for url in root.findall(".//sitemap:loc", namespace)
            if url.text and target in url.text
        ]
        return matching_urls

    except FileNotFoundError:
        return []


def download_all_xml():
    urls = parse_start_xml()

    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQFwxAF/d/lqiVAQAAxIPduBt4QfzIbqtpFjIX5nJXPN1vx4bSUsqKdhHw34Bn556sW/wDVkhfY0p3pm7nDsmWgMZu5DAAoPiYqKftTfHsuxCKIrqz5oQ2lN04Y9OWLuAD1Kh4vAhN/g0jA8mvDLu+gaASszOqDZSZZgyWEcw1YFcrWiK3cz5zm97yk8tVRq/gD/yZ17zVvmFFZZctWrBHkJP0th75fgI6i5n87mDsaLQDgM1L6jahwALTXmLD~1",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQFwxAF/h/lqiVAQAAxIPduBscyVdbfLzmjiN6zjXpG/5VYksQLL3n8CrhyDp0CdoOsVGkUePS671Ge54HHfjq401ZbIwBpPb4EOUQ8lwMuZNBMqJC0Z+nINR1AAo5rmKnkkCx0Z1uNg1RX6sMYI8PzXzBo+v3wzplsOEAfyrRvuW9u/um96ogFa6VR7GhreNOovjW/DF03CaxmS3tusAQzQ4bdpo5tognIeMMDsZ+XlnuXe2LQk2brAEqs7TxtXYhqQ3Rqi7rxiD36NsJ8JVdU7QXTjCO/AwMyZQVK9hVoCVgX+qqVByN3H5Rlup8SPvCKV9MiHeq8EatVT8Pw9wj1ZutDYyvIj/3gWzjOV8NdSTPX4EpDgP4l5oUPvIPBtXhpyLWr/Y8HFdLHv+TWbMYvUfs3Ka3wIHxzQ4AH/j0EKEYtcu57Lww/imSo4jVhHZiuec8SQzeSiCUBJUcOzBXC+ofofKINZ8gk8a0OFX/OCafh1xuoQG/lgRTYESBtw==~4273222~4342841",
    }

    for url in urls:

        response = requests.get(
            url,
            cookies=cookies,
            headers=headers,
            timeout=30,
        )
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL —Å –ø–æ–º–æ—â—å—é Path
        file_name = Path(urlparse(url).path).name  # –ò–∑–≤–ª–µ–∫–∞–µ—Ç 'prod-pl-PL_6.xml'
        file_path = xml_directory / file_name  # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å –ø–æ–º–æ—â—å—é /
        if file_path.exists():
            logger.info(f"–§–∞–π–ª {file_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            continue
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
            with open(file_path, "wb") as file:
                file.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {file_path}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")
    parse_all_sitemap_urls()


def parse_all_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("prod*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
            for url in root.findall(".//sitemap:url", namespace):
                loc = url.find("sitemap:loc", namespace)

                if loc is not None and loc.text:
                    urls.append(loc.text)

        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
            return []
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")
    url_data = pd.DataFrame(urls, columns=["url"])
    url_data.to_csv(all_urls_csv_file, index=False)


async def setup_browser_context():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    p = await async_playwright().start()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
    browser = await p.chromium.launch(
        headless=False
    )  # headless=True –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    context = await browser.new_context(
        bypass_csp=True,
        java_script_enabled=True,
        permissions=["geolocation"],
        device_scale_factor=1.0,
        has_touch=True,
        ignore_https_errors=True,
    )

    # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —à—Ä–∏—Ñ—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤
    await context.route(
        "**/*",
        lambda route, request: (
            route.abort()
            if request.resource_type in ["image", "media", "font", "stylesheet"]
            else route.continue_()
        ),
    )

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    page = await context.new_page()

    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É IKEA
    await page.goto("https://www.ikea.com/pl/pl/")

    # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    await asyncio.sleep(2)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–Ω–æ–ø–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è cookies –∏ –Ω–∞–∂–∏–º–∞–µ–º, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    try:
        cookie_button = await page.wait_for_selector(
            "#onetrust-accept-btn-handler", timeout=5000
        )
        if cookie_button:
            await cookie_button.click()
            # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∫–Ω–æ–ø–∫—É cookies
            await asyncio.sleep(2)
    except Exception as e:
        logger.warning(f"Cookie button not found or error clicking it: {e}")

    # –ñ–¥–µ–º, –ø–æ–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è —ç–ª–µ–º–µ–Ω—Ç —Å –ø–æ—á—Ç–æ–≤—ã–º –∏–Ω–¥–µ–∫—Å–æ–º, –∏ –Ω–∞–∂–∏–º–∞–µ–º –Ω–∞ –Ω–µ–≥–æ
    try:
        postal_code_button = await page.wait_for_selector(
            'span:text("Wpisz kod pocztowy")', timeout=10000
        )
        await postal_code_button.click()

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –∫–Ω–æ–ø–∫—É –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        await asyncio.sleep(2)

        # –ñ–¥–µ–º, –ø–æ–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è –ø–æ–ª–µ –≤–≤–æ–¥–∞
        postal_code_input = await page.wait_for_selector(
            'input[aria-describedby="hnf-postalcode-helper"]', timeout=10000
        )

        # –í–≤–æ–¥–∏–º –ø–æ—á—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        await postal_code_input.fill("22-100")

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª—è –≤–≤–æ–¥–∞
        await asyncio.sleep(2)

        # –ù–∞–∂–∏–º–∞–µ–º Enter
        await postal_code_input.press("Enter")

        # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è Enter
        await asyncio.sleep(2)
    except Exception as e:
        logger.warning(f"Error during postal code setup: {e}")

    return p, browser, context, page


async def process_batch_with_single_browser(urls_batch, worker_id):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ URL-–∞–¥—Ä–µ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
    logger.info(f"Worker {worker_id}: Starting to process {len(urls_batch)} URLs")

    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –≤–µ—Å—å –ø–∞–∫–µ—Ç
        p, browser, context, page = await setup_browser_context()
        logger.info(f"Worker {worker_id}: Browser setup completed")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ URL –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –±—Ä–∞—É–∑–µ—Ä–∞
        for url in urls_batch:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if output_html_file.exists():
                logger.debug(
                    f"Worker {worker_id}: File already exists for {url}, skipping"
                )
                continue

            try:
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π URL
                logger.info(f"Worker {worker_id}: Navigating to {url}")
                await page.goto(
                    url, timeout=60000
                )  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

                # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                # await asyncio.sleep(2)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML-–∫–æ–Ω—Ç–µ–Ω—Ç
                content = await page.content()
                with open(output_html_file, "w", encoding="utf-8") as f:
                    f.write(content)

                logger.info(f"Worker {worker_id}: Saved {url} to {output_html_file}")

            except Exception as e:
                logger.error(f"Worker {worker_id}: Error processing URL {url}: {e}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö URL –≤ –ø–∞–∫–µ—Ç–µ
        await browser.close()
        await p.stop()
        logger.info(f"Worker {worker_id}: Completed processing batch")

    except Exception as e:
        logger.error(f"Worker {worker_id}: Critical error in batch processing: {e}")


async def main_pl():
    # –ß—Ç–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL –∏–∑ CSV
    urls = []
    try:
        with open(find_urls_csv_file, newline="", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                urls.append(row["url"])
        logger.info(f"Loaded {len(urls)} URLs from CSV")
    except FileNotFoundError:
        logger.error(f"CSV file {find_urls_csv_file} not found.")
        return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ URL –Ω–∞ –ø–∞–∫–µ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞
    batch_size = len(urls) // NUM_WORKERS + (1 if len(urls) % NUM_WORKERS > 0 else 0)
    url_batches = [urls[i : i + batch_size] for i in range(0, len(urls), batch_size)]

    logger.info(f"Processing {len(urls)} URLs with {NUM_WORKERS} workers")
    logger.info(f"Each worker will process approximately {batch_size} URLs")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞–∫–µ—Ç–∞ URL
    tasks = []
    for i, batch in enumerate(url_batches):
        worker_id = i + 1
        tasks.append(process_batch_with_single_browser(batch, worker_id))

    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
    await asyncio.gather(*tasks)

    logger.info("All URLs processed")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–¥–∞ —Ç–æ–≤–∞—Ä–∞ IKEA
def is_valid_ikea_code(code):
    if not isinstance(code, str):
        return False
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç: —Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã
    pattern = r"^\d+\.\d+\.\d+$"
    return bool(re.match(pattern, str(code)))


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ IKEA –≤ formated_code (–±–µ–∑ —Ç–æ—á–µ–∫)
def format_ikea_code(code):
    if not isinstance(code, str):
        return None
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ –∏–∑ –∫–æ–¥–∞
    return code.replace(".", "")


# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel-—Ñ–∞–π–ª–æ–≤
def process_excel_files():
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ü—Ä–æ–º.xlsx
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ {prom_file}...")
        prom_df = pd.read_excel(prom_file)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É'
        if "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É" not in prom_df.columns:
            logger.error(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–æ–Ω–∫–∞ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {prom_file}. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'A'..."
            )
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–∞–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
            prom_df = prom_df.rename(columns={prom_df.columns[0]: "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"})

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ {rozetka_file}...")
        rozetka_df = pd.read_excel(rozetka_file)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ '–ê—Ä—Ç–∏–∫—É–ª'
        if "–ê—Ä—Ç–∏–∫—É–ª" not in rozetka_df.columns:
            logger.error(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –ö–æ–ª–æ–Ω–∫–∞ '–ê—Ä—Ç–∏–∫—É–ª' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {rozetka_file}. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'D'..."
            )
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É D
            try:
                rozetka_df = rozetka_df.rename(
                    columns={rozetka_df.columns[3]: "–ê—Ä—Ç–∏–∫—É–ª"}
                )
            except IndexError:
                logger.error(f"–û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ {rozetka_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ D.")
                return None

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
        prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"].astype(str)
        rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"].astype(str)

        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ü—Ä–æ–º.xlsx
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ü—Ä–æ–º.xlsx...")
        for idx, row in prom_df.iterrows():
            code = row["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"]
            is_valid = is_valid_ikea_code(code)
            formatted_code = format_ikea_code(code) if is_valid else None

            item = {
                "id_ikea": code,
                "find_ikea": formatted_code,
                "source": "prom",
                "valid": is_valid,
            }

            if not is_valid:
                logger.error(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥ –∏–∑ –ü—Ä–æ–º.xlsx: {code}")

            result.append(item)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx...")
        for idx, row in rozetka_df.iterrows():
            code = row["–ê—Ä—Ç–∏–∫—É–ª"]
            is_valid = is_valid_ikea_code(code)
            formatted_code = format_ikea_code(code) if is_valid else None

            item = {
                "id_ikea": code,
                "find_ikea": formatted_code,
                "source": "rozetka",
                "valid": is_valid,
            }

            if not is_valid:
                logger.error(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∫–æ–¥ –∏–∑ –†–æ–∑–µ—Ç–∫–∞.xlsx: {code}")

            result.append(item)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_json_file}...")
        with open(output_json_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=4)

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(result)} –∑–∞–ø–∏—Å–µ–π.")
        return result

    except FileNotFoundError as e:
        logger.error(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω - {e}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–æ–≤: {e}")

    return None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–æ–≤ IKEA —Å URL –∏–∑ CSV
def match_codes_with_urls(codes_data):
    if not codes_data:
        logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–¥–∞—Ö –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è.")
        return

    try:

        if not all_urls_csv_file.exists():
            logger.error(f"–§–∞–π–ª {all_urls_csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ URL –∏–∑ {all_urls_csv_file}...")
        urls_df = pd.read_csv(all_urls_csv_file)

        if "url" not in urls_df.columns:
            logger.error(f"–ö–æ–ª–æ–Ω–∫–∞ 'url' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {all_urls_csv_file}.")
            return

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–æ–¥–æ–≤
        matches = []
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL-–∞–¥—Ä–µ—Å–æ–≤
        find_urls = []

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∫–æ–¥—ã
        valid_codes = [item for item in codes_data if item["valid"]]

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(valid_codes)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–¥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –∏—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π URL
        for code_item in valid_codes:
            formatted_code = code_item["find_ikea"]
            if not formatted_code:
                continue

            # –ò—â–µ–º URL, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–¥ —Ç–æ–≤–∞—Ä–∞
            matching_urls = []
            for url in urls_df["url"]:
                if formatted_code in url:
                    matching_urls.append(url)
                    find_urls.append({"url": url})

            if matching_urls:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ (–±–µ–∑ URL)
                matches.append(
                    {
                        "id_ikea": code_item["id_ikea"],
                        "find_ikea": formatted_code,
                        "source": code_item["source"],
                    }
                )

        logger.info(
            f"–ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {matches_file}..."
        )

        with open(matches_file, "w", encoding="utf-8") as f:
            json.dump(matches, f, ensure_ascii=False, indent=4)

        # –°–æ–∑–¥–∞–µ–º CSV —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ URL
        find_urls_df = pd.DataFrame(find_urls, columns=["url"])
        find_urls_df.to_csv(find_urls_csv_file, index=False)

        logger.info(
            f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {matches_file} –∏ {find_urls_csv_file}."
        )

        return matches, find_urls

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–¥–æ–≤ —Å URL: {e}")
        return None, None


def main_loop():
    while True:
        # –ó–∞–ø—Ä–æ—Å –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print(
            "–í–≤–µ–¥–∏—Ç–µ 1 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Å—ã–ª–æ–∫"
            "\n–í–≤–µ–¥–∏—Ç–µ 2 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"
            "\n–í–≤–µ–¥–∏—Ç–µ 3 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –≤ Excel"
            "\n–í–≤–µ–¥–∏—Ç–µ 0 –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"
        )
        user_input = int(input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: "))

        if user_input == 1:
            download_all_xml()
        elif user_input == 2:
            asyncio.run(main_pl())
        elif user_input == 3:
            pass
        #     asyncio.run(parsing_page())
        elif user_input == 0:
            print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            break  # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        else:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è.")


if __name__ == "__main__":
    # main_loop()
    codes_data = process_excel_files()
    match_codes_with_urls(codes_data)


if __name__ == "__main__":
    # main_loop()
    codes_data = process_excel_files()
    match_codes_with_urls(codes_data)
