import csv
import hashlib
import json
import os
import sys
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urlparse

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

current_directory = Path.cwd()
xml_directory = current_directory / "xml"
data_directory = current_directory / "data"
log_directory = current_directory / "log"
html_directory = current_directory / "html"

log_directory.mkdir(parents=True, exist_ok=True)
data_directory.mkdir(parents=True, exist_ok=True)
xml_directory.mkdir(parents=True, exist_ok=True)
html_directory.mkdir(parents=True, exist_ok=True)

log_file_path = log_directory / "log_message.log"
start_xml_path = xml_directory / "sitemap.xml"
output_csv_file = data_directory / "output.csv"
output_json_file = data_directory / "output.json"

logger.remove()
# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logger.add(
    log_file_path,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {line} | {message}",
    level="DEBUG",
    encoding="utf-8",
    rotation="10 MB",
    retention="7 days",
)

# üîπ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å (—Ü–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥)
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{line}</cyan> | <cyan>{message}</cyan>",
    level="DEBUG",
    enqueue=True,
)
headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "ru,en;q=0.9,uk;q=0.8",
    "dnt": "1",
    "priority": "u=0, i",
    "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
}


def download_start_xml():
    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQHAxAF1AcH6mVAQAArt3WuBtwUXBti3t8ORPsEa+dFMMxmYOPPuW6mV9bRo4YAU43ETM+agNUiAjLv2HPh3eGG3iRPib/T1s03gUdXWs2G+vFGjnz9s7jX4W5uNuONMHOK0lVziHXhMm0TVYAwrk5VtVzxTdtraP3H0iEdry7euHhC8TBTXFKmo7wJGGhL2Tmsrir2z5g9Jd9Va+NeR0JLZmrb6zkOZgzDcNEtB4MZFE176DknLRKOibPy1LGjrmkIdF7ZuBoME9RbywxgScsUC2wCsjiKw0oACz1RecoLBuv2HTQIk/XJAsBgqHvkrymGhbvp9lk2wptWfjPe0cKKrP0c2nANArfeKboPNYrtD4wnjHtbDCfjCaLLccWhCcLPzIo1iCQCp5VgEi4j7uNzEctDya8AqqzafUJQ2pG8iptFKEJH/H+lfEQNVClKVxbLvZ+A3QOGYbwvnEiutcq8srawJo0rPg2hRA=~4273222~4342841",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQJAxAF0YyzK+VAQAAT5nauBtZgz5TU1DiJq+Ghr3pgpTkZ417WU3s5T88ORowu8uNdkfZlP9rBE8X6X1dNsj5dX+xV/uzXJxRqIoVmCRpWxiQahl2GDAyRTls4ss4IWu88Evh0+kQXwIKVBg19U8PnwjPPwP1H/NfeTclL3GeO4RD8Nn+0tcUxNgIHzGuN26X+CnEUInsB2UkpbYzgRVHn/UgJCuofQkBsR2BnGhGOuQuZbm9BV1CZ1aGBVmi~1",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
    }

    response = requests.get(
        "https://www.ikea.com/sitemaps/sitemap.xml",
        cookies=cookies,
        headers=headers,
        timeout=30,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
    if response.status_code == 200:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
        with open(start_xml_path, "wb") as file:
            file.write(response.content)
        logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {start_xml_path}")
    else:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")


def parse_start_xml():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    download_start_xml()
    target = "https://www.ikea.com/sitemaps/prod-pl-PL_"
    try:
        # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
        tree = ET.parse(start_xml_path)
        root = tree.getroot()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

        # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
        matching_urls = [
            url.text
            for url in root.findall(".//sitemap:loc", namespace)
            if url.text and target in url.text
        ]
        return matching_urls

    except FileNotFoundError:
        return []


def download_all_xml():
    urls = parse_start_xml()

    cookies = {
        "ikexp_id": "4ae34af6-60c6-4801-89ff-8080a441f26d",
        "ak_bmsc": "63624010FCFAF059993FE3731D18C012~000000000000000000000000000000~YAAQHwxAF05yZa6VAQAAwuC2uBuQuyxHPgi1ZcY9osMYW4hOPNAGPNItdPFFUdZpszT5rKClPsrSTa0HqJjPfjhu1UojeTVNeKoURT/ikcmmsPuXe0q5H8Gv0uCwJGejK/lVzT4B+xy+K1blRrGsPeHHRkAVkJgugqOv+s3bbHPFjy27pZTpk3t3AJ4dql2S6a21EiqxpFlgPYy1N4bYFZ7xLZuO84Now434qDfC3pduUHjOEvBPay/B4h5BQXe/1irzAfnrg1bbkvgFFC8eM8jsG++HjYNtxF/AZkRXLznopx+XweMCP4a+2yndvEzUA1IZloP9dn8Bq/e/WzEt2PV02REAwPoXEV6x85Tgb74u4cXGTopG1NwhKssreNRcuhfm1vavRY8=",
        "ikea_cookieconsent_pl": "%7B%221%22%3Atrue%2C%222%22%3Atrue%2C%223%22%3Atrue%2C%224%22%3Atrue%7D",
        "_fbp": "fb.1.1742560761431.444801568",
        "rtbhouse-split": "1",
        "episod_id": "1742560761504.f4ee1238",
        "BehtarIKEA": "7336e9bf-c5be-4abc-8ce1-f3c50fa4c270",
        "PIM-SESSION-ID": "pzixFKEDlO0aNhE0",
        "weDareSessionId": "5oebr-hrmlk-5jy",
        "_cs_mk_ga": "0.5746547625992453_1742562698666",
        "_abck": "F62F3894666D693DDA831F7A6E61F6B9~0~YAAQHAxAF//qHqmVAQAALavVuA2q8WFMQRBWH43KGH+5Ba1poiVvUoznX3tDuWZ0kWkjiRi6g1TnnlFMB4/tZ5MXLBnYa3nia5XJ/2CvQrXR/EKL0X61gTC03nghSnphF5QCC2rlwPmVpcvQk8UcG/fVJ9oH2QhSfUIOAwKH9xpb8+QJOMeYVlM9/zreuprbdO0vgS4dOLzhfOrFzSVyUrMXMRiwgSVP9NqbYQlCp2ai52qdwXH9e5ten/twYS9TKTePiUh251nFfcO22w0r/8jzIYGFCDuRtLdq3kImjHgtNkUwF0NteElo746WQhTJEI4mlm839ciKRXDp2tD58GreyT6eIwcpPxE4tmLp/Waqzypf2ek2mAZh7HnwtRZ+GnU1Q+iEYeWEOnBNW9h5T/EpCQbxq8+U8mJ2SeetwZN9ua4HS6aAr/XBWOxkOHKC10eNaGudMj0Ap/Ph4XfhXCDVxpRqqypKiI6VFN38Y2RCiML4Wg48hTY=~-1~-1~1742564350",
        "episod_session_id": "1742560761504.bd3bd789.1.1742563088177",
        "bm_sv": "34073B5BB713B0BE4B9D321DCB820AE8~YAAQFwxAF/d/lqiVAQAAxIPduBt4QfzIbqtpFjIX5nJXPN1vx4bSUsqKdhHw34Bn556sW/wDVkhfY0p3pm7nDsmWgMZu5DAAoPiYqKftTfHsuxCKIrqz5oQ2lN04Y9OWLuAD1Kh4vAhN/g0jA8mvDLu+gaASszOqDZSZZgyWEcw1YFcrWiK3cz5zm97yk8tVRq/gD/yZ17zVvmFFZZctWrBHkJP0th75fgI6i5n87mDsaLQDgM1L6jahwALTXmLD~1",
        "bm_sz": "6C0ED6E38C0ECA35FB70C981F1ADAE5D~YAAQFwxAF/h/lqiVAQAAxIPduBscyVdbfLzmjiN6zjXpG/5VYksQLL3n8CrhyDp0CdoOsVGkUePS671Ge54HHfjq401ZbIwBpPb4EOUQ8lwMuZNBMqJC0Z+nINR1AAo5rmKnkkCx0Z1uNg1RX6sMYI8PzXzBo+v3wzplsOEAfyrRvuW9u/um96ogFa6VR7GhreNOovjW/DF03CaxmS3tusAQzQ4bdpo5tognIeMMDsZ+XlnuXe2LQk2brAEqs7TxtXYhqQ3Rqi7rxiD36NsJ8JVdU7QXTjCO/AwMyZQVK9hVoCVgX+qqVByN3H5Rlup8SPvCKV9MiHeq8EatVT8Pw9wj1ZutDYyvIj/3gWzjOV8NdSTPX4EpDgP4l5oUPvIPBtXhpyLWr/Y8HFdLHv+TWbMYvUfs3Ka3wIHxzQ4AH/j0EKEYtcu57Lww/imSo4jVhHZiuec8SQzeSiCUBJUcOzBXC+ofofKINZ8gk8a0OFX/OCafh1xuoQG/lgRTYESBtw==~4273222~4342841",
    }

    for url in urls:

        response = requests.get(
            url,
            cookies=cookies,
            headers=headers,
            timeout=30,
        )
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL —Å –ø–æ–º–æ—â—å—é Path
        file_name = Path(urlparse(url).path).name  # –ò–∑–≤–ª–µ–∫–∞–µ—Ç 'prod-pl-PL_6.xml'
        file_path = xml_directory / file_name  # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å —Å –ø–æ–º–æ—â—å—é /
        if file_path.exists():
            logger.info(f"–§–∞–π–ª {file_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            continue
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        if response.status_code == 200:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª
            with open(file_path, "wb") as file:
                file.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {file_path}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {response.status_code}")
    parse_all_sitemap_urls()


def parse_all_sitemap_urls():
    """
    –ü–∞—Ä—Å–∏—Ç XML sitemap –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ç–µ–≥–æ–≤ <url><loc>

    Args:
        file_path (str): –ø—É—Ç—å –∫ XML —Ñ–∞–π–ª—É

    Returns:
        list: —Å–ø–∏—Å–æ–∫ URL-–æ–≤
    """
    urls = []
    for xml_file in xml_directory.glob("prod*.xml"):
        try:
            # –ü–∞—Ä—Å–∏–º XML —Ñ–∞–π–ª
            tree = ET.parse(xml_file)
            root = tree.getroot()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–Ω (namespace), –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            namespace = {"sitemap": "http://www.sitemaps.org/schemas/sitemap/0.9"}

            # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <url> –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º <loc>
            for url in root.findall(".//sitemap:url", namespace):
                loc = url.find("sitemap:loc", namespace)

                if loc is not None and loc.text:
                    urls.append(loc.text)

        except ET.ParseError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
            return []
        except FileNotFoundError:
            print(f"–§–∞–π–ª {xml_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL-–æ–≤")
    url_data = pd.DataFrame(urls, columns=["url"])
    url_data.to_csv(output_csv_file, index=False)


def main_th():
    urls = []
    with open(output_csv_file, newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            urls.append(row["url"])

    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = []
        for url in urls:
            output_html_file = (
                html_directory / f"html_{hashlib.md5(url.encode()).hexdigest()}.html"
            )

            if not os.path.exists(output_html_file):
                futures.append(executor.submit(get_html, url, output_html_file))
            else:
                print(f"–§–∞–π–ª –¥–ª—è {url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        results = []
        for future in as_completed(futures):
            # –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∏—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            results.append(future.result())


def fetch(url):
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    return response.text


def get_html(url, html_file):
    src = fetch(url)
    with open(html_file, "w", encoding="utf-8") as file:
        file.write(src)
    logger.info(html_file)


def update_ikea_matches_with_parsed_data(extracted_data):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö –≤ ikea_matches.json, –¥–æ–±–∞–≤–ª—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–Ω–µ –∏ –Ω–∞–ª–∏—á–∏–∏.

    Args:
        extracted_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ pars_htmls()
    """
    matches_file = data_directory / "ikea_matches.json"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏
    if not matches_file.exists():
        logger.error(f"–§–∞–π–ª {matches_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
    try:
        with open(matches_file, "r", encoding="utf-8") as f:
            ikea_matches = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ikea_matches)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {matches_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ JSON-—Ñ–∞–π–ª–∞ {matches_file}: {e}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ MPN
    mpn_to_data = {}
    for item in extracted_data:
        if item["mpn"]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ MPN –Ω–µ –ø—É—Å—Ç–æ–π
            mpn_to_data[item["mpn"]] = item

    logger.info(f"–°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å —Å {len(mpn_to_data)} —Ç–æ–≤–∞—Ä–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ MPN")

    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    updated_count = 0
    not_found_count = 0

    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö
    for ikea_item in ikea_matches:
        id_ikea = ikea_item["id_ikea"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ MPN
        if id_ikea in mpn_to_data:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ –∏ —Ü–µ–Ω–µ
            data_item = mpn_to_data[id_ikea]

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–ª–∏—á–∏–∏ –∏ —Ü–µ–Ω–µ
            ikea_item["product_in_stock"] = data_item["product_in_stock"]
            ikea_item["price"] = data_item["price"]

            logger.info(
                f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {id_ikea}: —Ü–µ–Ω–∞={data_item['price']}, –Ω–∞–ª–∏—á–∏–µ={data_item['product_in_stock']}"
            )
            updated_count += 1
        else:
            # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ –æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            ikea_item["product_in_stock"] = False
            ikea_item["price"] = None
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–≤–∞—Ä–∞ {id_ikea}")
            not_found_count += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    try:
        with open(matches_file, "w", encoding="utf-8") as f:
            json.dump(ikea_matches, f, ensure_ascii=False, indent=4)
        logger.info(
            f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {matches_file}. –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count}, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {not_found_count}"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")


# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ –ø–æ—Å–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏ pars_htmls:
# extracted_data = pars_htmls()
# update_ikea_matches_with_parsed_data(extracted_data)
def pars_htmls():
    logger.info("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü html")
    extracted_data = []

    # –ü—Ä–æ–π—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–º—É HTML —Ñ–∞–π–ª—É –≤ –ø–∞–ø–∫–µ
    for html_file in html_directory.glob("*.html"):
        with html_file.open(encoding="utf-8") as file:
            content = file.read()

        # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        soup = BeautifulSoup(content, "lxml")
        # 1. –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–∞
        product_title = soup.find("script", attrs={"id": "pip-range-json-ld"})
        json_string = product_title.string
        json_data = None
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ None
        if json_string:
            try:
                # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É –∫–∞–∫ JSON
                json_data = json.loads(json_string)
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {e}")
        mpn = json_data.get("mpn") if json_data else None

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
        price = None
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å lowPrice
        price = json_data.get("offers", {}).get("lowPrice")
        # –ï—Å–ª–∏ lowPrice –Ω–µ—Ç (None), –±–µ—Ä–µ–º price
        if price is None:
            price = json_data.get("offers", {}).get("price")
        price = float(price) * -100 if price else None
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —ç–ª–µ–º–µ–Ω—Ç —Å "Sklep - Dostƒôpne w magazynie"
        sklep_element = soup.select_one(
            "div.pip-store-section__button.js-stockcheck-section"
        )

        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –≤ —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è "Sklep" –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π
        all_status_elements = soup.select(
            "span.pip-status__label div.pip-store-section__button"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ "Sklep"
        if sklep_element and "Sklep" in sklep_element.text:
            availability_text = sklep_element.text.strip()

        elif all_status_elements:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —è–≤–Ω–æ "Sklep", –Ω–æ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—É—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∏–∑ –Ω–∏—Ö
            availability_text = all_status_elements[0].text.strip()
            logger.info(
                f"–§–∞–π–ª {mpn}: –ù–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {availability_text}"
            )
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏–∫–∞–∫–æ–π —ç–ª–µ–º–µ–Ω—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ
            alt_availability_element = soup.select_one(".pip-status__label")
            if alt_availability_element:
                availability_text = alt_availability_element.text.strip()
                logger.info(
                    f"–§–∞–π–ª {mpn}: –ù–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {availability_text}"
                )
            else:
                availability_text = "–°—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"
                logger.warning(f"–§–∞–π–ª {mpn}: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ñ—Ä–∞–∑–∞ "Sklep - Dostƒôpne w magazynie"
        has_in_store = "Sklep - Dostƒôpne w magazynie" in availability_text
        all_data = {
            "mpn": mpn,
            "price": price,
            "product_in_stock": has_in_store,
        }
        logger.info(all_data)
        extracted_data.append(all_data)
    with open(output_json_file, "w", encoding="utf-8") as json_file:
        json.dump(extracted_data, json_file, ensure_ascii=False, indent=4)
    # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –≤—ã–∑–æ–≤
    update_ikea_matches_with_parsed_data(extracted_data)


def update_excel_files_with_availability_info():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª—ã Excel –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ikea_matches.json.

    –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –†–æ–∑–µ—Ç–∫–∏:
    - –ò—â–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ id_ikea –≤ –∫–æ–ª–æ–Ω–∫–µ D (–ê—Ä—Ç–∏–∫—É–ª)
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É M (–ù–∞—è–≤–Ω—ñ—Å—Ç—å): "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ" –∏–ª–∏ "–ù–µ –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É H (–¶—ñ–Ω–∞) —Å —Ü–µ–Ω–æ–π

    –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ Prom:
    - –ò—â–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ id_ikea –≤ –∫–æ–ª–æ–Ω–∫–µ A (–ö–æ–¥_—Ç–æ–≤–∞—Ä—É)
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É P (–ù–∞—è–≤–Ω—ñ—Å—Ç—å): "7" –µ—Å–ª–∏ –≤ –Ω–∞–ª–∏—á–∏–∏, "-" –µ—Å–ª–∏ –Ω–µ—Ç
    - –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É I (–¶—ñ–Ω–∞) —Å —Ü–µ–Ω–æ–π
    """
    matches_file = data_directory / "ikea_matches.json"
    prom_file = current_directory / "–ü—Ä–æ–º.xlsx"
    rozetka_file = current_directory / "–†–æ–∑–µ—Ç–∫–∞.xlsx"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    if not matches_file.exists():
        logger.error(f"–§–∞–π–ª {matches_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    if not prom_file.exists():
        logger.error(f"–§–∞–π–ª {prom_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    if not rozetka_file.exists():
        logger.error(f"–§–∞–π–ª {rozetka_file} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ikea_matches.json
    try:
        with open(matches_file, "r", encoding="utf-8") as f:
            ikea_matches = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ikea_matches)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ {matches_file}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ JSON-—Ñ–∞–π–ª–∞ {matches_file}: {e}")
        return

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
    rozetka_items = [item for item in ikea_matches if item.get("source") == "rozetka"]
    prom_items = [item for item in ikea_matches if item.get("source") == "prom"]

    logger.info(
        f"–¢–æ–≤–∞—Ä–æ–≤ –¥–ª—è –†–æ–∑–µ—Ç–∫–∏: {len(rozetka_items)}, –¥–ª—è Prom: {len(prom_items)}"
    )

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª –†–æ–∑–µ—Ç–∫–∏
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª
        rozetka_df = pd.read_excel(rozetka_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –†–æ–∑–µ—Ç–∫–∞.xlsx, —Ä–∞–∑–º–µ—Ä: {rozetka_df.shape}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if "–ê—Ä—Ç–∏–∫—É–ª" not in rozetka_df.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É D
            try:
                column_D_name = rozetka_df.columns[3]  # 4-–π —Å—Ç–æ–ª–±–µ—Ü (–∏–Ω–¥–µ–∫—Å 3) - —ç—Ç–æ D
                rozetka_df = rozetka_df.rename(columns={column_D_name: "–ê—Ä—Ç–∏–∫—É–ª"})
                logger.info(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ D '{column_D_name}' –≤ '–ê—Ä—Ç–∏–∫—É–ª'")
            except IndexError:
                logger.error(f"–í —Ñ–∞–π–ª–µ {rozetka_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ D.")
                return

        # –ö–æ–ª–æ–Ω–∫–∞ M (–ù–∞—è–≤–Ω—ñ—Å—Ç—å) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 12
        availability_column_idx = 12
        availability_column_name = rozetka_df.columns[availability_column_idx]

        # –ö–æ–ª–æ–Ω–∫–∞ H (–¶—ñ–Ω–∞) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 7
        price_column_idx = 7
        price_column_name = rozetka_df.columns[price_column_idx]

        logger.info(
            f"–ö–æ–ª–æ–Ω–∫–∞ –Ω–∞–ª–∏—á–∏—è: {availability_column_name} (M), –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã: {price_column_name} (H)"
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—Ä—Ç–∏–∫—É–ª—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"].astype(str)

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        updated_count = 0
        for item in rozetka_items:
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –ê—Ä—Ç–∏–∫—É–ª —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å id_ikea
            mask = rozetka_df["–ê—Ä—Ç–∏–∫—É–ª"] == item["id_ikea"]
            if mask.any():
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ
                avail_value = (
                    "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
                    if item.get("product_in_stock", False)
                    else "–ù–µ –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
                )
                rozetka_df.loc[mask, availability_column_name] = avail_value

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                if item.get("price") is not None:
                    price_value = abs(
                        float(item["price"])
                    )  # –ë–µ—Ä–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
                    rozetka_df.loc[mask, price_column_name] = price_value

                updated_count += sum(mask)

        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ –†–æ–∑–µ—Ç–∫–∞.xlsx")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        output_rozetka_file = current_directory / "–†–æ–∑–µ—Ç–∫–∞_–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π.xlsx"
        rozetka_df.to_excel(output_rozetka_file, index=False)
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_rozetka_file}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –†–æ–∑–µ—Ç–∫–∞.xlsx: {e}")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª Prom
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª
        prom_df = pd.read_excel(prom_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –ü—Ä–æ–º.xlsx, —Ä–∞–∑–º–µ—Ä: {prom_df.shape}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É" not in prom_df.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –ø—Ä–æ–±—É–µ–º –∫–æ–ª–æ–Ω–∫—É A
            try:
                column_A_name = prom_df.columns[0]  # 1-–π —Å—Ç–æ–ª–±–µ—Ü (–∏–Ω–¥–µ–∫—Å 0) - —ç—Ç–æ A
                prom_df = prom_df.rename(columns={column_A_name: "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"})
                logger.info(f"–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ A '{column_A_name}' –≤ '–ö–æ–¥_—Ç–æ–≤–∞—Ä—É'")
            except IndexError:
                logger.error(f"–í —Ñ–∞–π–ª–µ {prom_file} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ A.")
                return

        # –ö–æ–ª–æ–Ω–∫–∞ P (–ù–∞—è–≤–Ω—ñ—Å—Ç—å) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 15
        availability_column_idx = 15
        availability_column_name = prom_df.columns[availability_column_idx]

        # –ö–æ–ª–æ–Ω–∫–∞ I (–¶—ñ–Ω–∞) - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å 8
        price_column_idx = 8
        price_column_name = prom_df.columns[price_column_idx]

        logger.info(
            f"–ö–æ–ª–æ–Ω–∫–∞ –Ω–∞–ª–∏—á–∏—è: {availability_column_name} (P), –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–Ω—ã: {price_column_name} (I)"
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–¥—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"].astype(str)

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        updated_count = 0
        for item in prom_items:
            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –ö–æ–¥_—Ç–æ–≤–∞—Ä—É —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å id_ikea
            mask = prom_df["–ö–æ–¥_—Ç–æ–≤–∞—Ä—É"] == item["id_ikea"]
            if mask.any():
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ
                avail_value = "7" if item.get("product_in_stock", False) else "-"
                prom_df.loc[mask, availability_column_name] = avail_value

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–Ω—É, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                if item.get("price") is not None:
                    price_value = abs(
                        float(item["price"])
                    )  # –ë–µ—Ä–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
                    prom_df.loc[mask, price_column_name] = price_value

                updated_count += sum(mask)

        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ –ü—Ä–æ–º.xlsx")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        output_prom_file = current_directory / "–ü—Ä–æ–º_–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π.xlsx"
        prom_df.to_excel(output_prom_file, index=False)
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_prom_file}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –ü—Ä–æ–º.xlsx: {e}")


# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏:
# update_excel_files_with_availability_info()
# def main_loop():
#     while True:
#         # –ó–∞–ø—Ä–æ—Å –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
#         print(
#             "–í–≤–µ–¥–∏—Ç–µ 1 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Å—ã–ª–æ–∫"
#             "\n–í–≤–µ–¥–∏—Ç–µ 2 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"
#             "\n–í–≤–µ–¥–∏—Ç–µ 3 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –≤ Excel"
#             "\n–í–≤–µ–¥–∏—Ç–µ 0 –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"
#         )
#         user_input = int(input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: "))

#         if user_input == 1:
#             download_all_xml()
#         elif user_input == 2:
#             main_th()
#         elif user_input == 3:
#             pars_htmls()
#         #     asyncio.run(parsing_page())
#         elif user_input == 0:
#             print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
#             break  # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
#         else:
#             print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è.")


if __name__ == "__main__":
    pars_htmls()
    update_excel_files_with_availability_info()
    # main_loop()
    # download_all_xml()
    # download_start_xml()
    # parse_all_sitemap_urls()
